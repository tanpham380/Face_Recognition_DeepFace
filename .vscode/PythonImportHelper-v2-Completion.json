[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "path",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "linalg",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "asarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "PIL.Image",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.Image",
        "description": "PIL.Image",
        "detail": "PIL.Image",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "data",
        "importPath": "torch.utils",
        "description": "torch.utils",
        "isExtraImport": true,
        "detail": "torch.utils",
        "documentation": {}
    },
    {
        "label": "data",
        "importPath": "torch.utils",
        "description": "torch.utils",
        "isExtraImport": true,
        "detail": "torch.utils",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "numbers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numbers",
        "description": "numbers",
        "detail": "numbers",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageDraw",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageEnhance",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "r3d_18",
        "importPath": "torchvision.models.video",
        "description": "torchvision.models.video",
        "isExtraImport": true,
        "detail": "torchvision.models.video",
        "documentation": {}
    },
    {
        "label": "R3D_18_Weights",
        "importPath": "torchvision.models.video",
        "description": "torchvision.models.video",
        "isExtraImport": true,
        "detail": "torchvision.models.video",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "NoisyActivation",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "images_to_batch",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "perform_val_bin",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "perform_val_bin",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "perform_rfw_val_bin",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "Options",
        "importPath": "options",
        "description": "options",
        "isExtraImport": true,
        "detail": "options",
        "documentation": {}
    },
    {
        "label": "Solver",
        "importPath": "solver",
        "description": "solver",
        "isExtraImport": true,
        "detail": "solver",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "PReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sigmoid",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MaxPool2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "PReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sigmoid",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Dropout2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "PReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MaxPool2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sigmoid",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU6",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "PReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "PReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MaxPool2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "PReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MaxPool2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "DataParallel",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "PReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sigmoid",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MaxPool2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "balanced_accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "auc",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "models",
        "description": "models",
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "IJBDataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "CosineLRScheduler",
        "importPath": "timm.scheduler.cosine_lr",
        "description": "timm.scheduler.cosine_lr",
        "isExtraImport": true,
        "detail": "timm.scheduler.cosine_lr",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ImageFolder",
        "importPath": "torchvision.datasets",
        "description": "torchvision.datasets",
        "isExtraImport": true,
        "detail": "torchvision.datasets",
        "documentation": {}
    },
    {
        "label": "ImageFolder",
        "importPath": "torchvision.datasets",
        "description": "torchvision.datasets",
        "isExtraImport": true,
        "detail": "torchvision.datasets",
        "documentation": {}
    },
    {
        "label": "pdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdb",
        "description": "pdb",
        "detail": "pdb",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "dataset.dataset_txt",
        "description": "dataset.dataset_txt",
        "isExtraImport": true,
        "detail": "dataset.dataset_txt",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "config_test",
        "description": "config_test",
        "isExtraImport": true,
        "detail": "config_test",
        "documentation": {}
    },
    {
        "label": "model_mobilefaceNet",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "model",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "operator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "operator",
        "description": "operator",
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "_flatten",
        "importPath": "tkinter",
        "description": "tkinter",
        "isExtraImport": true,
        "detail": "tkinter",
        "documentation": {}
    },
    {
        "label": "wasserstein_distance",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "model",
        "importPath": "generate_pseudo_labels.extract_embedding.model",
        "description": "generate_pseudo_labels.extract_embedding.model",
        "isExtraImport": true,
        "detail": "generate_pseudo_labels.extract_embedding.model",
        "documentation": {}
    },
    {
        "label": "model_mobilefaceNet",
        "importPath": "generate_pseudo_labels.extract_embedding.model",
        "description": "generate_pseudo_labels.extract_embedding.model",
        "isExtraImport": true,
        "detail": "generate_pseudo_labels.extract_embedding.model",
        "documentation": {}
    },
    {
        "label": "model",
        "importPath": "generate_pseudo_labels.extract_embedding.model",
        "description": "generate_pseudo_labels.extract_embedding.model",
        "isExtraImport": true,
        "detail": "generate_pseudo_labels.extract_embedding.model",
        "documentation": {}
    },
    {
        "label": "stats",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "description": "generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "isExtraImport": true,
        "detail": "generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "train_config",
        "description": "train_config",
        "isExtraImport": true,
        "detail": "train_config",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "torch.onnx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.onnx",
        "description": "torch.onnx",
        "detail": "torch.onnx",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "torchkit.backbone",
        "description": "torchkit.backbone",
        "isExtraImport": true,
        "detail": "torchkit.backbone",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "all_gather_tensor",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "importPath": "torchkit.util.utils",
        "description": "torchkit.util.utils",
        "isExtraImport": true,
        "detail": "torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "onnxruntime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxruntime",
        "description": "onnxruntime",
        "detail": "onnxruntime",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "GDC",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "GDC",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "LinearBlock",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "GNAP",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "GDC",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "importPath": "torchkit.backbone.common",
        "description": "torchkit.backbone.common",
        "isExtraImport": true,
        "detail": "torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "torch.cuda.amp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "torch.nn.init",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "accuracy_dist",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AllGather",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "get_class_split",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "accuracy_dist",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AllGather",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "accuracy_dist",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AllGather",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "CkptLoader",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "get_class_split",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "separate_resnet_bn_paras",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "accuracy_dist",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AllGather",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "accuracy_dist",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "AllGather",
        "importPath": "torchkit.util",
        "description": "torchkit.util",
        "isExtraImport": true,
        "detail": "torchkit.util",
        "documentation": {}
    },
    {
        "label": "adjust_lr",
        "importPath": "torchkit.hooks.learning_rate_hook",
        "description": "torchkit.hooks.learning_rate_hook",
        "isExtraImport": true,
        "detail": "torchkit.hooks.learning_rate_hook",
        "documentation": {}
    },
    {
        "label": "get_loss",
        "importPath": "torchkit.loss",
        "description": "torchkit.loss",
        "isExtraImport": true,
        "detail": "torchkit.loss",
        "documentation": {}
    },
    {
        "label": "get_loss",
        "importPath": "torchkit.loss",
        "description": "torchkit.loss",
        "isExtraImport": true,
        "detail": "torchkit.loss",
        "documentation": {}
    },
    {
        "label": "get_loss",
        "importPath": "torchkit.loss",
        "description": "torchkit.loss",
        "isExtraImport": true,
        "detail": "torchkit.loss",
        "documentation": {}
    },
    {
        "label": "get_loss",
        "importPath": "torchkit.loss",
        "description": "torchkit.loss",
        "isExtraImport": true,
        "detail": "torchkit.loss",
        "documentation": {}
    },
    {
        "label": "get_loss",
        "importPath": "torchkit.loss",
        "description": "torchkit.loss",
        "isExtraImport": true,
        "detail": "torchkit.loss",
        "documentation": {}
    },
    {
        "label": "get_loss",
        "importPath": "torchkit.loss",
        "description": "torchkit.loss",
        "isExtraImport": true,
        "detail": "torchkit.loss",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "importPath": "torchkit.task",
        "description": "torchkit.task",
        "isExtraImport": true,
        "detail": "torchkit.task",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "importPath": "torchkit.task",
        "description": "torchkit.task",
        "isExtraImport": true,
        "detail": "torchkit.task",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "importPath": "torchkit.task",
        "description": "torchkit.task",
        "isExtraImport": true,
        "detail": "torchkit.task",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "importPath": "torchkit.task",
        "description": "torchkit.task",
        "isExtraImport": true,
        "detail": "torchkit.task",
        "documentation": {}
    },
    {
        "label": "get_head",
        "importPath": "torchkit.head",
        "description": "torchkit.head",
        "isExtraImport": true,
        "detail": "torchkit.head",
        "documentation": {}
    },
    {
        "label": "get_head",
        "importPath": "torchkit.head",
        "description": "torchkit.head",
        "isExtraImport": true,
        "detail": "torchkit.head",
        "documentation": {}
    },
    {
        "label": "dct",
        "importPath": "torchjpeg",
        "description": "torchjpeg",
        "isExtraImport": true,
        "detail": "torchjpeg",
        "documentation": {}
    },
    {
        "label": "dct",
        "importPath": "torchjpeg",
        "description": "torchjpeg",
        "isExtraImport": true,
        "detail": "torchjpeg",
        "documentation": {}
    },
    {
        "label": "dct",
        "importPath": "torchjpeg",
        "description": "torchjpeg",
        "isExtraImport": true,
        "detail": "torchjpeg",
        "documentation": {}
    },
    {
        "label": "dct",
        "importPath": "torchjpeg",
        "description": "torchjpeg",
        "isExtraImport": true,
        "detail": "torchjpeg",
        "documentation": {}
    },
    {
        "label": "DuetFaceModel",
        "importPath": "tasks.duetface.duetface_model",
        "description": "tasks.duetface.duetface_model",
        "isExtraImport": true,
        "detail": "tasks.duetface.duetface_model",
        "documentation": {}
    },
    {
        "label": "ClientBackbone",
        "importPath": "tasks.duetface.duetface_model",
        "description": "tasks.duetface.duetface_model",
        "isExtraImport": true,
        "detail": "tasks.duetface.duetface_model",
        "documentation": {}
    },
    {
        "label": "ClientBackbone",
        "importPath": "tasks.duetface.local_backbones",
        "description": "tasks.duetface.local_backbones",
        "isExtraImport": true,
        "detail": "tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "ServerBackbone",
        "importPath": "tasks.duetface.local_backbones",
        "description": "tasks.duetface.local_backbones",
        "isExtraImport": true,
        "detail": "tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "torch.fft",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.fft",
        "description": "torch.fft",
        "detail": "torch.fft",
        "documentation": {}
    },
    {
        "label": "ConvexHull",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "Delaunay",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "PFLDInference",
        "importPath": "tasks.duetface.pfld_model",
        "description": "tasks.duetface.pfld_model",
        "isExtraImport": true,
        "detail": "tasks.duetface.pfld_model",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "SingleDataset",
        "importPath": "torchkit.data",
        "description": "torchkit.data",
        "isExtraImport": true,
        "detail": "torchkit.data",
        "documentation": {}
    },
    {
        "label": "MultiDataset",
        "importPath": "torchkit.data",
        "description": "torchkit.data",
        "isExtraImport": true,
        "detail": "torchkit.data",
        "documentation": {}
    },
    {
        "label": "MultiDistributedSampler",
        "importPath": "torchkit.data",
        "description": "torchkit.data",
        "isExtraImport": true,
        "detail": "torchkit.data",
        "documentation": {}
    },
    {
        "label": "example_pb2",
        "importPath": "torchkit.data",
        "description": "torchkit.data",
        "isExtraImport": true,
        "detail": "torchkit.data",
        "documentation": {}
    },
    {
        "label": "create_label2index",
        "importPath": "dataset",
        "description": "dataset",
        "isExtraImport": true,
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "BalancedBatchSampler",
        "importPath": "dataset",
        "description": "dataset",
        "isExtraImport": true,
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "EKD",
        "importPath": "distillation.ekd",
        "description": "distillation.ekd",
        "isExtraImport": true,
        "detail": "distillation.ekd",
        "documentation": {}
    },
    {
        "label": "UNet",
        "importPath": "tasks.minusface.utils",
        "description": "tasks.minusface.utils",
        "isExtraImport": true,
        "detail": "tasks.minusface.utils",
        "documentation": {}
    },
    {
        "label": "dct_transform",
        "importPath": "tasks.partialface.utils",
        "description": "tasks.partialface.utils",
        "isExtraImport": true,
        "detail": "tasks.partialface.utils",
        "documentation": {}
    },
    {
        "label": "idct_transform",
        "importPath": "tasks.partialface.utils",
        "description": "tasks.partialface.utils",
        "isExtraImport": true,
        "detail": "tasks.partialface.utils",
        "documentation": {}
    },
    {
        "label": "TrainTask",
        "importPath": "tasks.partialface.train",
        "description": "tasks.partialface.train",
        "isExtraImport": true,
        "detail": "tasks.partialface.train",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "importPath": "torchkit.task.base_task",
        "description": "torchkit.task.base_task",
        "isExtraImport": true,
        "detail": "torchkit.task.base_task",
        "documentation": {}
    },
    {
        "label": "LocalBaseTask",
        "importPath": "tasks.partialface",
        "description": "tasks.partialface",
        "isExtraImport": true,
        "detail": "tasks.partialface",
        "documentation": {}
    },
    {
        "label": "form_training_batch",
        "importPath": "tasks.partialface",
        "description": "tasks.partialface",
        "isExtraImport": true,
        "detail": "tasks.partialface",
        "documentation": {}
    },
    {
        "label": "MinusLoss",
        "importPath": "tasks.minusface.minusface",
        "description": "tasks.minusface.minusface",
        "isExtraImport": true,
        "detail": "tasks.minusface.minusface",
        "documentation": {}
    },
    {
        "label": "timeit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timeit",
        "description": "timeit",
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "numpy.matlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy.matlib",
        "description": "numpy.matlib",
        "detail": "numpy.matlib",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "skimage",
        "description": "skimage",
        "isExtraImport": true,
        "detail": "skimage",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "skimage",
        "description": "skimage",
        "isExtraImport": true,
        "detail": "skimage",
        "documentation": {}
    },
    {
        "label": "sklearn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn",
        "description": "sklearn",
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "metrics",
        "importPath": "sklearn",
        "description": "sklearn",
        "isExtraImport": true,
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "KFold",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "PCA",
        "importPath": "sklearn.decomposition",
        "description": "sklearn.decomposition",
        "isExtraImport": true,
        "detail": "sklearn.decomposition",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "_ntuple",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "with_statement",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "isfunction",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "ReduceOp",
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "isExtraImport": true,
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "ReduceOp",
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "isExtraImport": true,
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "ReduceOp",
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "isExtraImport": true,
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "descriptor",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "reflection",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "symbol_database",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "descriptor",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "reflection",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "symbol_database",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "dareblopy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dareblopy",
        "description": "dareblopy",
        "detail": "dareblopy",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "accumulate",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "accumulate",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "calc_logits",
        "importPath": "torchkit.head.localfc.common",
        "description": "torchkit.head.localfc.common",
        "isExtraImport": true,
        "detail": "torchkit.head.localfc.common",
        "documentation": {}
    },
    {
        "label": "calc_logits",
        "importPath": "torchkit.head.localfc.common",
        "description": "torchkit.head.localfc.common",
        "isExtraImport": true,
        "detail": "torchkit.head.localfc.common",
        "documentation": {}
    },
    {
        "label": "calc_logits",
        "importPath": "torchkit.head.localfc.common",
        "description": "torchkit.head.localfc.common",
        "isExtraImport": true,
        "detail": "torchkit.head.localfc.common",
        "documentation": {}
    },
    {
        "label": "calc_logits",
        "importPath": "torchkit.head.localfc.common",
        "description": "torchkit.head.localfc.common",
        "isExtraImport": true,
        "detail": "torchkit.head.localfc.common",
        "documentation": {}
    },
    {
        "label": "bisect",
        "importPath": "bisect",
        "description": "bisect",
        "isExtraImport": true,
        "detail": "bisect",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard.writer",
        "description": "torch.utils.tensorboard.writer",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard.writer",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "albumentations",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "albumentations",
        "description": "albumentations",
        "detail": "albumentations",
        "documentation": {}
    },
    {
        "label": "ToTensorV2",
        "importPath": "albumentations.pytorch.transforms",
        "description": "albumentations.pytorch.transforms",
        "isExtraImport": true,
        "detail": "albumentations.pytorch.transforms",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "add_face_margin",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "get_face_box",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "add_face_margin",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "get_face_box",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "save_test_results",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common.utils",
        "description": "common.utils",
        "isExtraImport": true,
        "detail": "common.utils",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "wandb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wandb",
        "description": "wandb",
        "detail": "wandb",
        "documentation": {}
    },
    {
        "label": "loguru",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "loguru",
        "description": "loguru",
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "CheckpointSaver",
        "importPath": "timm.utils",
        "description": "timm.utils",
        "isExtraImport": true,
        "detail": "timm.utils",
        "documentation": {}
    },
    {
        "label": "resume_checkpoint",
        "importPath": "timm.models",
        "description": "timm.models",
        "isExtraImport": true,
        "detail": "timm.models",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "optimizers",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "schedulers",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "optimizers",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "optimizers",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "omegaconf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "omegaconf",
        "description": "omegaconf",
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "socket",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "socket",
        "description": "socket",
        "detail": "socket",
        "documentation": {}
    },
    {
        "label": "queue",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "queue",
        "description": "queue",
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "brentq",
        "importPath": "scipy.optimize",
        "description": "scipy.optimize",
        "isExtraImport": true,
        "detail": "scipy.optimize",
        "documentation": {}
    },
    {
        "label": "brentq",
        "importPath": "scipy.optimize",
        "description": "scipy.optimize",
        "isExtraImport": true,
        "detail": "scipy.optimize",
        "documentation": {}
    },
    {
        "label": "interp1d",
        "importPath": "scipy.interpolate",
        "description": "scipy.interpolate",
        "isExtraImport": true,
        "detail": "scipy.interpolate",
        "documentation": {}
    },
    {
        "label": "interp1d",
        "importPath": "scipy.interpolate",
        "description": "scipy.interpolate",
        "isExtraImport": true,
        "detail": "scipy.interpolate",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "scipy.sparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.sparse",
        "description": "scipy.sparse",
        "detail": "scipy.sparse",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "spsolve",
        "importPath": "scipy.sparse.linalg",
        "description": "scipy.sparse.linalg",
        "isExtraImport": true,
        "detail": "scipy.sparse.linalg",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.networks",
        "description": "models.networks",
        "isExtraImport": true,
        "detail": "models.networks",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "irse",
        "importPath": "models.Pretrained_FR_Models",
        "description": "models.Pretrained_FR_Models",
        "isExtraImport": true,
        "detail": "models.Pretrained_FR_Models",
        "documentation": {}
    },
    {
        "label": "facenet",
        "importPath": "models.Pretrained_FR_Models",
        "description": "models.Pretrained_FR_Models",
        "isExtraImport": true,
        "detail": "models.Pretrained_FR_Models",
        "documentation": {}
    },
    {
        "label": "ir152",
        "importPath": "models.Pretrained_FR_Models",
        "description": "models.Pretrained_FR_Models",
        "isExtraImport": true,
        "detail": "models.Pretrained_FR_Models",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "wraps",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "imsave",
        "importPath": "skimage.io",
        "description": "skimage.io",
        "isExtraImport": true,
        "detail": "skimage.io",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "aiohttp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiohttp",
        "description": "aiohttp",
        "detail": "aiohttp",
        "documentation": {}
    },
    {
        "label": "async_timeout",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "async_timeout",
        "description": "async_timeout",
        "detail": "async_timeout",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.makeup_attack",
        "description": "models.makeup_attack",
        "isExtraImport": true,
        "detail": "models.makeup_attack",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.makeup_attack",
        "description": "models.makeup_attack",
        "isExtraImport": true,
        "detail": "models.makeup_attack",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "datasets.dataset",
        "description": "datasets.dataset",
        "isExtraImport": true,
        "detail": "datasets.dataset",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "datasets.dataset",
        "description": "datasets.dataset",
        "isExtraImport": true,
        "detail": "datasets.dataset",
        "documentation": {}
    },
    {
        "label": "from",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "from",
        "description": "from",
        "detail": "from",
        "documentation": {}
    },
    {
        "label": "models.models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "models.models",
        "description": "models.models",
        "detail": "models.models",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils.utils",
        "description": "utils.utils",
        "isExtraImport": true,
        "detail": "utils.utils",
        "documentation": {}
    },
    {
        "label": "lmdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lmdb",
        "description": "lmdb",
        "detail": "lmdb",
        "documentation": {}
    },
    {
        "label": "create_base_transforms",
        "importPath": "common.data",
        "description": "common.data",
        "isExtraImport": true,
        "detail": "common.data",
        "documentation": {}
    },
    {
        "label": "create_base_dataloader",
        "importPath": "common.data",
        "description": "common.data",
        "isExtraImport": true,
        "detail": "common.data",
        "documentation": {}
    },
    {
        "label": "create_base_transforms",
        "importPath": "common.data",
        "description": "common.data",
        "isExtraImport": true,
        "detail": "common.data",
        "documentation": {}
    },
    {
        "label": "create_base_dataloader",
        "importPath": "common.data",
        "description": "common.data",
        "isExtraImport": true,
        "detail": "common.data",
        "documentation": {}
    },
    {
        "label": "create_base_transforms",
        "importPath": "common.data",
        "description": "common.data",
        "isExtraImport": true,
        "detail": "common.data",
        "documentation": {}
    },
    {
        "label": "create_base_dataloader",
        "importPath": "common.data",
        "description": "common.data",
        "isExtraImport": true,
        "detail": "common.data",
        "documentation": {}
    },
    {
        "label": "create_base_transforms",
        "importPath": "common.data",
        "description": "common.data",
        "isExtraImport": true,
        "detail": "common.data",
        "documentation": {}
    },
    {
        "label": "create_base_dataloader",
        "importPath": "common.data",
        "description": "common.data",
        "isExtraImport": true,
        "detail": "common.data",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "common.utils.model_init",
        "description": "common.utils.model_init",
        "isExtraImport": true,
        "detail": "common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "common.utils.model_init",
        "description": "common.utils.model_init",
        "isExtraImport": true,
        "detail": "common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "common.utils.model_init",
        "description": "common.utils.model_init",
        "isExtraImport": true,
        "detail": "common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "common.utils.model_init",
        "description": "common.utils.model_init",
        "isExtraImport": true,
        "detail": "common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "importPath": "common.task",
        "description": "common.task",
        "isExtraImport": true,
        "detail": "common.task",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "importPath": "common.task",
        "description": "common.task",
        "isExtraImport": true,
        "detail": "common.task",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "importPath": "common.task",
        "description": "common.task",
        "isExtraImport": true,
        "detail": "common.task",
        "documentation": {}
    },
    {
        "label": "test_module",
        "importPath": "common.task.fas",
        "description": "common.task.fas",
        "isExtraImport": true,
        "detail": "common.task.fas",
        "documentation": {}
    },
    {
        "label": "test_module",
        "importPath": "common.task.fas",
        "description": "common.task.fas",
        "isExtraImport": true,
        "detail": "common.task.fas",
        "documentation": {}
    },
    {
        "label": "auto",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm",
        "description": "timm",
        "detail": "timm",
        "documentation": {}
    },
    {
        "label": "VideoReader",
        "importPath": "decord",
        "description": "decord",
        "isExtraImport": true,
        "detail": "decord",
        "documentation": {}
    },
    {
        "label": "cpu",
        "importPath": "decord",
        "description": "decord",
        "isExtraImport": true,
        "detail": "decord",
        "documentation": {}
    },
    {
        "label": "torch.utils.model_zoo",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.model_zoo",
        "description": "torch.utils.model_zoo",
        "detail": "torch.utils.model_zoo",
        "documentation": {}
    },
    {
        "label": "DeepFace",
        "importPath": "deepface",
        "description": "deepface",
        "isExtraImport": true,
        "detail": "deepface",
        "documentation": {}
    },
    {
        "label": "__version__",
        "importPath": "deepface",
        "description": "deepface",
        "isExtraImport": true,
        "detail": "deepface",
        "documentation": {}
    },
    {
        "label": "DeepFace",
        "importPath": "deepface",
        "description": "deepface",
        "isExtraImport": true,
        "detail": "deepface",
        "documentation": {}
    },
    {
        "label": "__version__",
        "importPath": "deepface",
        "description": "deepface",
        "isExtraImport": true,
        "detail": "deepface",
        "documentation": {}
    },
    {
        "label": "modeling",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "representation",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "verification",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "recognition",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "demography",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "detection",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "streaming",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "preprocessing",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "modeling",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "representation",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "verification",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "recognition",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "demography",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "detection",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "streaming",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "preprocessing",
        "importPath": "deepface.modules",
        "description": "deepface.modules",
        "isExtraImport": true,
        "detail": "deepface.modules",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "isExtraImport": true,
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "isExtraImport": true,
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "isExtraImport": true,
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "isExtraImport": true,
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "isExtraImport": true,
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "isExtraImport": true,
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "isExtraImport": true,
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "isExtraImport": true,
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "package_utils",
        "importPath": "deepface.commons",
        "description": "deepface.commons",
        "isExtraImport": true,
        "detail": "deepface.commons",
        "documentation": {}
    },
    {
        "label": "folder_utils",
        "importPath": "deepface.commons",
        "description": "deepface.commons",
        "isExtraImport": true,
        "detail": "deepface.commons",
        "documentation": {}
    },
    {
        "label": "package_utils",
        "importPath": "deepface.commons",
        "description": "deepface.commons",
        "isExtraImport": true,
        "detail": "deepface.commons",
        "documentation": {}
    },
    {
        "label": "folder_utils",
        "importPath": "deepface.commons",
        "description": "deepface.commons",
        "isExtraImport": true,
        "detail": "deepface.commons",
        "documentation": {}
    },
    {
        "label": "SQLiteManager",
        "importPath": "core.utils.database",
        "description": "core.utils.database",
        "isExtraImport": true,
        "detail": "core.utils.database",
        "documentation": {}
    },
    {
        "label": "ZoDB",
        "importPath": "core.utils.database",
        "description": "core.utils.database",
        "isExtraImport": true,
        "detail": "core.utils.database",
        "documentation": {}
    },
    {
        "label": "Persistent",
        "importPath": "persistent",
        "description": "persistent",
        "isExtraImport": true,
        "detail": "persistent",
        "documentation": {}
    },
    {
        "label": "Persistent",
        "importPath": "persistent",
        "description": "persistent",
        "isExtraImport": true,
        "detail": "persistent",
        "documentation": {}
    },
    {
        "label": "Persistent",
        "importPath": "persistent",
        "description": "persistent",
        "isExtraImport": true,
        "detail": "persistent",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "DB",
        "importPath": "ZODB",
        "description": "ZODB",
        "isExtraImport": true,
        "detail": "ZODB",
        "documentation": {}
    },
    {
        "label": "FileStorage",
        "importPath": "ZODB",
        "description": "ZODB",
        "isExtraImport": true,
        "detail": "ZODB",
        "documentation": {}
    },
    {
        "label": "transaction",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "transaction",
        "description": "transaction",
        "detail": "transaction",
        "documentation": {}
    },
    {
        "label": "PersistentDict",
        "importPath": "persistent.dict",
        "description": "persistent.dict",
        "isExtraImport": true,
        "detail": "persistent.dict",
        "documentation": {}
    },
    {
        "label": "DirectoryHash",
        "importPath": "core.utils.models.directory_hash",
        "description": "core.utils.models.directory_hash",
        "isExtraImport": true,
        "detail": "core.utils.models.directory_hash",
        "documentation": {}
    },
    {
        "label": "FaceData",
        "importPath": "core.utils.models.face_data",
        "description": "core.utils.models.face_data",
        "isExtraImport": true,
        "detail": "core.utils.models.face_data",
        "documentation": {}
    },
    {
        "label": "TaskQueue",
        "importPath": "core.utils.models.task_queue",
        "description": "core.utils.models.task_queue",
        "isExtraImport": true,
        "detail": "core.utils.models.task_queue",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "flask",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flask",
        "description": "flask",
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "g",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "current_app",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "g",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Blueprint",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "current_app",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "current_app",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "cosine",
        "importPath": "scipy.spatial.distance",
        "description": "scipy.spatial.distance",
        "isExtraImport": true,
        "detail": "scipy.spatial.distance",
        "documentation": {}
    },
    {
        "label": "cosine",
        "importPath": "scipy.spatial.distance",
        "description": "scipy.spatial.distance",
        "isExtraImport": true,
        "detail": "scipy.spatial.distance",
        "documentation": {}
    },
    {
        "label": "IMAGES_DIR",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "LOG_PATH_FILE",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "API_KEY",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "BASE_PATH",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "IMAGES_DIR",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "NUMBER_WORKER",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "BASE_PATH",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "IMAGES_DIR",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "TEMP_DIR",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "BASE_PATH",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "DB_PATH",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "IMAGES_DIR",
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "isExtraImport": true,
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "RotatingFileHandler",
        "importPath": "logging.handlers",
        "description": "logging.handlers",
        "isExtraImport": true,
        "detail": "logging.handlers",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "recreate_DB",
        "importPath": "core.service",
        "description": "core.service",
        "isExtraImport": true,
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "add_task_to_queue",
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "isExtraImport": true,
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "add_task_to_queue",
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "isExtraImport": true,
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "stop_workers",
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "isExtraImport": true,
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "start_workers",
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "isExtraImport": true,
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "find_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "service",
        "importPath": "core",
        "description": "core",
        "isExtraImport": true,
        "detail": "core",
        "documentation": {}
    },
    {
        "label": "require_api_key",
        "importPath": "core.utils.middleware",
        "description": "core.utils.middleware",
        "isExtraImport": true,
        "detail": "core.utils.middleware",
        "documentation": {}
    },
    {
        "label": "augment_image",
        "importPath": "core.utils.augment_images",
        "description": "core.utils.augment_images",
        "isExtraImport": true,
        "detail": "core.utils.augment_images",
        "documentation": {}
    },
    {
        "label": "delete_directory_if_empty",
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "isExtraImport": true,
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "delete_images_for_uid",
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "isExtraImport": true,
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "extract_base_identity",
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "isExtraImport": true,
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "save_image",
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "isExtraImport": true,
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "app",
        "description": "app",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "atexit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "atexit",
        "description": "atexit",
        "detail": "atexit",
        "documentation": {}
    },
    {
        "label": "DeepFaceController",
        "importPath": "core.deepface_controller.controller",
        "description": "core.deepface_controller.controller",
        "isExtraImport": true,
        "detail": "core.deepface_controller.controller",
        "documentation": {}
    },
    {
        "label": "check_and_update_directory_hash",
        "importPath": "core.utils.monitor_folder_hash",
        "description": "core.utils.monitor_folder_hash",
        "isExtraImport": true,
        "detail": "core.utils.monitor_folder_hash",
        "documentation": {}
    },
    {
        "label": "blueprint",
        "importPath": "core.routes",
        "description": "core.routes",
        "isExtraImport": true,
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "ctypes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ctypes",
        "description": "ctypes",
        "detail": "ctypes",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "minidom",
        "importPath": "xml.dom",
        "description": "xml.dom",
        "isExtraImport": true,
        "detail": "xml.dom",
        "documentation": {}
    },
    {
        "label": "DFEWDataset",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.dataset_DFEW",
        "description": "TFace.attribute.M3DFEL.datasets.dataset_DFEW",
        "peekOfCode": "class DFEWDataset(data.Dataset):\n    def __init__(self, args, mode):\n        \"\"\"Dataset for DFEW\n        Args:\n            args\n            mode: String(\"train\" or \"test\")\n            num_frames: the number of sampled frames from every video, default: 16\n            image_size: crop images to 112*112\n        \"\"\"\n        self.args = args",
        "detail": "TFace.attribute.M3DFEL.datasets.dataset_DFEW",
        "documentation": {}
    },
    {
        "label": "GroupRandomCrop",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.video_transform",
        "description": "TFace.attribute.M3DFEL.datasets.video_transform",
        "peekOfCode": "class GroupRandomCrop(object):\n    \"\"\"Random crop a group of images(video)\n    Args:\n        size: the size of images after random crop\n    \"\"\"\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size",
        "detail": "TFace.attribute.M3DFEL.datasets.video_transform",
        "documentation": {}
    },
    {
        "label": "GroupColorJitter",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.video_transform",
        "description": "TFace.attribute.M3DFEL.datasets.video_transform",
        "peekOfCode": "class GroupColorJitter(object):\n    \"\"\"Random color jitter a group of images(video)\n    Args:\n        bcsh\n    \"\"\"\n    def __init__(self, bcsh=0.4):\n        super().__init__()\n        brightness = contrast = saturation = hue = bcsh\n        self.brightness = self._check_input(brightness, \"brightness\")\n        self.contrast = self._check_input(contrast, \"contrast\")",
        "detail": "TFace.attribute.M3DFEL.datasets.video_transform",
        "documentation": {}
    },
    {
        "label": "GroupRandomHorizontalFlip",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.video_transform",
        "description": "TFace.attribute.M3DFEL.datasets.video_transform",
        "peekOfCode": "class GroupRandomHorizontalFlip(object):\n    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\n    \"\"\"\n    def __init__(self, is_flow=False):\n        self.is_flow = is_flow\n    def __call__(self, img_group, is_flow=False):\n        v = random.random()\n        if v < 0.5:\n            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n            if self.is_flow:",
        "detail": "TFace.attribute.M3DFEL.datasets.video_transform",
        "documentation": {}
    },
    {
        "label": "GroupScale",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.video_transform",
        "description": "TFace.attribute.M3DFEL.datasets.video_transform",
        "peekOfCode": "class GroupScale(object):\n    \"\"\" Rescales the input PIL.Image to the given 'size'.\n    'size' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.worker = torchvision.transforms.Resize(size, interpolation)",
        "detail": "TFace.attribute.M3DFEL.datasets.video_transform",
        "documentation": {}
    },
    {
        "label": "GroupResize",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.video_transform",
        "description": "TFace.attribute.M3DFEL.datasets.video_transform",
        "peekOfCode": "class GroupResize(object):\n    \"\"\"Random resize a group of images(video)\n    Args:\n        size\n    \"\"\"\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n    def __call__(self, img_group):\n        out_group = list()",
        "detail": "TFace.attribute.M3DFEL.datasets.video_transform",
        "documentation": {}
    },
    {
        "label": "GroupRandomSizedCrop",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.video_transform",
        "description": "TFace.attribute.M3DFEL.datasets.video_transform",
        "peekOfCode": "class GroupRandomSizedCrop(object):\n    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n    This is popularly used to train the Inception networks\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation",
        "detail": "TFace.attribute.M3DFEL.datasets.video_transform",
        "documentation": {}
    },
    {
        "label": "Stack",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.video_transform",
        "description": "TFace.attribute.M3DFEL.datasets.video_transform",
        "peekOfCode": "class Stack(object):\n    def __init__(self, roll=False):\n        self.roll = roll\n    def __call__(self, img_group):\n        if img_group[0].mode == 'L' or img_group[0].mode == 'F':\n            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n        elif img_group[0].mode == 'RGB':\n            if self.roll:\n                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n            else:",
        "detail": "TFace.attribute.M3DFEL.datasets.video_transform",
        "documentation": {}
    },
    {
        "label": "ToTorchFormatTensor",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.datasets.video_transform",
        "description": "TFace.attribute.M3DFEL.datasets.video_transform",
        "peekOfCode": "class ToTorchFormatTensor(object):\n    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n    def __init__(self, div=True):\n        self.div = div\n    def __call__(self, pic):\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n        else:",
        "detail": "TFace.attribute.M3DFEL.datasets.video_transform",
        "documentation": {}
    },
    {
        "label": "M3DFEL",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.models.M3DFEL",
        "description": "TFace.attribute.M3DFEL.models.M3DFEL",
        "peekOfCode": "class M3DFEL(nn.Module):\n    \"\"\"The proposed M3DFEL framework\n    Args:\n        args\n    \"\"\"\n    def __init__(self, args):\n        super(M3DFEL, self).__init__()\n        self.args = args\n        self.device = torch.device(\n            'cuda:%d' % args.gpu_ids[0] if args.gpu_ids else 'cpu')",
        "detail": "TFace.attribute.M3DFEL.models.M3DFEL",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.attribute.M3DFEL.main",
        "description": "TFace.attribute.M3DFEL.main",
        "peekOfCode": "def main():\n    \"\"\"Run the whole training process through solver\n    Change the options according to your situation, especially workers, gpu_ids, batch_size and epochs\n    \"\"\"\n    args = Options().parse()\n    solver = Solver(args)\n    solver.run()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.attribute.M3DFEL.main",
        "documentation": {}
    },
    {
        "label": "Options",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.options",
        "description": "TFace.attribute.M3DFEL.options",
        "peekOfCode": "class Options(object):\n    def __init__(self):\n        super(Options, self).__init__()\n    def initialize(self):\n        parser = argparse.ArgumentParser()\n        # basic settings\n        parser.add_argument('--mode', type=str, default=\"train\")\n        parser.add_argument('--dataset', type=str, default=\"DFEW\")\n        parser.add_argument('--gpu_ids', type=str, default='0,1,2,3',\n                            help='gpu ids, eg. 0,1,2; -1 for cpu.')",
        "detail": "TFace.attribute.M3DFEL.options",
        "documentation": {}
    },
    {
        "label": "Solver",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.solver",
        "description": "TFace.attribute.M3DFEL.solver",
        "peekOfCode": "class Solver(object):\n    def __init__(self, args):\n        \"\"\"Init the global settings including device, seed, models, dataloaders, crterions, optimizers and schedulers\n        Args:\n            args\n        \"\"\"\n        super(Solver, self).__init__()\n        self.args = args\n        self.log_path = os.path.join(self.args.output_path, \"log.txt\")\n        self.emotions = [\"hap\", \"sad\", \"neu\", \"ang\", \"sur\", \"dis\", \"fea\"]",
        "detail": "TFace.attribute.M3DFEL.solver",
        "documentation": {}
    },
    {
        "label": "DMIN",
        "kind": 6,
        "importPath": "TFace.attribute.M3DFEL.utils",
        "description": "TFace.attribute.M3DFEL.utils",
        "peekOfCode": "class DMIN(nn.Module):\n    \"\"\"The Dynamic Multi Instance Normalization Module\n    This module dynamicly normlizes the instances and bags\n    Args:\n        optimizer\n        n_iter_per_epoch:\n    \"\"\"\n    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n        super(DMIN, self).__init__()\n        # init the DMIN module",
        "detail": "TFace.attribute.M3DFEL.utils",
        "documentation": {}
    },
    {
        "label": "build_scheduler",
        "kind": 2,
        "importPath": "TFace.attribute.M3DFEL.utils",
        "description": "TFace.attribute.M3DFEL.utils",
        "peekOfCode": "def build_scheduler(args, optimizer, n_iter_per_epoch):\n    \"\"\"Build the scheduler\n    The CosineLRScheduler schedules every iter in every epoch, so it needs n_iter_per_epoch instead of epochs\n    Args:\n        optimizer\n        n_iter_per_epoch: \n    \"\"\"\n    num_steps = int(args.epochs * n_iter_per_epoch)\n    warmup_steps = int(args.warmup_epochs * n_iter_per_epoch)\n    # use the Cosine LR Scheduler for training",
        "detail": "TFace.attribute.M3DFEL.utils",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "TFace.quality.backbone.loss",
        "description": "TFace.quality.backbone.loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    '''\n    focal loss for training recognition model\n    '''\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.ce = torch.nn.CrossEntropyLoss()\n    def forward(self, input, target):",
        "detail": "TFace.quality.backbone.loss",
        "documentation": {}
    },
    {
        "label": "ArcFace",
        "kind": 6,
        "importPath": "TFace.quality.backbone.metric",
        "description": "TFace.quality.backbone.metric",
        "peekOfCode": "class ArcFace(nn.Module):\n    '''\n    ArcFace embedding for training recognition model\n    '''\n    def __init__(self, embedding_size, class_num, s=30.0, m=0.50):\n        \"\"\"ArcFace formula: \n            cos(m + theta) = cos(m)cos(theta) - sin(m)sin(theta)\n        Note that:\n            0 <= m + theta <= Pi\n        So if (m + theta) >= Pi, then theta >= Pi - m. In [0, Pi]",
        "detail": "TFace.quality.backbone.metric",
        "documentation": {}
    },
    {
        "label": "CosFace",
        "kind": 6,
        "importPath": "TFace.quality.backbone.metric",
        "description": "TFace.quality.backbone.metric",
        "peekOfCode": "class CosFace(nn.Module):\n    '''\n    CosFace embedding for training recognition model\n    '''\n    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n        \"\"\"\n        Args:\n            embedding_size: usually 128, 256, 512 ...\n            class_num: num of people when training\n            s: scale, see normface https://arxiv.org/abs/1704.06369",
        "detail": "TFace.quality.backbone.metric",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "peekOfCode": "class Dataset(data.Dataset):\n    '''\n    Build dataset via data list file\n    '''\n    def __init__(self, conf, label=True):\n        super().__init__()\n        # self.data_root = conf.data_root\n        self.img_list = conf.img_list\n        self.transform = conf.transform\n        self.batch_size = conf.batch_size",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "peekOfCode": "def load_data(conf, label=True, train=False):                                     # build dataloder\n    '''\n    Build dataloader \n    Two parameters including \"label\" and \"train\" are used for the output of dataloader\n    '''\n    dataset = Dataset(conf, label)\n    if train:\n        loader = DataLoader(dataset, \n                        batch_size=conf.batch_size, \n                        shuffle=True, ",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.dataset.dataset_txt",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "peekOfCode": "class Flatten(Module):\n    '''\n    This method is to flatten the features\n    '''\n    def forward(self, input):\n        return input.view(input.size(0), -1)\ndef l2_norm(input, axis=1):\n    '''\n    This method is for l2 normalization\n    '''",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "documentation": {}
    },
    {
        "label": "bottleneck_IR",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "peekOfCode": "class bottleneck_IR(Module):\n    def __init__(self, in_channel, depth, stride):\n        '''\n        This method is to initialize IR module\n        '''\n        super(bottleneck_IR, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "peekOfCode": "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\ndef get_block(in_channel, depth, num_units, stride=2):\n    '''\n    This method is to obtain blocks\n    '''\n    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    '''\n    This method is to obtain blocks",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "peekOfCode": "class Backbone(Module):\n    def __init__(self, input_size, num_layers, mode='ir', use_type = \"Rec\"):\n        '''\n        This method is to initialize model\n        if use for quality network, select self.use_type == \"Qua\"\n        if use for recognition network, select self.use_type == \"Rec\"\n        '''\n        super(Backbone, self).__init__()\n        assert input_size[0] in [112, 224], \"input_size should be [112, 112] or [224, 224]\"\n        assert num_layers in [50, 100, 152], \"num_layers should be 50, 100 or 152\"",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "peekOfCode": "def l2_norm(input, axis=1):\n    '''\n    This method is for l2 normalization\n    '''\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n    return output\nclass bottleneck_IR(Module):\n    def __init__(self, in_channel, depth, stride):\n        '''",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "documentation": {}
    },
    {
        "label": "get_block",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "peekOfCode": "def get_block(in_channel, depth, num_units, stride=2):\n    '''\n    This method is to obtain blocks\n    '''\n    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    '''\n    This method is to obtain blocks\n    '''\n    if num_layers == 50:",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "documentation": {}
    },
    {
        "label": "get_blocks",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "peekOfCode": "def get_blocks(num_layers):\n    '''\n    This method is to obtain blocks\n    '''\n    if num_layers == 50:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=4),\n            get_block(in_channel=128, depth=256, num_units=14),\n            get_block(in_channel=256, depth=512, num_units=3)",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "documentation": {}
    },
    {
        "label": "R50",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "peekOfCode": "def R50(input_size, use_type=\"Rec\"):\n    '''\n    This method is to create ResNet50 backbone\n    if use for quality network, select self.use_type == \"Qua\"\n    if use for recognition network, select self.use_type == \"Rec\"\n    '''\n    model = Backbone(input_size, 50, 'ir', use_type=use_type)\n    return model",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "peekOfCode": "class Flatten(Module):\n    '''\n    This method is to flatten the features\n    '''\n    def forward(self, input):\n        return input.view(input.size(0), -1)\nclass Conv_block(Module):\n    '''\n    This method is for convolution block\n    '''",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "documentation": {}
    },
    {
        "label": "Conv_block",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "peekOfCode": "class Conv_block(Module):\n    '''\n    This method is for convolution block\n    '''\n    def __init__(self, in_c, out_c, kernel=(1,1), stride=(1,1), padding=(0,0), groups=1):\n        super(Conv_block, self).__init__()\n        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, \\\n                            stride=stride, padding=padding, bias=False)\n        self.bn = BatchNorm2d(out_c)\n        self.prelu = PReLU(out_c)",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "documentation": {}
    },
    {
        "label": "Linear_block",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "peekOfCode": "class Linear_block(Module):\n    '''\n    This method is for linear block\n    '''\n    def __init__(self, in_c, out_c, kernel=(1,1), stride=(1,1), padding=(0,0), groups=1):\n        super(Linear_block, self).__init__()\n        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, \\\n                            stride=stride, padding=padding, bias=False)\n        self.bn = BatchNorm2d(out_c)\n    def forward(self, x):",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "documentation": {}
    },
    {
        "label": "Depth_Wise",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "peekOfCode": "class Depth_Wise(Module):\n    '''\n    This method is for depth wise\n    '''\n    def __init__(self, in_c, out_c, residual=False, kernel=(3,3), stride=(2,2), padding=(1,1), groups=1):\n        super(Depth_Wise, self).__init__()\n        self.conv = Conv_block(in_c, out_c=groups, kernel=(1,1), padding=(0,0), stride=(1,1))\n        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n        self.project = Linear_block(groups, out_c, kernel=(1,1), padding=(0,0), stride=(1,1))\n        self.residual = residual",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "documentation": {}
    },
    {
        "label": "Residual",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "peekOfCode": "class Residual(Module):\n    '''\n    This method is for residual model\n    '''\n    def __init__(self, c, num_block, groups, kernel=(3,3), stride=(1,1), padding=(1,1)):\n        super(Residual, self).__init__()\n        modules = []\n        for _ in range(num_block):\n            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, \\\n                            padding=padding, stride=stride, groups=groups))",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "documentation": {}
    },
    {
        "label": "GNAP",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "peekOfCode": "class GNAP(Module):\n    '''\n    This method is for GNAP model\n    '''\n    def __init__(self, embedding_size):\n        super(GNAP, self).__init__()\n        if embedding_size < 512:\n            self.conv = Conv_block(512, embedding_size)\n        self.filter = embedding_size\n        self.bn1 = BatchNorm2d(self.filter, affine=False)",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "documentation": {}
    },
    {
        "label": "GDC",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "peekOfCode": "class GDC(Module):\n    '''\n    This method is for GNAP model\n    '''\n    def __init__(self, embedding_size):\n        super(GDC, self).__init__()\n        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1,1), padding=(0,0))\n        self.conv_6_flatten = Flatten()\n        self.linear = Linear(512, embedding_size, bias=False)\n        #self.bn = BatchNorm1d(embedding_size, affine=False)",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "documentation": {}
    },
    {
        "label": "MobileFaceNet",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "peekOfCode": "class MobileFaceNet(Module):\n    def __init__(self, input_size, embedding_size = 512, output_name = \"GNAP\", use_type = \"Rec\"):\n        '''\n        This method is to initialize MobileFaceNet\n        '''\n        super(MobileFaceNet, self).__init__()\n        assert output_name in [\"GNAP\", 'GDC']\n        assert input_size[0] in [112]\n        self.conv1 = Conv_block(3, 64, kernel=(3,3), stride=(2,2), padding=(1,1))\n        self.conv2_dw = Conv_block(64, 64, kernel=(3,3), stride=(1,1), padding=(1,1), groups=64)",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.model.model_mobilefaceNet",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.config_test",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.config_test",
        "peekOfCode": "class Config:\n    # dataset\n    data_root = ''\n    img_list = '../DATA.labelpath'\n    eval_model = './model/MobileFaceNet_MS1M.pth'\n    outfile = '../feats_npy/Embedding_Features.npy'\n    # data preprocess\n    transform = T.Compose([\n        T.Resize((112, 112)),\n        T.ToTensor(),",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.config_test",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.config_test",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.config_test",
        "peekOfCode": "config = Config()",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.config_test",
        "documentation": {}
    },
    {
        "label": "dataSet",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "peekOfCode": "def dataSet():                                                     # Dataset setup\n    '''\n    Dataset setup\n    Bulid a dataloader for training\n    '''                                                \n    dataloader, class_num = load_data_txt(conf, label=False, train=False)\n    return dataloader,class_num\ndef backboneSet():                                                   # Network setup\n    '''\n    Backbone setup",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "documentation": {}
    },
    {
        "label": "backboneSet",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "peekOfCode": "def backboneSet():                                                   # Network setup\n    '''\n    Backbone setup\n    Load a Backbone for training, support MobileFaceNet(MFN) and ResNet50(R50)\n    '''      \n    # MobileFaceNet\n    if conf.backbone == 'MFN':\n        net = model_mobilefaceNet.MobileFaceNet([112,112], conf.embedding_size, \\\n                                output_name = 'GDC', use_type = \"Rec\").to(device)\n    # ResNet50",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "documentation": {}
    },
    {
        "label": "compcos",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "peekOfCode": "def compcos(feats1, feats2):                                         # Computing cosine distance\n    '''\n    Computing cosine distance\n    For similarity\n    '''   \n    cos = np.dot(feats1,feats2)/(np.linalg.norm(feats1)*np.linalg.norm(feats2))\n    return cos\ndef npy2txt(img_list, feats_nplist, outfile):                        # npy to txt for embedding save \n    '''\n    For save embeddings to txt file",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "documentation": {}
    },
    {
        "label": "npy2txt",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "description": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "peekOfCode": "def npy2txt(img_list, feats_nplist, outfile):                        # npy to txt for embedding save \n    '''\n    For save embeddings to txt file\n    '''   \n    allFeats = np.load(feats_nplist)\n    print(np.shape(allFeats))\n    with open(img_list, 'r') as f:\n        for index, value in tqdm(enumerate(f)):\n            imgPath = value.split()[0]\n            feats = allFeats[index]",
        "detail": "TFace.quality.generate_pseudo_labels.extract_embedding.extract_feats",
        "documentation": {}
    },
    {
        "label": "gentxt",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_datalist",
        "description": "TFace.quality.generate_pseudo_labels.gen_datalist",
        "peekOfCode": "def gentxt(data_root, outfile):         # generate data file via traveling the target dataset \n    '''\n    Use ImageFolder method to travel the target dataset \n    Save to two files including \".label\" and \".labelpath\"\n    '''\n    # output file1\n    outfile1 = open(outfile, 'w')\n    # output file2\n    outfile2 = open(outfile+'path', 'w') \n    data = ImageFolder(data_root)",
        "detail": "TFace.quality.generate_pseudo_labels.gen_datalist",
        "documentation": {}
    },
    {
        "label": "buildDict_people",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "description": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "peekOfCode": "def buildDict_people(datalistFile, featsFile):      # Building data dictionary\n    '''\n    This method is to build data dictionary \n    for collecting positive and negative pair similarities\n    '''\n    imgName = []\n    peopleName = []\n    peopleList = set()\n    with open(datalistFile, 'r') as f: datalist = f.readlines()\n    for i in datalist: ",
        "detail": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "documentation": {}
    },
    {
        "label": "getIndex",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "description": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "peekOfCode": "def getIndex(target_value, data_list):       # Search\n    '''\n    Search the index of sample by identity\n    '''   \n    index = [i for i, v in enumerate(data_list) if v == target_value]\n    return index\ndef cos(feats1, feats2):\n    '''\n    Computing cosine distance\n    For similarity",
        "detail": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "documentation": {}
    },
    {
        "label": "cos",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "description": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "peekOfCode": "def cos(feats1, feats2):\n    '''\n    Computing cosine distance\n    For similarity\n    '''   \n    cos = np.dot(feats1, feats2) / (np.linalg.norm(feats1) * np.linalg.norm(feats2))\n    return cos\ndef gen_samepeople_Similarity(featsDict, peopleList, peopleName, feats, outfile_dist_info, fix_num=24):\n    '''\n    This method is to collect positive pair similarities",
        "detail": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "documentation": {}
    },
    {
        "label": "gen_samepeople_Similarity",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "description": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "peekOfCode": "def gen_samepeople_Similarity(featsDict, peopleList, peopleName, feats, outfile_dist_info, fix_num=24):\n    '''\n    This method is to collect positive pair similarities\n    '''\n    print('='*20 + 'GENERATE POSITIVE PAIRS' + '='*20)\n    imgsName = list(featsDict.keys())\n    imgsame_list1 = []\n    imgsame_list2 = []\n    same_similaritys = []\n    onepeople_count = 0",
        "detail": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "documentation": {}
    },
    {
        "label": "gen_diffpeople_Similarity",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "description": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "peekOfCode": "def gen_diffpeople_Similarity(featsDict, peopleList, peopleName, feats, allpospair_nums, outfile_dist_info, fix_num=24):\n    '''\n    This method is to collect negative pair similarities\n    '''\n    print('='*20 + 'GENERATE NEGATIVE PAIRS' + '='*20)\n    imgsName = list(featsDict.keys())\n    imgdiff_list1 = []\n    imgdiff_list2 = []\n    diff_similaritys = []\n    id_similaritys = []",
        "detail": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "documentation": {}
    },
    {
        "label": "gen_distance",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "description": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "peekOfCode": "def gen_distance(pos_similarity_dist, neg_similarity_dist, outfile):\n    '''\n    This method is to calculate quality scores via wasserstein distance\n    '''\n    outfile = open(outfile, 'w')\n    print('='*20 + 'OUTPUT' + '='*20)\n    posimglist = list(pos_similarity_dist.keys())\n    negimglist = list(pos_similarity_dist.keys())\n    assert len(posimglist) == len(negimglist)\n    imglists = posimglist",
        "detail": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "documentation": {}
    },
    {
        "label": "cal_idscore",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "description": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "peekOfCode": "def cal_idscore(result_file):\n    '''\n    This method is to calculate quality scores of identity\n    '''\n    with open(result_file, 'r') as f: txtContent = f.readlines()\n    peoid= []\n    quality_scores = []\n    idsocre_dist = {}\n    for i in txtContent: idsocre_dist[i.split()[0].split('/')[1]] = [0,0]  # init\n    quality_scores = []",
        "detail": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "documentation": {}
    },
    {
        "label": "norm_labels",
        "kind": 2,
        "importPath": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "description": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "peekOfCode": "def norm_labels(data_root, outfile_wdistacne, outfile_result, id_score):\n    '''\n    This method is to normalize quality scores\n    '''\n    outfile_result = open(outfile_result, 'w')\n    with open(outfile_wdistacne, 'r') as f: txtContent = f.readlines()\n    imgpath = []\n    quality_scores = []\n    for i in tqdm(txtContent):\n        imgname = i.split()[0]",
        "detail": "TFace.quality.generate_pseudo_labels.gen_pseudo_labels",
        "documentation": {}
    },
    {
        "label": "read_img",
        "kind": 2,
        "importPath": "TFace.quality.eval",
        "description": "TFace.quality.eval",
        "peekOfCode": "def read_img(imgPath):     # read image & data pre-process\n    data = torch.randn(1, 3, 112, 112)\n    transform = T.Compose([\n        T.Resize((112, 112)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n    ])\n    img = Image.open(imgPath).convert(\"RGB\")\n    data[0, :, :, :] = transform(img)\n    return data",
        "detail": "TFace.quality.eval",
        "documentation": {}
    },
    {
        "label": "network",
        "kind": 2,
        "importPath": "TFace.quality.eval",
        "description": "TFace.quality.eval",
        "peekOfCode": "def network(eval_model, device):\n    net = model.R50([112, 112], use_type=\"Qua\").to(device)\n    net_dict = net.state_dict()     \n    data_dict = {\n        key.replace('module.', ''): value for key, value in torch.load(eval_model, map_location=device).items()}\n    net_dict.update(data_dict)\n    net.load_state_dict(net_dict)\n    net.eval()\n    return net\nif __name__ == \"__main__\":",
        "detail": "TFace.quality.eval",
        "documentation": {}
    },
    {
        "label": "TrainQualityTask",
        "kind": 6,
        "importPath": "TFace.quality.train",
        "description": "TFace.quality.train",
        "peekOfCode": "class TrainQualityTask():\n    \"\"\" TrainTask of quality model\n    \"\"\"\n    def __init__(self, config):\n        super(TrainQualityTask, self).__init__()\n        self.config = config\n    def dataSet(self):\n        # Data Setup\n        trainloader, class_num = load_data_txt(self.config, label=True, train=True)\n        return trainloader",
        "detail": "TFace.quality.train",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "TFace.quality.train_config",
        "description": "TFace.quality.train_config",
        "peekOfCode": "class Config:\n    # dataset\n    img_list = './generate_pseudo_labels/labels/pseudo_quality_score.txt'\n    finetuning_model = './generate_pseudo_labels/extract_embedding/model/MobileFaceNet_MS1M.pth'\n    # save settings\n    checkpoints = \"./checkpoints/MS1M_Quality_Regression/S1\"\n    checkpoints_name = \"MFN\"\n    # data preprocess\n    transform = T.Compose([\n        T.RandomHorizontalFlip(),",
        "detail": "TFace.quality.train_config",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "TFace.quality.train_config",
        "description": "TFace.quality.train_config",
        "peekOfCode": "config = Config()",
        "detail": "TFace.quality.train_config",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.deploy.converter.export_onnx",
        "description": "TFace.recognition.deploy.converter.export_onnx",
        "peekOfCode": "def main():\n    net = get_model(args.model_name)\n    input_size = [112, 112]\n    torch_model = net(input_size)\n    if not os.path.isfile(args.ckpt_path):\n        print(\"Invalid ckpt path: %s\" % args.ckpt_path)\n        return\n    torch_model.load_state_dict(torch.load(args.ckpt_path))\n    torch_model.eval()\n    batch_size = 1",
        "detail": "TFace.recognition.deploy.converter.export_onnx",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "TFace.recognition.deploy.converter.export_onnx",
        "description": "TFace.recognition.deploy.converter.export_onnx",
        "peekOfCode": "parser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument('--ckpt_path', default=None, type=str, required=True, help='')\nparser.add_argument('--onnx_name', default=None, type=str, required=True, help='')\nparser.add_argument('--model_name', default=None, type=str, required=True, help='') \nargs = parser.parse_args()\ndef main():\n    net = get_model(args.model_name)\n    input_size = [112, 112]\n    torch_model = net(input_size)\n    if not os.path.isfile(args.ckpt_path):",
        "detail": "TFace.recognition.deploy.converter.export_onnx",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "TFace.recognition.deploy.converter.export_onnx",
        "description": "TFace.recognition.deploy.converter.export_onnx",
        "peekOfCode": "args = parser.parse_args()\ndef main():\n    net = get_model(args.model_name)\n    input_size = [112, 112]\n    torch_model = net(input_size)\n    if not os.path.isfile(args.ckpt_path):\n        print(\"Invalid ckpt path: %s\" % args.ckpt_path)\n        return\n    torch_model.load_state_dict(torch.load(args.ckpt_path))\n    torch_model.eval()",
        "detail": "TFace.recognition.deploy.converter.export_onnx",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "kind": 2,
        "importPath": "TFace.recognition.deploy.converter.onnx_inference",
        "description": "TFace.recognition.deploy.converter.onnx_inference",
        "peekOfCode": "def l2_norm(x):\n    \"\"\" l2 normalize\n    \"\"\"\n    output = x / np.linalg.norm(x)\n    return output\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\ndef get_test_transform():\n    test_transform = transforms.Compose([    \n            transforms.ToTensor(),    ",
        "detail": "TFace.recognition.deploy.converter.onnx_inference",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "kind": 2,
        "importPath": "TFace.recognition.deploy.converter.onnx_inference",
        "description": "TFace.recognition.deploy.converter.onnx_inference",
        "peekOfCode": "def to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\ndef get_test_transform():\n    test_transform = transforms.Compose([    \n            transforms.ToTensor(),    \n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n    return test_transform\ndef main():\n    img = cv2.imread(\"brucelee.jpg\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)",
        "detail": "TFace.recognition.deploy.converter.onnx_inference",
        "documentation": {}
    },
    {
        "label": "get_test_transform",
        "kind": 2,
        "importPath": "TFace.recognition.deploy.converter.onnx_inference",
        "description": "TFace.recognition.deploy.converter.onnx_inference",
        "peekOfCode": "def get_test_transform():\n    test_transform = transforms.Compose([    \n            transforms.ToTensor(),    \n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n    return test_transform\ndef main():\n    img = cv2.imread(\"brucelee.jpg\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = get_test_transform()(img)\n    img = img.unsqueeze_(0)",
        "detail": "TFace.recognition.deploy.converter.onnx_inference",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.deploy.converter.onnx_inference",
        "description": "TFace.recognition.deploy.converter.onnx_inference",
        "peekOfCode": "def main():\n    img = cv2.imread(\"brucelee.jpg\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = get_test_transform()(img)\n    img = img.unsqueeze_(0)\n    print(img.shape)\n    onnx_path = args.onnx_path\n    session = onnxruntime.InferenceSession(onnx_path)\n    inputs = {session.get_inputs()[0].name: to_numpy(img)}\n    outs = session.run(None, inputs)[0]",
        "detail": "TFace.recognition.deploy.converter.onnx_inference",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "TFace.recognition.deploy.converter.onnx_inference",
        "description": "TFace.recognition.deploy.converter.onnx_inference",
        "peekOfCode": "parser = argparse.ArgumentParser(description='onnx inference')\nparser.add_argument('--onnx_path', default=None, type=str, required=True, help='')\nargs = parser.parse_args()\ndef l2_norm(x):\n    \"\"\" l2 normalize\n    \"\"\"\n    output = x / np.linalg.norm(x)\n    return output\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()",
        "detail": "TFace.recognition.deploy.converter.onnx_inference",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "TFace.recognition.deploy.converter.onnx_inference",
        "description": "TFace.recognition.deploy.converter.onnx_inference",
        "peekOfCode": "args = parser.parse_args()\ndef l2_norm(x):\n    \"\"\" l2 normalize\n    \"\"\"\n    output = x / np.linalg.norm(x)\n    return output\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\ndef get_test_transform():\n    test_transform = transforms.Compose([    ",
        "detail": "TFace.recognition.deploy.converter.onnx_inference",
        "documentation": {}
    },
    {
        "label": "BasicBlockIR",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "class BasicBlockIR(Module):\n    \"\"\" BasicBlock for IRNet\n    \"\"\"\n    def __init__(self, in_channel, depth, stride):\n        super(BasicBlockIR, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(\n                Conv2d(in_channel, depth, (1, 1), stride, bias=False),",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "BottleneckIR",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "class BottleneckIR(Module):\n    \"\"\" BasicBlock with bottleneck for IRNet\n    \"\"\"\n    def __init__(self, in_channel, depth, stride):\n        super(BottleneckIR, self).__init__()\n        reduction_channel = depth // 4\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "BasicBlockIRSE",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "class BasicBlockIRSE(BasicBlockIR):\n    def __init__(self, in_channel, depth, stride):\n        super(BasicBlockIRSE, self).__init__(in_channel, depth, stride)\n        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\nclass BottleneckIRSE(BottleneckIR):\n    def __init__(self, in_channel, depth, stride):\n        super(BottleneckIRSE, self).__init__(in_channel, depth, stride)\n        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\nclass Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "BottleneckIRSE",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "class BottleneckIRSE(BottleneckIR):\n    def __init__(self, in_channel, depth, stride):\n        super(BottleneckIRSE, self).__init__(in_channel, depth, stride)\n        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\nclass Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\ndef get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] +\\\n           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\ndef get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] +\\\n           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "class Backbone(Module):\n    def __init__(self, input_size, num_layers, mode='ir'):\n        \"\"\" Args:\n            input_size: input_size of backbone\n            num_layers: num_layers of backbone\n            mode: support ir or irse\n        \"\"\"\n        super(Backbone, self).__init__()\n        assert input_size[0] in [112, 224], \\\n            \"input_size should be [112, 112] or [224, 224]\"",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "get_block",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] +\\\n           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),\n            get_block(in_channel=128, depth=256, num_units=2),\n            get_block(in_channel=256, depth=512, num_units=2)",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "get_blocks",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),\n            get_block(in_channel=128, depth=256, num_units=2),\n            get_block(in_channel=256, depth=512, num_units=2)\n        ]\n    elif num_layers == 34:\n        blocks = [",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_18",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_18(input_size):\n    \"\"\" Constructs a ir-18 model.\n    \"\"\"\n    model = Backbone(input_size, 18, 'ir')\n    return model\ndef IR_34(input_size):\n    \"\"\" Constructs a ir-34 model.\n    \"\"\"\n    model = Backbone(input_size, 34, 'ir')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_34",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_34(input_size):\n    \"\"\" Constructs a ir-34 model.\n    \"\"\"\n    model = Backbone(input_size, 34, 'ir')\n    return model\ndef IR_50(input_size):\n    \"\"\" Constructs a ir-50 model.\n    \"\"\"\n    model = Backbone(input_size, 50, 'ir')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_50",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_50(input_size):\n    \"\"\" Constructs a ir-50 model.\n    \"\"\"\n    model = Backbone(input_size, 50, 'ir')\n    return model\ndef IR_101(input_size):\n    \"\"\" Constructs a ir-101 model.\n    \"\"\"\n    model = Backbone(input_size, 100, 'ir')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_101",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_101(input_size):\n    \"\"\" Constructs a ir-101 model.\n    \"\"\"\n    model = Backbone(input_size, 100, 'ir')\n    return model\ndef IR_152(input_size):\n    \"\"\" Constructs a ir-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_152",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_152(input_size):\n    \"\"\" Constructs a ir-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir')\n    return model\ndef IR_200(input_size):\n    \"\"\" Constructs a ir-200 model.\n    \"\"\"\n    model = Backbone(input_size, 200, 'ir')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_200",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_200(input_size):\n    \"\"\" Constructs a ir-200 model.\n    \"\"\"\n    model = Backbone(input_size, 200, 'ir')\n    return model\ndef IR_SE_50(input_size):\n    \"\"\" Constructs a ir_se-50 model.\n    \"\"\"\n    model = Backbone(input_size, 50, 'ir_se')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_SE_50",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_SE_50(input_size):\n    \"\"\" Constructs a ir_se-50 model.\n    \"\"\"\n    model = Backbone(input_size, 50, 'ir_se')\n    return model\ndef IR_SE_101(input_size):\n    \"\"\" Constructs a ir_se-101 model.\n    \"\"\"\n    model = Backbone(input_size, 100, 'ir_se')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_SE_101",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_SE_101(input_size):\n    \"\"\" Constructs a ir_se-101 model.\n    \"\"\"\n    model = Backbone(input_size, 100, 'ir_se')\n    return model\ndef IR_SE_152(input_size):\n    \"\"\" Constructs a ir_se-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir_se')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_SE_152",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_SE_152(input_size):\n    \"\"\" Constructs a ir_se-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir_se')\n    return model\ndef IR_SE_200(input_size):\n    \"\"\" Constructs a ir_se-200 model.\n    \"\"\"\n    model = Backbone(input_size, 200, 'ir_se')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "IR_SE_200",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "description": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "peekOfCode": "def IR_SE_200(input_size):\n    \"\"\" Constructs a ir_se-200 model.\n    \"\"\"\n    model = Backbone(input_size, 200, 'ir_se')\n    return model",
        "detail": "TFace.recognition.tasks.dctdp.model_irse_dct",
        "documentation": {}
    },
    {
        "label": "TrainTask",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.dctdp.train",
        "description": "TFace.recognition.tasks.dctdp.train",
        "peekOfCode": "class TrainTask(BaseTask):\n    \"\"\" TrainTask in distfc mode, which means classifier shards into multi workers\n    \"\"\"\n    def __init__(self, cfg_file):\n        super(TrainTask, self).__init__(cfg_file)\n    def make_model(self):\n        \"\"\" build training backbone and heads\n        \"\"\"\n        backbone_name = self.cfg['BACKBONE_NAME']\n        backbone_model = get_model(backbone_name)",
        "detail": "TFace.recognition.tasks.dctdp.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.train",
        "description": "TFace.recognition.tasks.dctdp.train",
        "peekOfCode": "def main():\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    task = TrainTask(os.path.join(task_dir, 'train.yaml'))\n    task.init_env()\n    task.train()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.recognition.tasks.dctdp.train",
        "documentation": {}
    },
    {
        "label": "NoisyActivation",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.dctdp.utils",
        "description": "TFace.recognition.tasks.dctdp.utils",
        "peekOfCode": "class NoisyActivation(nn.Module):\n    def __init__(self, input_shape=112, budget_mean=4, sensitivity=None):\n        super(NoisyActivation, self).__init__()\n        self.h, self.w = input_shape, input_shape\n        if sensitivity is None:\n            sensitivity = torch.ones([189, self.h, self.w]).cuda()\n        self.sensitivity = sensitivity.reshape(189 * self.h * self.w)\n        self.given_locs = torch.zeros((189, self.h, self.w))\n        size = self.given_locs.shape\n        self.budget = budget_mean * 189 * self.h * self.w",
        "detail": "TFace.recognition.tasks.dctdp.utils",
        "documentation": {}
    },
    {
        "label": "images_to_batch",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.dctdp.utils",
        "description": "TFace.recognition.tasks.dctdp.utils",
        "peekOfCode": "def images_to_batch(x):\n    x = (x + 1) / 2 * 255\n    x = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)\n    if x.shape[1] != 3:\n        print(\"Wrong input, Channel should equals to 3\")\n        return\n    x = dct.to_ycbcr(x)  # comvert RGB to YCBCR\n    x -= 128\n    bs, ch, h, w = x.shape\n    block_num = h // 8",
        "detail": "TFace.recognition.tasks.dctdp.utils",
        "documentation": {}
    },
    {
        "label": "make_interactive_models",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.duetface.base_task",
        "description": "TFace.recognition.tasks.duetface.base_task",
        "peekOfCode": "def make_interactive_models(self):\n    sub_channels = self.cfg['SUB_CHS']\n    main_feature_size = self.cfg['FEATURE_SIZE']\n    sub_feature_size = self.cfg['SUB_FEATURE_SIZE']\n    num_sub_channels = len(sub_channels) * 3\n    self.backbone = DuetFaceModel(num_sub_channels, main_feature_size, sub_feature_size,\n                                  main_model_name=self.cfg['BACKBONE_NAME'],\n                                  sub_model_name=self.cfg['SUB_BACKBONE_NAME'])\n    # load pre-trained client-side model from a given checkpoint\n    if self.cfg['LOAD_CKPT']:",
        "detail": "TFace.recognition.tasks.duetface.base_task",
        "documentation": {}
    },
    {
        "label": "InteractiveBlock",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.duetface_model",
        "description": "TFace.recognition.tasks.duetface.duetface_model",
        "peekOfCode": "class InteractiveBlock(nn.Module):\n    def __init__(self):\n        super(InteractiveBlock, self).__init__()\n        self.activation = nn.Sigmoid()\n        self.interface = None\n        self.weight = nn.Parameter(torch.tensor(0.))\n    def forward(self, x):\n        def reshape_and_normalize_masks(inputs, to_bool=False, squeeze=True):\n            if squeeze:\n                mask = inputs.mean(1)",
        "detail": "TFace.recognition.tasks.duetface.duetface_model",
        "documentation": {}
    },
    {
        "label": "DuetFaceModel",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.duetface_model",
        "description": "TFace.recognition.tasks.duetface.duetface_model",
        "peekOfCode": "class DuetFaceModel(nn.Module):\n    def __init__(self, num_sub_channels, len_features, len_sub_features, main_model_name='IR_18',\n                 sub_model_name='MobileFaceNet'):\n        super(DuetFaceModel, self).__init__()\n        if main_model_name == 'IR_18':\n            model_size = 18\n        else:\n            model_size = 50\n        # reshape in and output, feature length, and override the server-side unit module\n        self.server_model = ServerBackbone([112, 112], model_size, 192 - num_sub_channels, len_features,",
        "detail": "TFace.recognition.tasks.duetface.duetface_model",
        "documentation": {}
    },
    {
        "label": "DuetFaceBasicBlock",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.duetface_model",
        "description": "TFace.recognition.tasks.duetface.duetface_model",
        "peekOfCode": "class DuetFaceBasicBlock(nn.Module):\n    \"\"\" BasicBlock for IRNet\n    \"\"\"\n    def __init__(self, in_channel, depth, stride, feature_channel, kernel_size, stage=0, embedding=False):\n        super(DuetFaceBasicBlock, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = nn.MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, depth, (1, 1), stride, bias=False),",
        "detail": "TFace.recognition.tasks.duetface.duetface_model",
        "documentation": {}
    },
    {
        "label": "get_activation",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.duetface.duetface_model",
        "description": "TFace.recognition.tasks.duetface.duetface_model",
        "peekOfCode": "def get_activation(name):\n    def hook(model, input, output):\n        # detach, otherwise the activation will not be removed during backward\n        activation[name] = output.detach()\n    return hook\nclass InteractiveBlock(nn.Module):\n    def __init__(self):\n        super(InteractiveBlock, self).__init__()\n        self.activation = nn.Sigmoid()\n        self.interface = None",
        "detail": "TFace.recognition.tasks.duetface.duetface_model",
        "documentation": {}
    },
    {
        "label": "activation",
        "kind": 5,
        "importPath": "TFace.recognition.tasks.duetface.duetface_model",
        "description": "TFace.recognition.tasks.duetface.duetface_model",
        "peekOfCode": "activation = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        # detach, otherwise the activation will not be removed during backward\n        activation[name] = output.detach()\n    return hook\nclass InteractiveBlock(nn.Module):\n    def __init__(self):\n        super(InteractiveBlock, self).__init__()\n        self.activation = nn.Sigmoid()",
        "detail": "TFace.recognition.tasks.duetface.duetface_model",
        "documentation": {}
    },
    {
        "label": "IntConv",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.local_backbones",
        "description": "TFace.recognition.tasks.duetface.local_backbones",
        "peekOfCode": "class IntConv(nn.Module):\n    def __init__(self, channels_in, channels_out, feature_channels_in, kernel_size):\n        super(IntConv, self).__init__()\n        self.channels_out = channels_out\n        self.channels_in = channels_in\n        self.kernel_size = kernel_size\n        self.features_channels_in = feature_channels_in\n        self.kernel = nn.Sequential(\n            nn.Linear(self.features_channels_in, self.features_channels_in, bias=False),\n            nn.LeakyReLU(0.1, True),",
        "detail": "TFace.recognition.tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "BasicBlockIntIR",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.local_backbones",
        "description": "TFace.recognition.tasks.duetface.local_backbones",
        "peekOfCode": "class BasicBlockIntIR(nn.Module):\n    \"\"\" BasicBlock for IRNet\n    \"\"\"\n    def __init__(self, in_channel, depth, stride, feature_channel, kernel_size, stage=0, embedding=False):\n        super(BasicBlockIntIR, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = nn.MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, depth, (1, 1), stride, bias=False),",
        "detail": "TFace.recognition.tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.local_backbones",
        "description": "TFace.recognition.tasks.duetface.local_backbones",
        "peekOfCode": "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\ndef get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] + \\\n           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),",
        "detail": "TFace.recognition.tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "ClientBackbone",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.local_backbones",
        "description": "TFace.recognition.tasks.duetface.local_backbones",
        "peekOfCode": "class ClientBackbone(nn.Module):\n    def __init__(self, channels_in, channels_out, client_backbone_name='MobileFaceNet'):\n        super(ClientBackbone, self).__init__()\n        # sub_backbone_name = 'IR_18'\n        self.client_backbone = get_model(client_backbone_name)\n        self.client_backbone = self.client_backbone([112, 112])\n        if client_backbone_name == 'MobileFaceNet':\n            self.client_backbone = self._adjust_client_backbone_model(self.client_backbone, channels_in, channels_out)\n        if client_backbone_name == 'IR_18':\n            # deprecated",
        "detail": "TFace.recognition.tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "ServerBackbone",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.local_backbones",
        "description": "TFace.recognition.tasks.duetface.local_backbones",
        "peekOfCode": "class ServerBackbone(nn.Module):\n    def __init__(self, input_size, num_layers, in_channels, out_channels, feature_channels, kernel_size,\n                 unit_module=None):\n        super(ServerBackbone, self).__init__()\n        assert input_size[0] in [112, 224], \\\n            \"input_size should be [112, 112] or [224, 224]\"\n        assert num_layers in [18, 34, 50, 100, 152, 200], \\\n            \"num_layers should be 18, 34, 50, 100 or 152\"\n        self.input_layer = nn.Sequential(nn.Conv2d(in_channels, 64, (3, 3), 1, 1, bias=False),\n                                         nn.BatchNorm2d(64), nn.PReLU(64))",
        "detail": "TFace.recognition.tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "get_block",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.duetface.local_backbones",
        "description": "TFace.recognition.tasks.duetface.local_backbones",
        "peekOfCode": "def get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] + \\\n           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),\n            get_block(in_channel=128, depth=256, num_units=2),\n            get_block(in_channel=256, depth=512, num_units=2)",
        "detail": "TFace.recognition.tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "get_blocks",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.duetface.local_backbones",
        "description": "TFace.recognition.tasks.duetface.local_backbones",
        "peekOfCode": "def get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),\n            get_block(in_channel=128, depth=256, num_units=2),\n            get_block(in_channel=256, depth=512, num_units=2)\n        ]\n    elif num_layers == 34:\n        blocks = [",
        "detail": "TFace.recognition.tasks.duetface.local_backbones",
        "documentation": {}
    },
    {
        "label": "InvertedResidual",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.pfld_model",
        "description": "TFace.recognition.tasks.duetface.pfld_model",
        "peekOfCode": "class InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, use_res_connect, expand_ratio=6):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        self.use_res_connect = use_res_connect\n        self.conv = nn.Sequential(\n            nn.Conv2d(inp, inp * expand_ratio, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(inp * expand_ratio),\n            nn.ReLU(inplace=True),",
        "detail": "TFace.recognition.tasks.duetface.pfld_model",
        "documentation": {}
    },
    {
        "label": "PFLDInference",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.pfld_model",
        "description": "TFace.recognition.tasks.duetface.pfld_model",
        "peekOfCode": "class PFLDInference(nn.Module):\n    def __init__(self):\n        super(PFLDInference, self).__init__()\n        self.conv1 = nn.Conv2d(3,\n                               64,\n                               kernel_size=3,\n                               stride=2,\n                               padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)",
        "detail": "TFace.recognition.tasks.duetface.pfld_model",
        "documentation": {}
    },
    {
        "label": "AuxiliaryNet",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.pfld_model",
        "description": "TFace.recognition.tasks.duetface.pfld_model",
        "peekOfCode": "class AuxiliaryNet(nn.Module):\n    def __init__(self):\n        super(AuxiliaryNet, self).__init__()\n        self.conv1 = conv_bn(64, 128, 3, 2)\n        self.conv2 = conv_bn(128, 128, 3, 1)\n        self.conv3 = conv_bn(128, 32, 3, 2)\n        self.conv4 = conv_bn(32, 128, 7, 1)\n        self.max_pool1 = nn.MaxPool2d(3)\n        self.fc1 = nn.Linear(128, 32)\n        self.fc2 = nn.Linear(32, 3)",
        "detail": "TFace.recognition.tasks.duetface.pfld_model",
        "documentation": {}
    },
    {
        "label": "conv_bn",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.duetface.pfld_model",
        "description": "TFace.recognition.tasks.duetface.pfld_model",
        "peekOfCode": "def conv_bn(inp, oup, kernel, stride, padding=1):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, kernel, stride, padding, bias=False),\n        nn.BatchNorm2d(oup), nn.ReLU(inplace=True))\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, use_res_connect, expand_ratio=6):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        self.use_res_connect = use_res_connect",
        "detail": "TFace.recognition.tasks.duetface.pfld_model",
        "documentation": {}
    },
    {
        "label": "TrainTask",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.duetface.train",
        "description": "TFace.recognition.tasks.duetface.train",
        "peekOfCode": "class TrainTask(BaseTask):\n    \"\"\" TrainTask in distfc mode, which means classifier shards into multi workers\n    \"\"\"\n    def __init__(self, cfg_file):\n        super(TrainTask, self).__init__(cfg_file)\n    def loop_step(self, epoch):\n        backbone, heads = self.backbone, list(self.heads.values())\n        backbone.train()  # set to training mode\n        if self.cfg['MODE'] == 'INT' and self.cfg['LOAD_CKPT']:\n            # freeze client-side model and facial landmark detector",
        "detail": "TFace.recognition.tasks.duetface.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.duetface.train",
        "description": "TFace.recognition.tasks.duetface.train",
        "peekOfCode": "def main():\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    task = TrainTask(os.path.join(task_dir, 'train_dct.yaml'))\n    task.init_env()\n    task.train()\ndef _images_to_dct(x, sub_channels=None, size=8, stride=8, pad=0, dilation=1):\n    x = x * 0.5 + 0.5  # x to [0, 1]\n    x = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)\n    x *= 255\n    if x.shape[1] == 3:",
        "detail": "TFace.recognition.tasks.duetface.train",
        "documentation": {}
    },
    {
        "label": "CC",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.distillation.cc",
        "description": "TFace.recognition.tasks.ekd.distillation.cc",
        "peekOfCode": "class CC(nn.Module):\n    \"\"\" Correlation Congruence for Knowledge Distillation. ICCV 2019\n    \"\"\"\n    def __init__(self, gamma=0.4, p=2):\n        super().__init__()\n        self.gamma = gamma\n        self.p = p\n    def forward(self, feat_s, feat_t):\n        corr_mat_s = self.get_correlation_matrix(feat_s)\n        corr_mat_t = self.get_correlation_matrix(feat_t)",
        "detail": "TFace.recognition.tasks.ekd.distillation.cc",
        "documentation": {}
    },
    {
        "label": "CosineDistance",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.distillation.cosine_distance",
        "description": "TFace.recognition.tasks.ekd.distillation.cosine_distance",
        "peekOfCode": "class CosineDistance(nn.Module):\n    \"\"\" Feature-Feature cosine distance\n    \"\"\"\n    def __init__(self):\n        super(CosineDistance, self).__init__()\n    def forward(self, f_s, f_t):\n        batch_size = f_s.size(0)\n        # just to make sure the tensor is 2D\n        f_s = f_s.view(batch_size, -1)\n        f_t = f_t.view(batch_size, -1)",
        "detail": "TFace.recognition.tasks.ekd.distillation.cosine_distance",
        "documentation": {}
    },
    {
        "label": "HardDarkRank",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.distillation.darkrank",
        "description": "TFace.recognition.tasks.ekd.distillation.darkrank",
        "peekOfCode": "class HardDarkRank(nn.Module):\n    \"\"\" DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer. AAAI 2018\n    \"\"\"\n    def __init__(self, alpha=3, beta=3, permute_len=3):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.permute_len = permute_len\n    def forward(self, g_s, g_t):\n        g_s = F.normalize(g_s)",
        "detail": "TFace.recognition.tasks.ekd.distillation.darkrank",
        "documentation": {}
    },
    {
        "label": "euclidean_dist",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.ekd.distillation.darkrank",
        "description": "TFace.recognition.tasks.ekd.distillation.darkrank",
        "peekOfCode": "def euclidean_dist(x, y):\n    \"\"\"\n    Args:\n        x: pytorch Variable, with shape [m, d]\n        y: pytorch Variable, with shape [n, d]\n    Returns:\n        dist: pytorch Variable, with shape [m, n]\n    \"\"\"\n    m, n = x.size(0), y.size(0)\n    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)",
        "detail": "TFace.recognition.tasks.ekd.distillation.darkrank",
        "documentation": {}
    },
    {
        "label": "EKD",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.distillation.ekd",
        "description": "TFace.recognition.tasks.ekd.distillation.ekd",
        "peekOfCode": "class EKD(nn.Module):\n    \"\"\" Evaluation-oriented knowledge distillation for deep face recognition, CVPR2022\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.topk = 2000\n        self.t = 0.01\n        self.anchor = [10, 100, 1000, 10000, 100000, 1000000]\n        self.momentum = 0.01\n        self.register_buffer('s_anchor', torch.zeros(len(self.anchor)))",
        "detail": "TFace.recognition.tasks.ekd.distillation.ekd",
        "documentation": {}
    },
    {
        "label": "HintLoss",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.distillation.fitnet",
        "description": "TFace.recognition.tasks.ekd.distillation.fitnet",
        "peekOfCode": "class HintLoss(nn.Module):\n    \"\"\" Fitnets: hints for thin deep nets, ICLR 2015\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.crit = nn.MSELoss()\n    def forward(self, f_s, f_t):\n        loss = self.crit(f_s, f_t)\n        return loss",
        "detail": "TFace.recognition.tasks.ekd.distillation.fitnet",
        "documentation": {}
    },
    {
        "label": "RKDLoss",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.distillation.rkd",
        "description": "TFace.recognition.tasks.ekd.distillation.rkd",
        "peekOfCode": "class RKDLoss(nn.Module):\n    \"\"\" Relational Knowledge Disitllation, CVPR2019\n    \"\"\"\n    def __init__(self, w_d=25, w_a=50):\n        super().__init__()\n        self.w_d = w_d\n        self.w_a = w_a\n    def forward(self, f_s, f_t):\n        student = f_s.view(f_s.shape[0], -1)\n        teacher = f_t.view(f_t.shape[0], -1)",
        "detail": "TFace.recognition.tasks.ekd.distillation.rkd",
        "documentation": {}
    },
    {
        "label": "SimPerserving",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.distillation.similarity_perserving",
        "description": "TFace.recognition.tasks.ekd.distillation.similarity_perserving",
        "peekOfCode": "class SimPerserving(nn.Module):\n    \"\"\" Similarity-Preserving Knowledge Distillation, ICCV2019\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n    def forward(self, g_s, g_t):\n        return self.similarity_loss(g_s, g_t)\n    # different from the origin paper, here use the embedding feature\n    def similarity_loss(self, f_s, f_t):\n        batch_size = f_s.size(0)",
        "detail": "TFace.recognition.tasks.ekd.distillation.similarity_perserving",
        "documentation": {}
    },
    {
        "label": "DistillKL",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.distillation.simple_kd",
        "description": "TFace.recognition.tasks.ekd.distillation.simple_kd",
        "peekOfCode": "class DistillKL(nn.Module):\n    \"\"\" Distilling the Knowledge in a Neural Network, NIPSW 2015\n    \"\"\"\n    def __init__(self, t=4):\n        super().__init__()\n        self.t = t\n    def forward(self, y_s, y_t):\n        p_s = F.log_softmax(y_s/self.t, dim=1)\n        p_t = F.softmax(y_t/self.t, dim=1)\n        loss = F.kl_div(p_s, p_t, size_average=False) * (self.t**2) / y_s.shape[0]",
        "detail": "TFace.recognition.tasks.ekd.distillation.simple_kd",
        "documentation": {}
    },
    {
        "label": "BalancedBatchSampler",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.dataset",
        "description": "TFace.recognition.tasks.ekd.dataset",
        "peekOfCode": "class BalancedBatchSampler(Sampler):\n    \"\"\" BalancedBatchSampler class\n        Each batch includes `general_batch_size` general samples and `balanced_batch_size` balanced samples,\n        general samples are directly randomly sampled, But balanced samples means picking `subsample_size`\n        samples for each label.\n    \"\"\"\n    def __init__(self, labels2index, general_batch_size,\n                 balanced_batch_size, world_size,\n                 total_size, subsample_size=4):\n        self.general_batch_size = general_batch_size",
        "detail": "TFace.recognition.tasks.ekd.dataset",
        "documentation": {}
    },
    {
        "label": "create_label2index",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.ekd.dataset",
        "description": "TFace.recognition.tasks.ekd.dataset",
        "peekOfCode": "def create_label2index(dataset):\n    \"\"\" create label2index map for BalancedBatchSampler,\n        dataset is a SingleDataset object\n    \"\"\"\n    label2index = defaultdict(list)\n    for i, sample in enumerate(dataset.inputs):\n        label = sample[-1]\n        label2index[label].append(i)\n    return label2index\nclass BalancedBatchSampler(Sampler):",
        "detail": "TFace.recognition.tasks.ekd.dataset",
        "documentation": {}
    },
    {
        "label": "TrainTask",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.ekd.train",
        "description": "TFace.recognition.tasks.ekd.train",
        "peekOfCode": "class TrainTask(BaseTask):\n    \"\"\" Knowledge distillation TrainTask\n    \"\"\"\n    def __init__(self, cfg_file):\n        super(TrainTask, self).__init__(cfg_file)\n    def loop_step(self, epoch):\n        \"\"\"\n        load_data\n            |\n        extract feature",
        "detail": "TFace.recognition.tasks.ekd.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.ekd.train",
        "description": "TFace.recognition.tasks.ekd.train",
        "peekOfCode": "def main():\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    task = TrainTask(os.path.join(task_dir, 'train.yaml'))\n    task.init_env()\n    task.train()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.recognition.tasks.ekd.train",
        "documentation": {}
    },
    {
        "label": "MinusGenerativeModel",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.minusface.minusface",
        "description": "TFace.recognition.tasks.minusface.minusface",
        "peekOfCode": "class MinusGenerativeModel(nn.Module):\n    def __init__(self, mode='stage1', backbone=UNet):\n        super().__init__()\n        self.mode = mode\n        # by default, we use a U-Net as generator\n        # it may be replaced with other network architectures\n        if self.mode == 'toy':\n            self.backbone = backbone(3, 3)\n        else:\n            self.backbone = backbone(192, 192)",
        "detail": "TFace.recognition.tasks.minusface.minusface",
        "documentation": {}
    },
    {
        "label": "MinusBackbone",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.minusface.minusface",
        "description": "TFace.recognition.tasks.minusface.minusface",
        "peekOfCode": "class MinusBackbone(nn.Module):\n    def __init__(self, mode='stage1', n_duplicate=1, generator=None, recognizer=None):\n        super().__init__()\n        self.mode = mode\n        # by default, we produce X_p as (3, 112, 112)\n        # X_p can be enlarged by setting the following n_duplicate to other than 1\n        # it equals sample X_p n_duplicate times independently, then concatenate them by channel dimension\n        # we experimentally find this practice marginally enhances accuracy (as recognizable features increases)\n        # at the cost of some increased transmission overhead\n        self.n_duplicate = n_duplicate",
        "detail": "TFace.recognition.tasks.minusface.minusface",
        "documentation": {}
    },
    {
        "label": "KLSparseLoss",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.minusface.minusface",
        "description": "TFace.recognition.tasks.minusface.minusface",
        "peekOfCode": "class KLSparseLoss(nn.Module):\n    def __init__(self, sparsity_target=0.05):\n        super().__init__()\n        self.sparsity_target = sparsity_target\n    @staticmethod\n    def kl_divergence(p, q):\n        q = torch.clamp(q, 1e-8, 1 - 1e-8)\n        s1 = torch.sum(p * torch.log(p / q))\n        s2 = torch.sum((1 - p) * torch.log((1 - p) / (1 - q)))\n        return s1 + s2",
        "detail": "TFace.recognition.tasks.minusface.minusface",
        "documentation": {}
    },
    {
        "label": "MinusLoss",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.minusface.minusface",
        "description": "TFace.recognition.tasks.minusface.minusface",
        "peekOfCode": "class MinusLoss(nn.Module):\n    def __init__(self, mode='stage1', weights=None):\n        super().__init__()\n        self.mode = mode\n        self.generative_loss = torch.nn.L1Loss()\n        self.recognition_loss = get_loss('DistCrossEntropy')\n        # by default, the sparsity loss is not used (weight set to 0)\n        # it was a KL sparse loss designed to encourage the sparsity of generator's latents\n        # we find it could occasionally benefit the quality of residues by setting to a small value (e.g., 1e-3)\n        # it is however not used for our paper's results",
        "detail": "TFace.recognition.tasks.minusface.minusface",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.minusface.train",
        "description": "TFace.recognition.tasks.minusface.train",
        "peekOfCode": "def main():\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    task = TrainTask(os.path.join(task_dir, 'train.yaml'))\n    task.init_env()\n    task.train()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.recognition.tasks.minusface.train",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.minusface.utils",
        "description": "TFace.recognition.tasks.minusface.utils",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        conv_relu = []\n        conv_relu.append(nn.Conv2d(in_channels=in_channels, out_channels=middle_channels,\n                                   kernel_size=3, padding=1, stride=1))\n        conv_relu.append(nn.ReLU())\n        conv_relu.append(nn.Conv2d(in_channels=middle_channels, out_channels=out_channels,\n                                   kernel_size=3, padding=1, stride=1))\n        conv_relu.append(nn.ReLU())",
        "detail": "TFace.recognition.tasks.minusface.utils",
        "documentation": {}
    },
    {
        "label": "UNet",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.minusface.utils",
        "description": "TFace.recognition.tasks.minusface.utils",
        "peekOfCode": "class UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3):\n        super().__init__()\n        self.left_conv_1 = ConvBlock(in_channels=in_channels, middle_channels=64, out_channels=64)\n        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.left_conv_2 = ConvBlock(in_channels=64, middle_channels=128, out_channels=128)\n        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.left_conv_3 = ConvBlock(in_channels=128, middle_channels=256, out_channels=256)\n        self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.left_conv_4 = ConvBlock(in_channels=256, middle_channels=512, out_channels=512)",
        "detail": "TFace.recognition.tasks.minusface.utils",
        "documentation": {}
    },
    {
        "label": "AugmentedMultiDataset",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.partialface.base_task",
        "description": "TFace.recognition.tasks.partialface.base_task",
        "peekOfCode": "class AugmentedMultiDataset(MultiDataset):\n    def __init__(self, data_root, index_root, names, transform, num_aug, **kwargs) -> None:\n        super().__init__(data_root, index_root, names, transform, **kwargs)\n        self.num_aug = num_aug\n    def _build_inputs(self, world_size=None, rank=None):\n        \"\"\" Read index file and saved in ``self.inputs``\n            If ``self.is_shard`` is True, ``total_sample_nums`` > ``sample_nums``\n        \"\"\"\n        for i, name in enumerate(self.names):\n            index_file = os.path.join(self.index_root, name + \".txt\")",
        "detail": "TFace.recognition.tasks.partialface.base_task",
        "documentation": {}
    },
    {
        "label": "LocalBaseTask",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.partialface.base_task",
        "description": "TFace.recognition.tasks.partialface.base_task",
        "peekOfCode": "class LocalBaseTask(BaseTask):\n    def __init__(self, cfg_file):\n        super().__init__(cfg_file=cfg_file)\n        # if self.cfg['METHOD'] == 'PartialFace':\n        #     self.num_aug = self.cfg['NUM_AUG']\n        #     self.num_chs = self.cfg['NUM_CHS']\n    def make_inputs(self):\n        \"\"\" make datasets\n        \"\"\"\n        rgb_mean = self.cfg['RGB_MEAN']",
        "detail": "TFace.recognition.tasks.partialface.base_task",
        "documentation": {}
    },
    {
        "label": "TrainTask",
        "kind": 6,
        "importPath": "TFace.recognition.tasks.partialface.train",
        "description": "TFace.recognition.tasks.partialface.train",
        "peekOfCode": "class TrainTask(LocalBaseTask):\n    \"\"\" TrainTask in distfc mode, which means classifier shards into multi workers\n    \"\"\"\n    def __init__(self, cfg_file):\n        super(TrainTask, self).__init__(cfg_file)\n    def gather_from_branch(self, features, grad=True):\n        if not grad:\n            with torch.no_grad():\n                features_gather = AllGather(features, self.world_size)\n        else:",
        "detail": "TFace.recognition.tasks.partialface.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.partialface.train",
        "description": "TFace.recognition.tasks.partialface.train",
        "peekOfCode": "def main():\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    task = TrainTask(os.path.join(task_dir, 'train.yaml'))\n    task.init_env()\n    task.train()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.recognition.tasks.partialface.train",
        "documentation": {}
    },
    {
        "label": "dct_transform",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.partialface.utils",
        "description": "TFace.recognition.tasks.partialface.utils",
        "peekOfCode": "def dct_transform(x, chs_remove=None, chs_pad=False,\n                  size=8, stride=8, pad=0, dilation=1, ratio=8):\n    \"\"\"\n        Transform a spatial image into its frequency channels.\n        Prune low-frequency channels if necessary.\n    \"\"\"\n    # assert x is a (3, H, W) RGB image\n    assert x.shape[1] == 3\n    # convert the spatial image's range into [0, 1], recommended by TorchJPEG\n    x = x * 0.5 + 0.5",
        "detail": "TFace.recognition.tasks.partialface.utils",
        "documentation": {}
    },
    {
        "label": "idct_transform",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.partialface.utils",
        "description": "TFace.recognition.tasks.partialface.utils",
        "peekOfCode": "def idct_transform(x, size=8, stride=8, pad=0, dilation=1, ratio=8):\n    \"\"\"\n        The inverse of DCT transform.\n        Transform frequency channels (must be 192 channels, can be padded with 0) back to the spatial image.\n    \"\"\"\n    b, _, h, w = x.shape\n    x = x.view(b, 3, 64, h, w)\n    x = x.permute(0, 1, 3, 4, 2)\n    x = x.view(b, 3, h * w, 8, 8)\n    x = dct.block_idct(x)",
        "detail": "TFace.recognition.tasks.partialface.utils",
        "documentation": {}
    },
    {
        "label": "create_channel_subsets",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.partialface.utils",
        "description": "TFace.recognition.tasks.partialface.utils",
        "peekOfCode": "def create_channel_subsets(x, chs_prune=None, selection=None, n_subset=6):\n    \"\"\"\n        Turn spatial images into subsets of frequency channels\n    \"\"\"\n    # low-frequency channels to prune\n    if chs_prune is None:\n        chs_prune = [0, 1, 2, 3, 8, 9, 10, 16, 17, 24]\n    # pre-specified combinations for subset selection (bold S in our paper)\n    #   wlog., we split high-frequency channels into 6 subsets, each containing s=9 channels\n    #   the specific combination can be randomly and model-wisely generated",
        "detail": "TFace.recognition.tasks.partialface.utils",
        "documentation": {}
    },
    {
        "label": "form_training_batch",
        "kind": 2,
        "importPath": "TFace.recognition.tasks.partialface.utils",
        "description": "TFace.recognition.tasks.partialface.utils",
        "peekOfCode": "def form_training_batch(inputs, labels):\n    \"\"\"\n        Create the training batch of random subsets of frequency channels\n        from the standard inputs of spatial images\n    \"\"\"\n    b, _, _, _ = inputs.shape\n    # this specifies the image-wise potential choices of subsets based on the subset's index\n    #   wlog., we select 3 out of 6 subsets for each sample image to form the training dataset\n    #   this ensures a secure training, as the model cannot access all subsets of any given image\n    #   len(choice_index) = C(3,6) = 20",
        "detail": "TFace.recognition.tasks.partialface.utils",
        "documentation": {}
    },
    {
        "label": "read_template_media_list",
        "kind": 2,
        "importPath": "TFace.recognition.test.IJB_Evaluation",
        "description": "TFace.recognition.test.IJB_Evaluation",
        "peekOfCode": "def read_template_media_list(path):\n    # ijb_meta = np.loadtxt(path, dtype=str)\n    ijb_meta = pd.read_csv(path, sep=' ', header=None).values\n    templates = ijb_meta[:, 1].astype(np.int)\n    medias = ijb_meta[:, 2].astype(np.int)\n    return templates, medias\ndef read_template_pair_list(path):\n    # pairs = np.loadtxt(path, dtype=int)\n    pairs = pd.read_csv(path, sep=' ', header=None).values\n    t1 = pairs[:, 0].astype(np.int)",
        "detail": "TFace.recognition.test.IJB_Evaluation",
        "documentation": {}
    },
    {
        "label": "read_template_pair_list",
        "kind": 2,
        "importPath": "TFace.recognition.test.IJB_Evaluation",
        "description": "TFace.recognition.test.IJB_Evaluation",
        "peekOfCode": "def read_template_pair_list(path):\n    # pairs = np.loadtxt(path, dtype=int)\n    pairs = pd.read_csv(path, sep=' ', header=None).values\n    t1 = pairs[:, 0].astype(np.int)\n    t2 = pairs[:, 1].astype(np.int)\n    label = pairs[:, 2].astype(np.int)\n    return t1, t2, label\ndef get_image_feature(feature_path, faceness_path):\n    img_feats = np.load(feature_path)\n    faceness_scores = np.load(faceness_path)",
        "detail": "TFace.recognition.test.IJB_Evaluation",
        "documentation": {}
    },
    {
        "label": "get_image_feature",
        "kind": 2,
        "importPath": "TFace.recognition.test.IJB_Evaluation",
        "description": "TFace.recognition.test.IJB_Evaluation",
        "peekOfCode": "def get_image_feature(feature_path, faceness_path):\n    img_feats = np.load(feature_path)\n    faceness_scores = np.load(faceness_path)\n    return img_feats, faceness_scores\ndef image2template_feature(img_feats=None, templates=None, medias=None):\n    # ==========================================================\n    # 1. face image feature l2 normalization. img_feats:[number_image x feats_dim]\n    # 2. compute media feature.\n    # 3. compute template feature.\n    # ==========================================================",
        "detail": "TFace.recognition.test.IJB_Evaluation",
        "documentation": {}
    },
    {
        "label": "image2template_feature",
        "kind": 2,
        "importPath": "TFace.recognition.test.IJB_Evaluation",
        "description": "TFace.recognition.test.IJB_Evaluation",
        "peekOfCode": "def image2template_feature(img_feats=None, templates=None, medias=None):\n    # ==========================================================\n    # 1. face image feature l2 normalization. img_feats:[number_image x feats_dim]\n    # 2. compute media feature.\n    # 3. compute template feature.\n    # ==========================================================\n    unique_templates = np.unique(templates)\n    template_feats = np.zeros((len(unique_templates), img_feats.shape[1]))\n    for count_template, uqt in enumerate(unique_templates):\n        (ind_t,) = np.where(templates == uqt)",
        "detail": "TFace.recognition.test.IJB_Evaluation",
        "documentation": {}
    },
    {
        "label": "verification",
        "kind": 2,
        "importPath": "TFace.recognition.test.IJB_Evaluation",
        "description": "TFace.recognition.test.IJB_Evaluation",
        "peekOfCode": "def verification(template_norm_feats=None, unique_templates=None, p1=None, p2=None):\n    # ==========================================================\n    #         Compute set-to-set Similarity Score.\n    # ==========================================================\n    template2id = np.zeros((max(unique_templates) + 1, 1), dtype=int)\n    for count_template, uqt in enumerate(unique_templates):\n        template2id[uqt] = count_template\n    score = np.zeros((len(p1),))   # save cosine distance between pairs\n    total_pairs = np.array(range(len(p1)))\n    batchsize = 100000  # small batchsize instead of all pairs in one batch due to the memory limiation",
        "detail": "TFace.recognition.test.IJB_Evaluation",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "TFace.recognition.test.IJB_Evaluation",
        "description": "TFace.recognition.test.IJB_Evaluation",
        "peekOfCode": "def parse_args():\n    \"\"\" Parse input arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Extract features for business')\n    parser.add_argument('--dataset', dest='dataset', help='dataset type, IJBB or IJBC',\n                                     default='IJBB', type=str)\n    parser.add_argument('--meta_dir', dest='meta_dir', help=\"ijb meta dir\",\n                        default='', type=str)\n    parser.add_argument('--feature', dest='feature_name',\n                        help='Path to text file containing relative paths for every example.',",
        "detail": "TFace.recognition.test.IJB_Evaluation",
        "documentation": {}
    },
    {
        "label": "IJBDataset",
        "kind": 6,
        "importPath": "TFace.recognition.test.datasets",
        "description": "TFace.recognition.test.datasets",
        "peekOfCode": "class IJBDataset(Dataset):\n    \"\"\" IJBDataset, parse image data and score\n    \"\"\"\n    def __init__(self, root_dir, record_dir, transform):\n        super(IJBDataset, self).__init__()\n        self.transform = transform\n        self.root_dir = root_dir\n        self.imgs, self.faceness_scores, self.lmks = self.read_samples_from_record(root_dir, record_dir)\n        print(\"Number of Sampels:{}\".format(len(self.imgs)))\n    def __getitem__(self, index):",
        "detail": "TFace.recognition.test.datasets",
        "documentation": {}
    },
    {
        "label": "crop_transform",
        "kind": 2,
        "importPath": "TFace.recognition.test.datasets",
        "description": "TFace.recognition.test.datasets",
        "peekOfCode": "def crop_transform(rimg, landmark, image_size):\n    \"\"\" warpAffine face img by landmark\n    \"\"\"\n    assert landmark.shape[0] == 68 or landmark.shape[0] == 5\n    assert landmark.shape[1] == 2\n    if landmark.shape[0] == 68:  # 68 landmark, select the five-point\n        landmark5 = np.zeros((5, 2), dtype=np.float32)\n        landmark5[0] = (landmark[36]+landmark[39])/2\n        landmark5[1] = (landmark[42]+landmark[45])/2\n        landmark5[2] = landmark[30]",
        "detail": "TFace.recognition.test.datasets",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "TFace.recognition.test.extract_features",
        "description": "TFace.recognition.test.extract_features",
        "peekOfCode": "def parse_args():\n    \"\"\" Parse input arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='verfication tool')\n    parser.add_argument('--ckpt_path', default=None, required=True, help='model_path')\n    parser.add_argument('--backbone', default='MobileFaceNet', help='backbone type')\n    parser.add_argument('--gpu_ids', default='0', help='gpu ids')\n    parser.add_argument('--batch_size', default=64, type=int, help='batch size')\n    parser.add_argument('--data_root', default='', required=True, help='validation data root')\n    parser.add_argument('--filename_list', default='', required=True, help='file_list')",
        "detail": "TFace.recognition.test.extract_features",
        "documentation": {}
    },
    {
        "label": "de_preprocess",
        "kind": 2,
        "importPath": "TFace.recognition.test.extract_features",
        "description": "TFace.recognition.test.extract_features",
        "peekOfCode": "def de_preprocess(tensor):\n    return tensor * 0.5 + 0.5\ndef hflip_batch(imgs_tensor):\n    \"\"\" flip batch data\n    \"\"\"\n    hflip = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),\n            transforms.functional.hflip,\n            transforms.ToTensor(),",
        "detail": "TFace.recognition.test.extract_features",
        "documentation": {}
    },
    {
        "label": "hflip_batch",
        "kind": 2,
        "importPath": "TFace.recognition.test.extract_features",
        "description": "TFace.recognition.test.extract_features",
        "peekOfCode": "def hflip_batch(imgs_tensor):\n    \"\"\" flip batch data\n    \"\"\"\n    hflip = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),\n            transforms.functional.hflip,\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5])",
        "detail": "TFace.recognition.test.extract_features",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "kind": 2,
        "importPath": "TFace.recognition.test.extract_features",
        "description": "TFace.recognition.test.extract_features",
        "peekOfCode": "def extract_features(data_loader, backbone):\n    \"\"\" extract features from origin img and flipped,\n        concated into single feature\n    \"\"\"\n    embeddings = []\n    flip_embeddings = []\n    faceness_scores = []\n    backbone.eval()\n    print(\"Number of Test Image: {}\".format(len(data_loader)))\n    with torch.no_grad():",
        "detail": "TFace.recognition.test.extract_features",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.test.extract_features",
        "description": "TFace.recognition.test.extract_features",
        "peekOfCode": "def main():\n    args = parse_args()\n    torch.manual_seed(1337)\n    input_size = [112, 112]\n    backbone = get_model(args.backbone)(input_size)\n    if not os.path.exists(args.ckpt_path):\n        raise RuntimeError(\"%s not exists\" % args.ckpt_path)\n    # load ckpt\n    backbone.load_state_dict(torch.load(args.ckpt_path))\n    gpus = [int(x) for x in args.gpu_ids.rstrip().split(',')]",
        "detail": "TFace.recognition.test.extract_features",
        "documentation": {}
    },
    {
        "label": "de_preprocess",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def de_preprocess(tensor):\n    \"\"\"preprocess function\n    \"\"\"\n    return tensor * 0.5 + 0.5\ndef hflip_batch(imgs_tensor):\n    \"\"\" bacth data Horizontally flip\n    \"\"\"\n    hflip = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "hflip_batch",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def hflip_batch(imgs_tensor):\n    \"\"\" bacth data Horizontally flip\n    \"\"\"\n    hflip = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),\n            transforms.functional.hflip,\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5])",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "ccrop_batch",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def ccrop_batch(imgs_tensor):\n    \"\"\"crop image tensor\n    \"\"\"\n    ccrop = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n    ccropped_imgs = torch.empty_like(imgs_tensor)",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def l2_norm(input, axis=1):\n    \"\"\"l2 normalize\n    \"\"\"\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n    return output\ndef calculate_roc(thresholds,\n                  embeddings1,\n                  embeddings2,\n                  actual_issame,",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "calculate_roc",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def calculate_roc(thresholds,\n                  embeddings1,\n                  embeddings2,\n                  actual_issame,\n                  nrof_folds=10,\n                  pca=0):\n    \"\"\" Calculate accuracy with k-fold test method.\n        The whole test set divided into k folds, in every test loop,\n        the k-1 folds data is used to choose the best threshold, and\n        the left 1 fold is used to calculate acc with the best threshold.",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "calculate_accuracy",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def calculate_accuracy(threshold, dist, actual_issame):\n    \"\"\" calculate acc, tpr, fpr by given threshold\n    \"\"\"\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame),\n                               np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def evaluate(embeddings, actual_issame, nrof_folds=10, pca=0):\n    \"\"\" evaluate function\n    \"\"\"\n    thresholds = np.arange(0, 4, 0.01)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy, best_thresholds, bad_case = calculate_roc(\n        thresholds,\n        embeddings1,\n        embeddings2,",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "gen_plot",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def gen_plot(fpr, tpr):\n    \"\"\" plot roc curve\n    \"\"\"\n    import matplotlib.pyplot as plt\n    plt.switch_backend('agg')\n    import io\n    plt.figure()\n    plt.xlabel(\"FPR\", fontsize=14)\n    plt.ylabel(\"TPR\", fontsize=14)\n    plt.title(\"ROC Curve\", fontsize=14)",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "perform_val",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def perform_val(embedding_size,\n                batch_size,\n                backbone,\n                carray,\n                issame,\n                nrof_folds=10,\n                tta=True):\n    \"\"\" Perform accuracy and threshold with the carray is read from bcolz dir.\n        When tta is set True, each test sample should be fliped, then the embedding\n        is fused by the original one and the fliped one.",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "perform_val_bin",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def perform_val_bin(embedding_size,\n                    batch_size,\n                    backbone,\n                    carray,\n                    issame,\n                    nrof_folds=10,\n                    tta=True):\n    \"\"\" Perform accuracy and threshold with the carray is read from bin.\n        When tta is set True, each test sample should be fliped, then the embedding\n        is fused by the original one and the fliped one.",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "rfw_evaluate",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def rfw_evaluate(embeddings, actual_issame, nrof_folds=10, pca = 0):\n    \"\"\"evaluate fucntion\n    \"\"\"\n    thresholds = np.arange(-1, 1, 0.001)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy, _, _ = calculate_roc(thresholds, embeddings1, embeddings2,\n                                             np.asarray(actual_issame), nrof_folds=nrof_folds, pca = pca)\n    thresholds = np.arange(-1, 1, 0.001)\n    val, val_std, far = rfw_calculate_val(thresholds, embeddings1, embeddings2,",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "rfw_calculate_val",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def rfw_calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10):\n    \"\"\"evaluate fucntion\n    \"\"\"\n    assert(embeddings1.shape[0] == embeddings2.shape[0])\n    assert(embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n    val = np.zeros(nrof_folds)\n    far = np.zeros(nrof_folds)",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "rfw_calculate_val_far",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def rfw_calculate_val_far(threshold, dist, actual_issame):\n    \"\"\"evaluate fucntion\n    \"\"\"\n    predict_issame = np.less(dist, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    val = float(true_accept) / float(n_same)\n    far = float(false_accept) / float(n_diff)",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "perform_rfw_val_bin",
        "kind": 2,
        "importPath": "TFace.recognition.test.utils",
        "description": "TFace.recognition.test.utils",
        "peekOfCode": "def perform_rfw_val_bin(data_set, model, device, batch_size=64, nfolds=10):\n    \"\"\" Perform accuracy and threshold with the carray is read from bin.\n    \"\"\"\n    data_list = data_set[0]\n    issame_list = data_set[1]\n    embeddings_list = []\n    with torch.no_grad():\n        for i in range(len(data_list)):\n            data = data_list[i]\n            embeddings = None",
        "detail": "TFace.recognition.test.utils",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification",
        "description": "TFace.recognition.test.verification",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(description='verfication tool')\n    parser.add_argument('--ckpt_path', default=None, required=True, help='model_path')\n    parser.add_argument('--backbone', default='MobileFaceNet', help='backbone type')\n    parser.add_argument('--gpu_ids', default='0', help='gpu ids')\n    parser.add_argument('--batch_size', default=64, help='batch size')\n    parser.add_argument('--data_root', default='', required=True, help='validation data root')\n    parser.add_argument('--embedding_size', default=512, help='embedding_size')\n    args = parser.parse_args()\n    return args",
        "detail": "TFace.recognition.test.verification",
        "documentation": {}
    },
    {
        "label": "get_val_pair_from_bin",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification",
        "description": "TFace.recognition.test.verification",
        "peekOfCode": "def get_val_pair_from_bin(path, name):\n    \"\"\" read data for bin files\n    \"\"\"\n    import pickle\n    import cv2\n    path = os.path.join(path, name)\n    bins, issame_list = pickle.load(open(path, 'rb'), encoding='bytes')\n    images = []\n    for i in range(len(issame_list)*2):\n        _bin = bins[i]",
        "detail": "TFace.recognition.test.verification",
        "documentation": {}
    },
    {
        "label": "get_val_data_from_bin",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification",
        "description": "TFace.recognition.test.verification",
        "peekOfCode": "def get_val_data_from_bin(data_path):\n    lfw, lfw_issame = get_val_pair_from_bin(data_path, 'lfw.bin')\n    cfp_fp, cfp_fp_issame = get_val_pair_from_bin(data_path, 'cfp_fp.bin')\n    agedb_30, agedb_30_issame = get_val_pair_from_bin(data_path, 'agedb_30.bin')\n    calfw, calfw_issame = get_val_pair_from_bin(data_path, 'calfw.bin')\n    cplfw, cplfw_issame = get_val_pair_from_bin(data_path, 'cplfw.bin')\n    return (lfw, cfp_fp, agedb_30, cplfw, calfw,\n            lfw_issame, cfp_fp_issame, agedb_30_issame,\n            cplfw_issame, calfw_issame)\ndef get_val_pair(path, name):",
        "detail": "TFace.recognition.test.verification",
        "documentation": {}
    },
    {
        "label": "get_val_pair",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification",
        "description": "TFace.recognition.test.verification",
        "peekOfCode": "def get_val_pair(path, name):\n    \"\"\" read data from boclz dir\n    \"\"\"\n    import bcolz\n    carray = bcolz.carray(rootdir=os.path.join(path, name), mode='r')\n    issame = np.load('{}/{}_list.npy'.format(path, name))\n    return carray, issame\ndef get_val_data(data_path):\n    lfw, lfw_issame = get_val_pair(data_path, 'lfw')\n    cfp_fp, cfp_fp_issame = get_val_pair(data_path, 'cfp_fp')",
        "detail": "TFace.recognition.test.verification",
        "documentation": {}
    },
    {
        "label": "get_val_data",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification",
        "description": "TFace.recognition.test.verification",
        "peekOfCode": "def get_val_data(data_path):\n    lfw, lfw_issame = get_val_pair(data_path, 'lfw')\n    cfp_fp, cfp_fp_issame = get_val_pair(data_path, 'cfp_fp')\n    agedb_30, agedb_30_issame = get_val_pair(data_path, 'agedb_30')\n    calfw, calfw_issame = get_val_pair(data_path, 'calfw')\n    cplfw, cplfw_issame = get_val_pair(data_path, 'cplfw')\n    return (lfw, cfp_fp, agedb_30, cplfw, calfw,\n            lfw_issame, cfp_fp_issame, agedb_30_issame,\n            cplfw_issame, calfw_issame)\ndef main():",
        "detail": "TFace.recognition.test.verification",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification",
        "description": "TFace.recognition.test.verification",
        "peekOfCode": "def main():\n    \"\"\" Perform evaluation on LFW, CFP-FP, AgeDB, CALFW, CPLFW datasets,\n        each dataset consists of some positive and negative pair data.\n    \"\"\"\n    args = parse_args()\n    torch.manual_seed(1337)\n    input_size = [112, 112]\n    # load backbone\n    backbone = get_model(args.backbone)(input_size)\n    if not os.path.exists(args.ckpt_path):",
        "detail": "TFace.recognition.test.verification",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification_rfw",
        "description": "TFace.recognition.test.verification_rfw",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(description='verfication tool')\n    parser.add_argument('--ckpt_path', default=None, required=True, help='model_path')\n    parser.add_argument('--backbone', default='IR_34', help='backbone type')\n    parser.add_argument('--gpu_ids', default='0', help='gpu ids')\n    parser.add_argument('--batch_size', default=64, help='batch size')\n    parser.add_argument('--data_root', default='', required=True, help='validation data root')\n    parser.add_argument('--embedding_size', default=512, help='embedding_size')\n    args = parser.parse_args()\n    return args",
        "detail": "TFace.recognition.test.verification_rfw",
        "documentation": {}
    },
    {
        "label": "load_rfw_bin",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification_rfw",
        "description": "TFace.recognition.test.verification_rfw",
        "peekOfCode": "def load_rfw_bin(path, name):\n    \"\"\" read data for bin files\n        \"\"\"\n    import pickle\n    import cv2\n    path = os.path.join(path, name)\n    bins, issame_list = pickle.load(open(path, 'rb'), encoding='bytes')\n    data_list = [[], []]\n    for i in range(len(issame_list)*2):\n        _bin = bins[i]",
        "detail": "TFace.recognition.test.verification_rfw",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.test.verification_rfw",
        "description": "TFace.recognition.test.verification_rfw",
        "peekOfCode": "def main():\n    \"\"\" Perform evaluation on RFW datasets,\n        each dataset consists of some positive and negative pair data.\n    \"\"\"\n    args = parse_args()\n    torch.manual_seed(1337)\n    input_size = [112, 112]\n    val_data_dir = args.data_root\n    batch_size = args.batch_size\n    # load backbone",
        "detail": "TFace.recognition.test.verification_rfw",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "TFace.recognition.tools.convert_new_index",
        "description": "TFace.recognition.tools.convert_new_index",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description='convert training index file')\n    parser.add_argument('--old', default=None, type=str, required=True,\n                        help='path to old training list')\n    parser.add_argument('--tfr_index', default=None, type=str, required=True,\n                        help='path to tfrecord index file')\n    parser.add_argument('--new', default=None, type=str, required=True,\n                        help='path to new training list')",
        "detail": "TFace.recognition.tools.convert_new_index",
        "documentation": {}
    },
    {
        "label": "build_dict",
        "kind": 2,
        "importPath": "TFace.recognition.tools.convert_new_index",
        "description": "TFace.recognition.tools.convert_new_index",
        "peekOfCode": "def build_dict(tfr_index):\n    d = {}\n    print(\"reading {}\".format(tfr_index))\n    tfr_name = os.path.basename(tfr_index).replace('.index', '')\n    with open(tfr_index, 'r') as f:\n        for line in f:\n            file_name, shard_index, offset = line.rstrip().split('\\t')\n            d[file_name] = '{}\\t{}\\t{}'.format(tfr_name, shard_index, offset)\n    print(\"build dict done\")\n    return d",
        "detail": "TFace.recognition.tools.convert_new_index",
        "documentation": {}
    },
    {
        "label": "convert",
        "kind": 2,
        "importPath": "TFace.recognition.tools.convert_new_index",
        "description": "TFace.recognition.tools.convert_new_index",
        "peekOfCode": "def convert(index_file, d, out_index_file):\n    print(\"write to new index file {}\".format(out_index_file))\n    with open(index_file, 'r') as f, open(out_index_file, 'w') as out_f:\n        for line in f:\n            if '\\t' in line:\n                file_name, label = line.rstrip().split('\\t')\n            else:\n                file_name, label = line.rstrip().split(' ')\n            tfr_string = d.get(file_name, None)\n            if tfr_string is None:",
        "detail": "TFace.recognition.tools.convert_new_index",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.tools.convert_new_index",
        "description": "TFace.recognition.tools.convert_new_index",
        "peekOfCode": "def main():\n    args = parse_args()\n    d = build_dict(args.tfr_index)\n    convert(args.old, d, args.new)\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.recognition.tools.convert_new_index",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "TFace.recognition.tools.decode",
        "description": "TFace.recognition.tools.decode",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description='decode tfrecord')\n    parser.add_argument('--tfrecords_dir', default=None, type=str, required=True,\n                        help='path to the output of tfrecords dir path')\n    parser.add_argument('--output_dir', default=None, type=str, required=True,\n                        help='path to the output of decoded imgs')\n    parser.add_argument('--limit', default=10, type=int, required=True,\n                        help='limit num of decoded samples')",
        "detail": "TFace.recognition.tools.decode",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 2,
        "importPath": "TFace.recognition.tools.decode",
        "description": "TFace.recognition.tools.decode",
        "peekOfCode": "def parser(feature_list):\n    for key, feature in feature_list:\n        if key == 'image':\n            image_raw = feature.bytes_list.value[0]\n            return image_raw\n    raise ValueError(\"No key=image in feature list\")\ndef get_record(record_file, offset):\n    with open(record_file, 'rb') as ifs:\n        ifs.seek(offset)\n        byte_len_crc = ifs.read(12)",
        "detail": "TFace.recognition.tools.decode",
        "documentation": {}
    },
    {
        "label": "get_record",
        "kind": 2,
        "importPath": "TFace.recognition.tools.decode",
        "description": "TFace.recognition.tools.decode",
        "peekOfCode": "def get_record(record_file, offset):\n    with open(record_file, 'rb') as ifs:\n        ifs.seek(offset)\n        byte_len_crc = ifs.read(12)\n        proto_len = struct.unpack('Q', byte_len_crc[:8])[0]\n        # proto,crc\n        pb_data = ifs.read(proto_len)\n        if len(pb_data) < proto_len:\n            print(\"read pb_data err,proto_len:%s pb_data len:%s\" % (proto_len, len(pb_data)))\n            return None",
        "detail": "TFace.recognition.tools.decode",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.tools.decode",
        "description": "TFace.recognition.tools.decode",
        "peekOfCode": "def main():\n    args = parse_args()\n    tfrecords_dir = os.path.normpath(args.tfrecords_dir)\n    tfrecords_name = tfrecords_dir.split('/')[-1]\n    output_dir = args.output_dir\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n    limit = args.limit\n    print(tfrecords_dir)\n    print(tfrecords_name)",
        "detail": "TFace.recognition.tools.decode",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "TFace.recognition.tools.img2tfrecord",
        "description": "TFace.recognition.tools.img2tfrecord",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description='imgs to tfrecord')\n    parser.add_argument('--img_list', default=None, type=str, required=True,\n                        help='path to the image file')\n    parser.add_argument('--pts_list', default=None, type=str, required=True,\n                        help='path to 5p list')\n    parser.add_argument('--tfrecords_name', default='TFR-MS1M', type=str,  required=True,\n                        help='path to the output of tfrecords dir path')",
        "detail": "TFace.recognition.tools.img2tfrecord",
        "documentation": {}
    },
    {
        "label": "get_img2lmk",
        "kind": 2,
        "importPath": "TFace.recognition.tools.img2tfrecord",
        "description": "TFace.recognition.tools.img2tfrecord",
        "peekOfCode": "def get_img2lmk(pts_file):\n    img2lmk = {}\n    with open(pts_file, 'r') as f:\n        for line in f:\n            line = line.rstrip().split(' ')\n            lmk = np.array([float(x) for x in line[1: -1]], dtype=np.float32)\n            lmk = lmk.reshape((5, 2))\n            filename = line[0]\n            img2lmk[filename] = lmk\n    return img2lmk",
        "detail": "TFace.recognition.tools.img2tfrecord",
        "documentation": {}
    },
    {
        "label": "crop_transform",
        "kind": 2,
        "importPath": "TFace.recognition.tools.img2tfrecord",
        "description": "TFace.recognition.tools.img2tfrecord",
        "peekOfCode": "def crop_transform(rimg, landmark, image_size):\n    \"\"\" warpAffine face img by landmark\n    \"\"\"\n    assert landmark.shape[0] == 68 or landmark.shape[0] == 5\n    assert landmark.shape[1] == 2\n    if landmark.shape[0] == 68:\n        landmark5 = np.zeros((5, 2), dtype=np.float32)\n        landmark5[0] = (landmark[36] + landmark[39]) / 2\n        landmark5[1] = (landmark[42] + landmark[45]) / 2\n        landmark5[2] = landmark[30]",
        "detail": "TFace.recognition.tools.img2tfrecord",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.tools.img2tfrecord",
        "description": "TFace.recognition.tools.img2tfrecord",
        "peekOfCode": "def main():\n    args = parse_args()\n    tfrecords_dir = os.path.join('./', args.tfrecords_name)\n    tfrecords_name = args.tfrecords_name\n    if not os.path.isdir(tfrecords_dir):\n        os.makedirs(tfrecords_dir)\n    img2lmk = get_img2lmk(args.pts_list)\n    count = 0\n    cur_shard_size = 0\n    cur_shard_idx = -1",
        "detail": "TFace.recognition.tools.img2tfrecord",
        "documentation": {}
    },
    {
        "label": "FrozenBatchNorm2d",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.layers.batch_norm",
        "description": "TFace.recognition.torchkit.backbone.fbnets.layers.batch_norm",
        "peekOfCode": "class FrozenBatchNorm2d(nn.Module):\n    \"\"\"\n    BatchNorm2d where the batch statistics and the affine parameters\n    are fixed\n    \"\"\"\n    def __init__(self, n):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer(\"weight\", torch.ones(n))\n        self.register_buffer(\"bias\", torch.zeros(n))\n        self.register_buffer(\"running_mean\", torch.zeros(n))",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.layers.batch_norm",
        "documentation": {}
    },
    {
        "label": "_NewEmptyTensorOp",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "description": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "peekOfCode": "class _NewEmptyTensorOp(torch.autograd.Function):\n    \"\"\"create new empty tensor\n    \"\"\"\n    @staticmethod\n    def forward(ctx, x, new_shape):\n        ctx.shape = x.shape\n        return x.new_empty(new_shape)\n    @staticmethod\n    def backward(ctx, grad):\n        shape = ctx.shape",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "description": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "peekOfCode": "class Conv2d(torch.nn.Conv2d):\n    \"\"\"conv2d module using _NewEmptyTensorOp\n    \"\"\"\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(Conv2d, self).forward(x)\n        # get output shape\n        output_shape = [\n            (i + 2 * p - (di * (k - 1) + 1)) // d + 1\n            for i, p, di, k, d in zip(",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "documentation": {}
    },
    {
        "label": "ConvTranspose2d",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "description": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "peekOfCode": "class ConvTranspose2d(torch.nn.ConvTranspose2d):\n    \"\"\"ConvTranspose2d module using _NewEmptyTensorOp\n    \"\"\"\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(ConvTranspose2d, self).forward(x)\n        # get output shape\n        output_shape = [\n            (i - 1) * d - 2 * p + (di * (k - 1) + 1) + op\n            for i, p, di, k, d, op in zip(",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "description": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "peekOfCode": "class BatchNorm2d(torch.nn.BatchNorm2d):\n    \"\"\"BatchNorm2d module using _NewEmptyTensorOp\n    \"\"\"\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(BatchNorm2d, self).forward(x)\n        # get output shape\n        output_shape = x.shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\ndef interpolate(",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "description": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "peekOfCode": "def interpolate(\n    input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None\n):\n    \"\"\"interpolate op using _NewEmptyTensorOp\n    \"\"\"\n    if input.numel() > 0:\n        return torch.nn.functional.interpolate(\n            input, size, scale_factor, mode, align_corners\n        )\n    def _check_size_scale_factor(dim):",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.layers.misc",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class Flatten(nn.Module):\n    \"\"\"flatten tensor to 1D\n    \"\"\"\n    def __init__(self):\n        super(Flatten, self).__init__()\n    def forward(self, x):\n        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n        return x.view(-1, shape)\nclass Identity(nn.Module):\n    \"\"\"identity module",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "Identity",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class Identity(nn.Module):\n    \"\"\"identity module\n    \"\"\"\n    def __init__(self, C_in, C_out, stride):\n        super(Identity, self).__init__()\n        self.output_depth = C_out\n        self.conv = (\n            ConvBNRelu(\n                C_in,\n                C_out,",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "CascadeConv3x3",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class CascadeConv3x3(nn.Sequential):\n    \"\"\"cascade conv block\n    \"\"\"\n    def __init__(self, C_in, C_out, stride):\n        assert stride in [1, 2]\n        ops = [\n            Conv2d(C_in, C_in, 3, stride, 1, bias=False),\n            BatchNorm2d(C_in),\n            nn.ReLU(inplace=True),\n            Conv2d(C_in, C_out, 3, 1, 1, bias=False),",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "Shift",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class Shift(nn.Module):\n    \"\"\"shift blcok\n    \"\"\"\n    def __init__(self, C, kernel_size, stride, padding):\n        super(Shift, self).__init__()\n        self.C = C\n        kernel = torch.zeros((C, 1, kernel_size, kernel_size), dtype=torch.float32)\n        ch_idx = 0\n        assert stride in [1, 2]\n        self.stride = stride",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "ShiftBlock5x5",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class ShiftBlock5x5(nn.Sequential):\n    \"\"\"shift blcok with 5*5 kernel\n    \"\"\"\n    def __init__(self, C_in, C_out, expansion, stride):\n        assert stride in [1, 2]\n        self.res_connect = (stride == 1) and (C_in == C_out)\n        C_mid = make_divisible(C_in * expansion, 8, 8)\n        ops = [\n            # pw\n            Conv2d(C_in, C_mid, 1, 1, 0, bias=False),",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "ChannelShuffle",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class ChannelShuffle(nn.Module):\n    \"\"\"Channel shuffle block\n    \"\"\"\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    def forward(self, x):\n        \"\"\"Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\"\"\"\n        N, C, H, W = x.size()\n        g = self.groups",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "ConvBNRelu",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class ConvBNRelu(nn.Sequential):\n    \"\"\"basic conv-bn-relu block\n    \"\"\"\n    def __init__(\n        self,\n        input_depth,\n        output_depth,\n        kernel,\n        stride,\n        pad,",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class SEModule(nn.Module):\n    \"\"\"se module with reduction 4\n    \"\"\"\n    reduction = 4\n    def __init__(self, C):\n        super(SEModule, self).__init__()\n        mid = max(make_divisible(C // self.reduction), 16)\n        conv1 = Conv2d(C, mid, 1, 1, 0)\n        conv2 = Conv2d(mid, C, 1, 1, 0)\n        self.op = nn.Sequential(",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class Upsample(nn.Module):\n    \"\"\"upsample module\n    \"\"\"\n    def __init__(self, scale_factor, mode, align_corners=None):\n        super(Upsample, self).__init__()\n        self.scale = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n    def forward(self, x):\n        return interpolate(",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "IRFBlock",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class IRFBlock(nn.Module):\n    \"\"\"inverted residual block \n    \"\"\"\n    def __init__(\n        self,\n        input_depth,\n        output_depth,\n        expansion,\n        stride,\n        bn_type=\"bn\",",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "FBNetBuilder",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class FBNetBuilder(object):\n    \"\"\"FBNet builder\n    \"\"\"\n    def __init__(\n        self,\n        width_ratio,\n        bn_type=\"bn\",\n        width_divisor=1,\n        dw_skip_bn=False,\n        dw_skip_relu=False,",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "FBNet",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "class FBNet(nn.Module):\n    \"\"\"FBNet\n    \"\"\"\n    def __init__(\n        self, builder, arch_def, dim_in, input_size=None\n    ):\n        super(FBNet, self).__init__()\n        if input_size is None:\n            input_size = [112, 112]\n        assert input_size[0] in [112, 224], \\",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "make_divisible",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "def make_divisible(x, divisible_by=8, min_val=8):\n    \"\"\"make channels divisible\n    \"\"\"\n    return max(int(np.ceil(int(x) * 1. / divisible_by) * divisible_by), min_val)\nPRIMITIVES = {\n    \"skip\": lambda C_in, C_out, expansion, stride, **kwargs: Identity(\n        C_in, C_out, stride\n    ),\n    \"ir_k3\": lambda C_in, C_out, expansion, stride, **kwargs: IRFBlock(\n        C_in, C_out, expansion, stride, **kwargs",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "expand_stage_cfg",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "def expand_stage_cfg(stage_cfg):\n    \"\"\" For a single stage\n    \"\"\"\n    assert isinstance(stage_cfg, list)\n    ret = []\n    for x in stage_cfg:\n        ret += _expand_block_cfg(x)\n    return ret\ndef expand_stages_cfg(stage_cfgs):\n    \"\"\" For a list of stages",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "expand_stages_cfg",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "def expand_stages_cfg(stage_cfgs):\n    \"\"\" For a list of stages\n    \"\"\"\n    assert isinstance(stage_cfgs, list)\n    ret = []\n    for x in stage_cfgs:\n        ret.append(expand_stage_cfg(x))\n    return ret\ndef _block_cfgs_to_list(block_cfgs):\n    assert isinstance(block_cfgs, list)",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "unify_arch_def",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "def unify_arch_def(arch_def):\n    \"\"\" unify the arch_def to:\n        {\n            ...,\n            \"arch\": [\n                {\n                    \"stage_idx\": idx,\n                    \"block_idx\": idx,\n                    ...\n                },",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "get_num_stages",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "def get_num_stages(arch_def):\n    ret = 0\n    for x in arch_def[\"stages\"]:\n        ret = max(x[\"stage_idx\"], ret)\n    ret = ret + 1\n    return ret\ndef get_blocks(arch_def, stage_indices=None, block_indices=None):\n    ret = copy.deepcopy(arch_def)\n    ret[\"stages\"] = []\n    for block in arch_def[\"stages\"]:",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "get_blocks",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "def get_blocks(arch_def, stage_indices=None, block_indices=None):\n    ret = copy.deepcopy(arch_def)\n    ret[\"stages\"] = []\n    for block in arch_def[\"stages\"]:\n        keep = True\n        if stage_indices not in (None, []) and block[\"stage_idx\"] not in stage_indices:\n            keep = False\n        if block_indices not in (None, []) and block[\"block_idx\"] not in block_indices:\n            keep = False\n        if keep:",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "get_fbnet_model",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "def get_fbnet_model(arch, input_size):\n    assert arch in MODEL_ARCH\n    arch_def = MODEL_ARCH[arch]\n    arch_def = unify_arch_def(arch_def)\n    builder = FBNetBuilder(width_ratio=1.0, bn_type=\"bn\", width_divisor=8, dw_skip_bn=True, dw_skip_relu=True)\n    model = FBNet(builder, arch_def, dim_in=3, input_size=input_size)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "logger = logging.getLogger(__name__)\njson_file = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'fbnet_modeldef.json')\nwith open(json_file) as f:\n    MODEL_ARCH = json.load(f)\ndef make_divisible(x, divisible_by=8, min_val=8):\n    \"\"\"make channels divisible\n    \"\"\"\n    return max(int(np.ceil(int(x) * 1. / divisible_by) * divisible_by), min_val)\nPRIMITIVES = {\n    \"skip\": lambda C_in, C_out, expansion, stride, **kwargs: Identity(",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "json_file",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "json_file = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'fbnet_modeldef.json')\nwith open(json_file) as f:\n    MODEL_ARCH = json.load(f)\ndef make_divisible(x, divisible_by=8, min_val=8):\n    \"\"\"make channels divisible\n    \"\"\"\n    return max(int(np.ceil(int(x) * 1. / divisible_by) * divisible_by), min_val)\nPRIMITIVES = {\n    \"skip\": lambda C_in, C_out, expansion, stride, **kwargs: Identity(\n        C_in, C_out, stride",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "PRIMITIVES",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "description": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "peekOfCode": "PRIMITIVES = {\n    \"skip\": lambda C_in, C_out, expansion, stride, **kwargs: Identity(\n        C_in, C_out, stride\n    ),\n    \"ir_k3\": lambda C_in, C_out, expansion, stride, **kwargs: IRFBlock(\n        C_in, C_out, expansion, stride, **kwargs\n    ),\n    \"ir_k5\": lambda C_in, C_out, expansion, stride, **kwargs: IRFBlock(\n        C_in, C_out, expansion, stride, kernel=5, **kwargs\n    ),",
        "detail": "TFace.recognition.torchkit.backbone.fbnets.fbnet_builder",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.common",
        "description": "TFace.recognition.torchkit.backbone.common",
        "peekOfCode": "class Flatten(Module):\n    \"\"\" Flat tensor\n    \"\"\"\n    def forward(self, input):\n        return input.view(input.size(0), -1)\nclass LinearBlock(Module):\n    \"\"\" Convolution block without no-linear activation layer\n    \"\"\"\n    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n        super(LinearBlock, self).__init__()",
        "detail": "TFace.recognition.torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "LinearBlock",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.common",
        "description": "TFace.recognition.torchkit.backbone.common",
        "peekOfCode": "class LinearBlock(Module):\n    \"\"\" Convolution block without no-linear activation layer\n    \"\"\"\n    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n        super(LinearBlock, self).__init__()\n        self.conv = Conv2d(in_c, out_c, kernel, stride, padding, groups=groups, bias=False)\n        self.bn = BatchNorm2d(out_c)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)",
        "detail": "TFace.recognition.torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "GNAP",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.common",
        "description": "TFace.recognition.torchkit.backbone.common",
        "peekOfCode": "class GNAP(Module):\n    \"\"\" Global Norm-Aware Pooling block\n    \"\"\"\n    def __init__(self, in_c):\n        super(GNAP, self).__init__()\n        self.bn1 = BatchNorm2d(in_c, affine=False)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.bn2 = BatchNorm1d(in_c, affine=False)\n    def forward(self, x):\n        x = self.bn1(x)",
        "detail": "TFace.recognition.torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "GDC",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.common",
        "description": "TFace.recognition.torchkit.backbone.common",
        "peekOfCode": "class GDC(Module):\n    \"\"\" Global Depthwise Convolution block\n    \"\"\"\n    def __init__(self, in_c, embedding_size):\n        super(GDC, self).__init__()\n        self.conv_6_dw = LinearBlock(in_c, in_c,\n                                     groups=in_c,\n                                     kernel=(7, 7),\n                                     stride=(1, 1),\n                                     padding=(0, 0))",
        "detail": "TFace.recognition.torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.common",
        "description": "TFace.recognition.torchkit.backbone.common",
        "peekOfCode": "class SEModule(Module):\n    \"\"\" SE block\n    \"\"\"\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = Conv2d(channels, channels // reduction,\n                          kernel_size=1, padding=0, bias=False)\n        nn.init.xavier_uniform_(self.fc1.weight.data)\n        self.relu = ReLU(inplace=True)",
        "detail": "TFace.recognition.torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.common",
        "description": "TFace.recognition.torchkit.backbone.common",
        "peekOfCode": "def initialize_weights(modules):\n    \"\"\" Weight initilize, conv2d and linear is initialized with kaiming_normal\n    \"\"\"\n    for m in modules:\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight,\n                                    mode='fan_out',\n                                    nonlinearity='relu')\n            if m.bias is not None:\n                m.bias.data.zero_()",
        "detail": "TFace.recognition.torchkit.backbone.common",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "class ConvBlock(Module):\n    \"\"\" Convolution block\n    \"\"\"\n    def __init__(self, in_c, out_c, kernel_size=1, stride=1, padding=0, groups=1,\n                 use_bn=True, activation=(lambda: ReLU(inplace=True))):\n        super(ConvBlock, self).__init__()\n        self.conv = Conv2d(in_c, out_c, kernel_size, stride, padding, groups=groups, bias=False)\n        self.bn = BatchNorm2d(out_c) if use_bn else None\n        if activation is None:\n            self.act = None",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "EffiDwsConvUnit",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "class EffiDwsConvUnit(nn.Module):\n    \"\"\"\n    EfficientNet specific depthwise separable convolution block/unit with BatchNorms and activations at each convolution\n    layers.\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "EffiInvResUnit",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "class EffiInvResUnit(nn.Module):\n    \"\"\"\n    EfficientNet inverted residual unit.\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    kernel_size : int or tuple/list of 2 int",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "EffiInitBlock",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "class EffiInitBlock(nn.Module):\n    \"\"\"\n    EfficientNet specific initial block.\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    bn_eps : float",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "EfficientNet",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "class EfficientNet(nn.Module):\n    \"\"\"\n    EfficientNet model from 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,'\n    https://arxiv.org/abs/1905.11946.\n    Parameters:\n    ----------\n    channels : list of list of int\n        Number of output channels for each unit.\n    init_block_channels : int\n        Number of output channels for initial unit.",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "conv1x1_block",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def conv1x1_block(in_channels, out_channels, stride=1, padding=0,\n                  use_bn=True, activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    1x1 version of the standard convolution block.\n    \"\"\"\n    return ConvBlock(in_c=in_channels, out_c=out_channels, kernel_size=1, stride=stride,\n                     padding=padding, groups=1, use_bn=use_bn, activation=activation)\ndef conv3x3_block(in_channels, out_channels, stride=1, padding=1,\n                  use_bn=True, activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "conv3x3_block",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def conv3x3_block(in_channels, out_channels, stride=1, padding=1,\n                  use_bn=True, activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    3x3 version of the standard convolution block.\n    \"\"\"\n    return ConvBlock(in_c=in_channels, out_c=out_channels, kernel_size=3, stride=stride,\n                     padding=padding, groups=1, use_bn=use_bn, activation=activation)\ndef dwconv3x3_block(in_channels, out_channels, stride=1, padding=1,\n                    use_bn=True, activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "dwconv3x3_block",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def dwconv3x3_block(in_channels, out_channels, stride=1, padding=1,\n                    use_bn=True, activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    3x3 depthwise version of the standard convolution block.\n    \"\"\"\n    return ConvBlock(in_c=in_channels, out_c=out_channels, kernel_size=3, stride=stride,\n                     padding=padding, groups=out_channels, use_bn=use_bn, activation=activation)\ndef dwconv5x5_block(in_channels, out_channels, stride=1, padding=2,\n                    use_bn=True, activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "dwconv5x5_block",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def dwconv5x5_block(in_channels, out_channels, stride=1, padding=2,\n                    use_bn=True, activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    5x5 depthwise version of the standard convolution block.\n    \"\"\"\n    return ConvBlock(in_c=in_channels, out_c=out_channels, kernel_size=5, stride=stride,\n                     padding=padding, groups=out_channels, use_bn=use_bn, activation=activation)\ndef round_channels(channels, divisor=8):\n    \"\"\"\n    Round weighted channel number (make divisible operation).",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "round_channels",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def round_channels(channels, divisor=8):\n    \"\"\"\n    Round weighted channel number (make divisible operation).\n    Parameters:\n    ----------\n    channels : int or float\n        Original number of channels.\n    divisor : int, default 8\n        Alignment value.\n    Returns",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "calc_tf_padding",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def calc_tf_padding(x,\n                    kernel_size,\n                    stride=1,\n                    dilation=1):\n    \"\"\"\n    Calculate TF-same like padding size.\n    Parameters:\n    ----------\n    x : tensor\n        Input tensor.",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "efficientnet",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def efficientnet(input_size, embedding_size=512, version='b1', **kwargs):\n    \"\"\"\n    Create EfficientNet model with specific parameters.\n    \"\"\"\n    assert input_size[0] in [112]\n    if version.endswith('b') or version.endswith('c'):\n        version = version[:-1]\n        tf_mode = True\n        bn_eps = 1e-3\n    else:",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "EfficientNetB0",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def EfficientNetB0(input_size):\n    \"\"\"  Constructs an EfficientNet-B0 model.\n    \"\"\"\n    return efficientnet(input_size, embedding_size=512, version='b0')\ndef EfficientNetB1(input_size):\n    \"\"\"  Constructs an EfficientNet-B0 model.\n    \"\"\"\n    return efficientnet(input_size, embedding_size=512, version='b1')",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "EfficientNetB1",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "description": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "peekOfCode": "def EfficientNetB1(input_size):\n    \"\"\"  Constructs an EfficientNet-B0 model.\n    \"\"\"\n    return efficientnet(input_size, embedding_size=512, version='b1')",
        "detail": "TFace.recognition.torchkit.backbone.model_efficientnet",
        "documentation": {}
    },
    {
        "label": "SqueezeExcite",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "description": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "peekOfCode": "class SqueezeExcite(nn.Module):\n    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n        super(SqueezeExcite, self).__init__()\n        self.gate_fn = gate_fn\n        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n        self.act1 = act_layer(inplace=True)\n        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)",
        "detail": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "documentation": {}
    },
    {
        "label": "ConvBnAct",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "description": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "peekOfCode": "class ConvBnAct(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size,\n                 stride=1, act_layer=nn.PReLU):\n        super(ConvBnAct, self).__init__()\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_chs)\n        self.act1 = act_layer(out_chs)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)",
        "detail": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "documentation": {}
    },
    {
        "label": "GhostModule",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "description": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "peekOfCode": "class GhostModule(nn.Module):\n    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n        super(GhostModule, self).__init__()\n        self.oup = oup\n        init_channels = math.ceil(oup / ratio)\n        new_channels = init_channels*(ratio-1)\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n            nn.BatchNorm2d(init_channels),\n            nn.PReLU(init_channels) if relu else nn.Sequential(),",
        "detail": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "documentation": {}
    },
    {
        "label": "GhostBottleneck",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "description": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "peekOfCode": "class GhostBottleneck(nn.Module):\n    \"\"\" Ghost bottleneck w/ optional SE\"\"\"\n    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,\n                 stride=1, act_layer=nn.ReLU, se_ratio=0.):\n        super(GhostBottleneck, self).__init__()\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.stride = stride\n        # Point-wise expansion\n        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\n        # Depth-wise convolution",
        "detail": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "documentation": {}
    },
    {
        "label": "GhostNet",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "description": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "peekOfCode": "class GhostNet(nn.Module):\n    def __init__(self, input_size, embedding_size=512, width=1.3):\n        super(GhostNet, self).__init__()\n        # setting of inverted residual blocks\n        assert input_size[0] in [112]\n        self.cfgs = [\n            # k, t, c, SE, s\n            # stage1\n            [[3,  16,  16, 0, 1]],\n            # stage2",
        "detail": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "documentation": {}
    },
    {
        "label": "hard_sigmoid",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "description": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "peekOfCode": "def hard_sigmoid(x, inplace: bool = False):\n    if inplace:\n        return x.add_(3.).clamp_(0., 6.).div_(6.)\n    else:\n        return F.relu6(x + 3.) / 6.\nclass SqueezeExcite(nn.Module):\n    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n        super(SqueezeExcite, self).__init__()\n        self.gate_fn = gate_fn",
        "detail": "TFace.recognition.torchkit.backbone.model_ghostnet",
        "documentation": {}
    },
    {
        "label": "BasicBlockIR",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "class BasicBlockIR(Module):\n    \"\"\" BasicBlock for IRNet\n    \"\"\"\n    def __init__(self, in_channel, depth, stride):\n        super(BasicBlockIR, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(\n                Conv2d(in_channel, depth, (1, 1), stride, bias=False),",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "BottleneckIR",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "class BottleneckIR(Module):\n    \"\"\" BasicBlock with bottleneck for IRNet\n    \"\"\"\n    def __init__(self, in_channel, depth, stride):\n        super(BottleneckIR, self).__init__()\n        reduction_channel = depth // 4\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "BasicBlockIRSE",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "class BasicBlockIRSE(BasicBlockIR):\n    def __init__(self, in_channel, depth, stride):\n        super(BasicBlockIRSE, self).__init__(in_channel, depth, stride)\n        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\nclass BottleneckIRSE(BottleneckIR):\n    def __init__(self, in_channel, depth, stride):\n        super(BottleneckIRSE, self).__init__(in_channel, depth, stride)\n        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\nclass Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "BottleneckIRSE",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "class BottleneckIRSE(BottleneckIR):\n    def __init__(self, in_channel, depth, stride):\n        super(BottleneckIRSE, self).__init__(in_channel, depth, stride)\n        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\nclass Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\ndef get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] +\\\n           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\ndef get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] +\\\n           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "class Backbone(Module):\n    def __init__(self, input_size, num_layers, mode='ir', input_channel=3):\n        \"\"\" Args:\n            input_size: input_size of backbone\n            num_layers: num_layers of backbone\n            mode: support ir or irse\n        \"\"\"\n        super(Backbone, self).__init__()\n        assert input_size[0] in [112, 224], \\\n            \"input_size should be [112, 112] or [224, 224]\"",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "get_block",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] +\\\n           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),\n            get_block(in_channel=128, depth=256, num_units=2),\n            get_block(in_channel=256, depth=512, num_units=2)",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "get_blocks",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def get_blocks(num_layers):\n    if num_layers == 18:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=2),\n            get_block(in_channel=64, depth=128, num_units=2),\n            get_block(in_channel=128, depth=256, num_units=2),\n            get_block(in_channel=256, depth=512, num_units=2)\n        ]\n    elif num_layers == 34:\n        blocks = [",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_18",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_18(input_size, **kwargs):\n    \"\"\" Constructs a ir-18 model.\n    \"\"\"\n    model = Backbone(input_size, 18, 'ir', **kwargs)\n    return model\ndef IR_34(input_size, **kwargs):\n    \"\"\" Constructs a ir-34 model.\n    \"\"\"\n    model = Backbone(input_size, 34, 'ir', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_34",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_34(input_size, **kwargs):\n    \"\"\" Constructs a ir-34 model.\n    \"\"\"\n    model = Backbone(input_size, 34, 'ir', **kwargs)\n    return model\ndef IR_50(input_size, **kwargs):\n    \"\"\" Constructs a ir-50 model.\n    \"\"\"\n    model = Backbone(input_size, 50, 'ir', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_50",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_50(input_size, **kwargs):\n    \"\"\" Constructs a ir-50 model.\n    \"\"\"\n    model = Backbone(input_size, 50, 'ir', **kwargs)\n    return model\ndef IR_101(input_size, **kwargs):\n    \"\"\" Constructs a ir-101 model.\n    \"\"\"\n    model = Backbone(input_size, 100, 'ir', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_101",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_101(input_size, **kwargs):\n    \"\"\" Constructs a ir-101 model.\n    \"\"\"\n    model = Backbone(input_size, 100, 'ir', **kwargs)\n    return model\ndef IR_152(input_size, **kwargs):\n    \"\"\" Constructs a ir-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_152",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_152(input_size, **kwargs):\n    \"\"\" Constructs a ir-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir', **kwargs)\n    return model\ndef IR_200(input_size, **kwargs):\n    \"\"\" Constructs a ir-200 model.\n    \"\"\"\n    model = Backbone(input_size, 200, 'ir', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_200",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_200(input_size, **kwargs):\n    \"\"\" Constructs a ir-200 model.\n    \"\"\"\n    model = Backbone(input_size, 200, 'ir', **kwargs)\n    return model\ndef IR_SE_50(input_size, **kwargs):\n    \"\"\" Constructs a ir_se-50 model.\n    \"\"\"\n    model = Backbone(input_size, 50, 'ir_se', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_SE_50",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_SE_50(input_size, **kwargs):\n    \"\"\" Constructs a ir_se-50 model.\n    \"\"\"\n    model = Backbone(input_size, 50, 'ir_se', **kwargs)\n    return model\ndef IR_SE_101(input_size, **kwargs):\n    \"\"\" Constructs a ir_se-101 model.\n    \"\"\"\n    model = Backbone(input_size, 100, 'ir_se', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_SE_101",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_SE_101(input_size, **kwargs):\n    \"\"\" Constructs a ir_se-101 model.\n    \"\"\"\n    model = Backbone(input_size, 100, 'ir_se', **kwargs)\n    return model\ndef IR_SE_152(input_size, **kwargs):\n    \"\"\" Constructs a ir_se-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir_se', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_SE_152",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_SE_152(input_size, **kwargs):\n    \"\"\" Constructs a ir_se-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir_se', **kwargs)\n    return model\ndef IR_SE_200(input_size, **kwargs):\n    \"\"\" Constructs a ir_se-200 model.\n    \"\"\"\n    model = Backbone(input_size, 200, 'ir_se', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "IR_SE_200",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_irse",
        "description": "TFace.recognition.torchkit.backbone.model_irse",
        "peekOfCode": "def IR_SE_200(input_size, **kwargs):\n    \"\"\" Constructs a ir_se-200 model.\n    \"\"\"\n    model = Backbone(input_size, 200, 'ir_se', **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_irse",
        "documentation": {}
    },
    {
        "label": "Conv_block",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "description": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "peekOfCode": "class Conv_block(Module):\n    \"\"\" Convolution block with no-linear activation layer\n    \"\"\"\n    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n        super(Conv_block, self).__init__()\n        self.conv = Conv2d(in_c, out_c, kernel, stride, padding, groups=groups, bias=False)\n        self.bn = BatchNorm2d(out_c)\n        self.prelu = PReLU(out_c)\n    def forward(self, x):\n        x = self.conv(x)",
        "detail": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "documentation": {}
    },
    {
        "label": "Depth_Wise",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "description": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "peekOfCode": "class Depth_Wise(Module):\n    \"\"\" Depthwise block\n    \"\"\"\n    def __init__(self, in_c, out_c, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1, residual=False):\n        super(Depth_Wise, self).__init__()\n        self.conv = Conv_block(in_c, groups, (1, 1), (1, 1), (0, 0))\n        self.conv_dw = Conv_block(groups, groups, kernel, stride, padding, groups=groups)\n        self.project = LinearBlock(groups, out_c, (1, 1), (1, 1), (0, 0))\n        self.residual = residual\n    def forward(self, x):",
        "detail": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "documentation": {}
    },
    {
        "label": "Residual",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "description": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "peekOfCode": "class Residual(Module):\n    \"\"\" Residual block\n    \"\"\"\n    def __init__(self, channel, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n        super(Residual, self).__init__()\n        modules = []\n        for _ in range(num_block):\n            modules.append(Depth_Wise(channel, channel,\n                                      kernel=kernel,\n                                      stride=stride,",
        "detail": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "documentation": {}
    },
    {
        "label": "MobileFaceNet",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "description": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "peekOfCode": "class MobileFaceNet(Module):\n    \"\"\" MobileFaceNet backbone\n    \"\"\"\n    def __init__(self, input_size, embedding_size=512, output_name=\"GDC\"):\n        \"\"\" Args:\n            input_size: input_size of backbone\n            embedding_size: embedding_size of last feature\n            output_name: support GDC or GNAP\n        \"\"\"\n        super(MobileFaceNet, self).__init__()",
        "detail": "TFace.recognition.torchkit.backbone.model_mobilefacenet",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_resnet",
        "description": "TFace.recognition.torchkit.backbone.model_resnet",
        "peekOfCode": "class Bottleneck(Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = BatchNorm2d(planes * self.expansion)",
        "detail": "TFace.recognition.torchkit.backbone.model_resnet",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.backbone.model_resnet",
        "description": "TFace.recognition.torchkit.backbone.model_resnet",
        "peekOfCode": "class ResNet(Module):\n    \"\"\" ResNet backbone\n    \"\"\"\n    def __init__(self, input_size, block, layers, zero_init_residual=True):\n        \"\"\" Args:\n            input_size: input_size of backbone\n            block: block function\n            layers: layers in each block\n        \"\"\"\n        super(ResNet, self).__init__()",
        "detail": "TFace.recognition.torchkit.backbone.model_resnet",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_resnet",
        "description": "TFace.recognition.torchkit.backbone.model_resnet",
        "peekOfCode": "def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\" 3x3 convolution with padding\n    \"\"\"\n    return Conv2d(in_planes,\n                  out_planes,\n                  kernel_size=3,\n                  stride=stride,\n                  padding=1,\n                  bias=False)\ndef conv1x1(in_planes, out_planes, stride=1):",
        "detail": "TFace.recognition.torchkit.backbone.model_resnet",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_resnet",
        "description": "TFace.recognition.torchkit.backbone.model_resnet",
        "peekOfCode": "def conv1x1(in_planes, out_planes, stride=1):\n    \"\"\" 1x1 convolution\n    \"\"\"\n    return Conv2d(in_planes,\n                  out_planes,\n                  kernel_size=1,\n                  stride=stride,\n                  bias=False)\nclass Bottleneck(Module):\n    expansion = 4",
        "detail": "TFace.recognition.torchkit.backbone.model_resnet",
        "documentation": {}
    },
    {
        "label": "ResNet_50",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_resnet",
        "description": "TFace.recognition.torchkit.backbone.model_resnet",
        "peekOfCode": "def ResNet_50(input_size, **kwargs):\n    \"\"\" Constructs a ResNet-50 model.\n    \"\"\"\n    model = ResNet(input_size, Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\ndef ResNet_101(input_size, **kwargs):\n    \"\"\" Constructs a ResNet-101 model.\n    \"\"\"\n    model = ResNet(input_size, Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_resnet",
        "documentation": {}
    },
    {
        "label": "ResNet_101",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_resnet",
        "description": "TFace.recognition.torchkit.backbone.model_resnet",
        "peekOfCode": "def ResNet_101(input_size, **kwargs):\n    \"\"\" Constructs a ResNet-101 model.\n    \"\"\"\n    model = ResNet(input_size, Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\ndef ResNet_152(input_size, **kwargs):\n    \"\"\" Constructs a ResNet-152 model.\n    \"\"\"\n    model = ResNet(input_size, Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_resnet",
        "documentation": {}
    },
    {
        "label": "ResNet_152",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.backbone.model_resnet",
        "description": "TFace.recognition.torchkit.backbone.model_resnet",
        "peekOfCode": "def ResNet_152(input_size, **kwargs):\n    \"\"\" Constructs a ResNet-152 model.\n    \"\"\"\n    model = ResNet(input_size, Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model",
        "detail": "TFace.recognition.torchkit.backbone.model_resnet",
        "documentation": {}
    },
    {
        "label": "SingleDataset",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.data.dataset",
        "description": "TFace.recognition.torchkit.data.dataset",
        "peekOfCode": "class SingleDataset(Dataset):\n    \"\"\" SingleDataset\n    \"\"\"\n    def __init__(self, data_root, index_root, name, transform, **kwargs) -> None:\n        \"\"\" Create a ``SingleDataset`` object\n            Args:\n            data_root: image or tfrecord data root path\n            index_root: index file root path\n            name: dataset name\n            transform: transform for data augmentation",
        "detail": "TFace.recognition.torchkit.data.dataset",
        "documentation": {}
    },
    {
        "label": "MultiDataset",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.data.dataset",
        "description": "TFace.recognition.torchkit.data.dataset",
        "peekOfCode": "class MultiDataset(Dataset):\n    \"\"\" MultiDataset, which contains multiple dataset and should be used\n        together with ``MultiDistributedSampler``\n    \"\"\"\n    def __init__(self, data_root, index_root, names, transform, **kwargs) -> None:\n        \"\"\" Create a ``MultiDataset`` object\n            Args:\n            data_root: image or tfrecord data root path\n            index_root: index file root path\n            name: dataset names for multiple dataset",
        "detail": "TFace.recognition.torchkit.data.dataset",
        "documentation": {}
    },
    {
        "label": "_sym_db",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "_sym_db = _symbol_database.Default()\nfrom . import feature_pb2 as feature__pb2\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='example.proto',\n  package='face',\n  syntax='proto3',\n  serialized_options=b'\\370\\001\\001',\n  serialized_pb=b'\\n\\rexample.proto\\x12\\x04\\x66\\x61\\x63\\x65\\x1a\\rfeature.proto\\\"+\\n\\x07\\x45xample\\x12 \\n\\x08\\x66\\x65\\x61tures\\x18\\x01 \\x01(\\x0b\\x32\\x0e.face.Features\\\"]\\n\\x0fSequenceExample\\x12\\x1f\\n\\x07\\x63ontext\\x18\\x01 \\x01(\\x0b\\x32\\x0e.face.Features\\x12)\\n\\rfeature_lists\\x18\\x02 \\x01(\\x0b\\x32\\x12.face.FeatureListsB\\x03\\xf8\\x01\\x01\\x62\\x06proto3'\n  ,\n  dependencies=[feature__pb2.DESCRIPTOR, ])",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "DESCRIPTOR = _descriptor.FileDescriptor(\n  name='example.proto',\n  package='face',\n  syntax='proto3',\n  serialized_options=b'\\370\\001\\001',\n  serialized_pb=b'\\n\\rexample.proto\\x12\\x04\\x66\\x61\\x63\\x65\\x1a\\rfeature.proto\\\"+\\n\\x07\\x45xample\\x12 \\n\\x08\\x66\\x65\\x61tures\\x18\\x01 \\x01(\\x0b\\x32\\x0e.face.Features\\\"]\\n\\x0fSequenceExample\\x12\\x1f\\n\\x07\\x63ontext\\x18\\x01 \\x01(\\x0b\\x32\\x0e.face.Features\\x12)\\n\\rfeature_lists\\x18\\x02 \\x01(\\x0b\\x32\\x12.face.FeatureListsB\\x03\\xf8\\x01\\x01\\x62\\x06proto3'\n  ,\n  dependencies=[feature__pb2.DESCRIPTOR, ])\n_EXAMPLE = _descriptor.Descriptor(\n  name='Example',",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "_EXAMPLE",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "_EXAMPLE = _descriptor.Descriptor(\n  name='Example',\n  full_name='face.Example',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='features', full_name='face.Example.features', index=0,\n      number=1, type=11, cpp_type=10, label=1,",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "_SEQUENCEEXAMPLE",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "_SEQUENCEEXAMPLE = _descriptor.Descriptor(\n  name='SequenceExample',\n  full_name='face.SequenceExample',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='context', full_name='face.SequenceExample.context', index=0,\n      number=1, type=11, cpp_type=10, label=1,",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "_EXAMPLE.fields_by_name['features'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "_EXAMPLE.fields_by_name['features'].message_type = feature__pb2._FEATURES\n_SEQUENCEEXAMPLE.fields_by_name['context'].message_type = feature__pb2._FEATURES\n_SEQUENCEEXAMPLE.fields_by_name['feature_lists'].message_type = feature__pb2._FEATURELISTS\nDESCRIPTOR.message_types_by_name['Example'] = _EXAMPLE\nDESCRIPTOR.message_types_by_name['SequenceExample'] = _SEQUENCEEXAMPLE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nExample = _reflection.GeneratedProtocolMessageType('Example', (_message.Message,), {\n  'DESCRIPTOR': _EXAMPLE,\n  '__module__': 'example_pb2'\n  # @@protoc_insertion_point(class_scope:face.Example)",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "_SEQUENCEEXAMPLE.fields_by_name['context'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "_SEQUENCEEXAMPLE.fields_by_name['context'].message_type = feature__pb2._FEATURES\n_SEQUENCEEXAMPLE.fields_by_name['feature_lists'].message_type = feature__pb2._FEATURELISTS\nDESCRIPTOR.message_types_by_name['Example'] = _EXAMPLE\nDESCRIPTOR.message_types_by_name['SequenceExample'] = _SEQUENCEEXAMPLE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nExample = _reflection.GeneratedProtocolMessageType('Example', (_message.Message,), {\n  'DESCRIPTOR': _EXAMPLE,\n  '__module__': 'example_pb2'\n  # @@protoc_insertion_point(class_scope:face.Example)\n  })",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "_SEQUENCEEXAMPLE.fields_by_name['feature_lists'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "_SEQUENCEEXAMPLE.fields_by_name['feature_lists'].message_type = feature__pb2._FEATURELISTS\nDESCRIPTOR.message_types_by_name['Example'] = _EXAMPLE\nDESCRIPTOR.message_types_by_name['SequenceExample'] = _SEQUENCEEXAMPLE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nExample = _reflection.GeneratedProtocolMessageType('Example', (_message.Message,), {\n  'DESCRIPTOR': _EXAMPLE,\n  '__module__': 'example_pb2'\n  # @@protoc_insertion_point(class_scope:face.Example)\n  })\n_sym_db.RegisterMessage(Example)",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['Example']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['Example'] = _EXAMPLE\nDESCRIPTOR.message_types_by_name['SequenceExample'] = _SEQUENCEEXAMPLE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nExample = _reflection.GeneratedProtocolMessageType('Example', (_message.Message,), {\n  'DESCRIPTOR': _EXAMPLE,\n  '__module__': 'example_pb2'\n  # @@protoc_insertion_point(class_scope:face.Example)\n  })\n_sym_db.RegisterMessage(Example)\nSequenceExample = _reflection.GeneratedProtocolMessageType('SequenceExample', (_message.Message,), {",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['SequenceExample']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['SequenceExample'] = _SEQUENCEEXAMPLE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nExample = _reflection.GeneratedProtocolMessageType('Example', (_message.Message,), {\n  'DESCRIPTOR': _EXAMPLE,\n  '__module__': 'example_pb2'\n  # @@protoc_insertion_point(class_scope:face.Example)\n  })\n_sym_db.RegisterMessage(Example)\nSequenceExample = _reflection.GeneratedProtocolMessageType('SequenceExample', (_message.Message,), {\n  'DESCRIPTOR': _SEQUENCEEXAMPLE,",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "Example",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "Example = _reflection.GeneratedProtocolMessageType('Example', (_message.Message,), {\n  'DESCRIPTOR': _EXAMPLE,\n  '__module__': 'example_pb2'\n  # @@protoc_insertion_point(class_scope:face.Example)\n  })\n_sym_db.RegisterMessage(Example)\nSequenceExample = _reflection.GeneratedProtocolMessageType('SequenceExample', (_message.Message,), {\n  'DESCRIPTOR': _SEQUENCEEXAMPLE,\n  '__module__': 'example_pb2'\n  # @@protoc_insertion_point(class_scope:face.SequenceExample)",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "SequenceExample",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "SequenceExample = _reflection.GeneratedProtocolMessageType('SequenceExample', (_message.Message,), {\n  'DESCRIPTOR': _SEQUENCEEXAMPLE,\n  '__module__': 'example_pb2'\n  # @@protoc_insertion_point(class_scope:face.SequenceExample)\n  })\n_sym_db.RegisterMessage(SequenceExample)\nDESCRIPTOR._options = None\n# @@protoc_insertion_point(module_scope)",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR._options",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.example_pb2",
        "description": "TFace.recognition.torchkit.data.example_pb2",
        "peekOfCode": "DESCRIPTOR._options = None\n# @@protoc_insertion_point(module_scope)",
        "detail": "TFace.recognition.torchkit.data.example_pb2",
        "documentation": {}
    },
    {
        "label": "_sym_db",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_sym_db = _symbol_database.Default()\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='feature.proto',\n  package='face',\n  syntax='proto3',\n  serialized_options=b'\\370\\001\\001',\n  serialized_pb=b'\\n\\rfeature.proto\\x12\\x04\\x66\\x61\\x63\\x65\\\"\\x1a\\n\\tBytesList\\x12\\r\\n\\x05value\\x18\\x01 \\x03(\\x0c\\\"\\x1e\\n\\tFloatList\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x02\\x42\\x02\\x10\\x01\\\"\\x1e\\n\\tInt64List\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x03\\x42\\x02\\x10\\x01\\\"\\x86\\x01\\n\\x07\\x46\\x65\\x61ture\\x12%\\n\\nbytes_list\\x18\\x01 \\x01(\\x0b\\x32\\x0f.face.BytesListH\\x00\\x12%\\n\\nfloat_list\\x18\\x02 \\x01(\\x0b\\x32\\x0f.face.FloatListH\\x00\\x12%\\n\\nint64_list\\x18\\x03 \\x01(\\x0b\\x32\\x0f.face.Int64ListH\\x00\\x42\\x06\\n\\x04kind\\\"w\\n\\x08\\x46\\x65\\x61tures\\x12,\\n\\x07\\x66\\x65\\x61ture\\x18\\x01 \\x03(\\x0b\\x32\\x1b.face.Features.FeatureEntry\\x1a=\\n\\x0c\\x46\\x65\\x61tureEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x1c\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\r.face.Feature:\\x02\\x38\\x01\\\"-\\n\\x0b\\x46\\x65\\x61tureList\\x12\\x1e\\n\\x07\\x66\\x65\\x61ture\\x18\\x01 \\x03(\\x0b\\x32\\r.face.Feature\\\"\\x90\\x01\\n\\x0c\\x46\\x65\\x61tureLists\\x12\\x39\\n\\x0c\\x66\\x65\\x61ture_list\\x18\\x01 \\x03(\\x0b\\x32#.face.FeatureLists.FeatureListEntry\\x1a\\x45\\n\\x10\\x46\\x65\\x61tureListEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12 \\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x11.face.FeatureList:\\x02\\x38\\x01\\x42\\x03\\xf8\\x01\\x01\\x62\\x06proto3'\n)\n_BYTESLIST = _descriptor.Descriptor(\n  name='BytesList',",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR = _descriptor.FileDescriptor(\n  name='feature.proto',\n  package='face',\n  syntax='proto3',\n  serialized_options=b'\\370\\001\\001',\n  serialized_pb=b'\\n\\rfeature.proto\\x12\\x04\\x66\\x61\\x63\\x65\\\"\\x1a\\n\\tBytesList\\x12\\r\\n\\x05value\\x18\\x01 \\x03(\\x0c\\\"\\x1e\\n\\tFloatList\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x02\\x42\\x02\\x10\\x01\\\"\\x1e\\n\\tInt64List\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x03\\x42\\x02\\x10\\x01\\\"\\x86\\x01\\n\\x07\\x46\\x65\\x61ture\\x12%\\n\\nbytes_list\\x18\\x01 \\x01(\\x0b\\x32\\x0f.face.BytesListH\\x00\\x12%\\n\\nfloat_list\\x18\\x02 \\x01(\\x0b\\x32\\x0f.face.FloatListH\\x00\\x12%\\n\\nint64_list\\x18\\x03 \\x01(\\x0b\\x32\\x0f.face.Int64ListH\\x00\\x42\\x06\\n\\x04kind\\\"w\\n\\x08\\x46\\x65\\x61tures\\x12,\\n\\x07\\x66\\x65\\x61ture\\x18\\x01 \\x03(\\x0b\\x32\\x1b.face.Features.FeatureEntry\\x1a=\\n\\x0c\\x46\\x65\\x61tureEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x1c\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\r.face.Feature:\\x02\\x38\\x01\\\"-\\n\\x0b\\x46\\x65\\x61tureList\\x12\\x1e\\n\\x07\\x66\\x65\\x61ture\\x18\\x01 \\x03(\\x0b\\x32\\r.face.Feature\\\"\\x90\\x01\\n\\x0c\\x46\\x65\\x61tureLists\\x12\\x39\\n\\x0c\\x66\\x65\\x61ture_list\\x18\\x01 \\x03(\\x0b\\x32#.face.FeatureLists.FeatureListEntry\\x1a\\x45\\n\\x10\\x46\\x65\\x61tureListEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12 \\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x11.face.FeatureList:\\x02\\x38\\x01\\x42\\x03\\xf8\\x01\\x01\\x62\\x06proto3'\n)\n_BYTESLIST = _descriptor.Descriptor(\n  name='BytesList',\n  full_name='face.BytesList',",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_BYTESLIST",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_BYTESLIST = _descriptor.Descriptor(\n  name='BytesList',\n  full_name='face.BytesList',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='value', full_name='face.BytesList.value', index=0,\n      number=1, type=12, cpp_type=9, label=3,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FLOATLIST",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FLOATLIST = _descriptor.Descriptor(\n  name='FloatList',\n  full_name='face.FloatList',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='value', full_name='face.FloatList.value', index=0,\n      number=1, type=2, cpp_type=6, label=3,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_INT64LIST",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_INT64LIST = _descriptor.Descriptor(\n  name='Int64List',\n  full_name='face.Int64List',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='value', full_name='face.Int64List.value', index=0,\n      number=1, type=3, cpp_type=2, label=3,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='face.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='bytes_list', full_name='face.Feature.bytes_list', index=0,\n      number=1, type=11, cpp_type=10, label=1,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURES_FEATUREENTRY",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURES_FEATUREENTRY = _descriptor.Descriptor(\n  name='FeatureEntry',\n  full_name='face.Features.FeatureEntry',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='key', full_name='face.Features.FeatureEntry.key', index=0,\n      number=1, type=9, cpp_type=9, label=1,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURES",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURES = _descriptor.Descriptor(\n  name='Features',\n  full_name='face.Features',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='feature', full_name='face.Features.feature', index=0,\n      number=1, type=11, cpp_type=10, label=3,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURELIST",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURELIST = _descriptor.Descriptor(\n  name='FeatureList',\n  full_name='face.FeatureList',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='feature', full_name='face.FeatureList.feature', index=0,\n      number=1, type=11, cpp_type=10, label=3,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURELISTS_FEATURELISTENTRY",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURELISTS_FEATURELISTENTRY = _descriptor.Descriptor(\n  name='FeatureListEntry',\n  full_name='face.FeatureLists.FeatureListEntry',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='key', full_name='face.FeatureLists.FeatureListEntry.key', index=0,\n      number=1, type=9, cpp_type=9, label=1,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURELISTS",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURELISTS = _descriptor.Descriptor(\n  name='FeatureLists',\n  full_name='face.FeatureLists',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='feature_list', full_name='face.FeatureLists.feature_list', index=0,\n      number=1, type=11, cpp_type=10, label=3,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE.fields_by_name['bytes_list'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURE.fields_by_name['bytes_list'].message_type = _BYTESLIST\n_FEATURE.fields_by_name['float_list'].message_type = _FLOATLIST\n_FEATURE.fields_by_name['int64_list'].message_type = _INT64LIST\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['bytes_list'])\n_FEATURE.fields_by_name['bytes_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['float_list'])\n_FEATURE.fields_by_name['float_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE.fields_by_name['float_list'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURE.fields_by_name['float_list'].message_type = _FLOATLIST\n_FEATURE.fields_by_name['int64_list'].message_type = _INT64LIST\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['bytes_list'])\n_FEATURE.fields_by_name['bytes_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['float_list'])\n_FEATURE.fields_by_name['float_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['int64_list'])",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE.fields_by_name['int64_list'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURE.fields_by_name['int64_list'].message_type = _INT64LIST\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['bytes_list'])\n_FEATURE.fields_by_name['bytes_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['float_list'])\n_FEATURE.fields_by_name['float_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['int64_list'])\n_FEATURE.fields_by_name['int64_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE.fields_by_name['bytes_list'].containing_oneof",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURE.fields_by_name['bytes_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['float_list'])\n_FEATURE.fields_by_name['float_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['int64_list'])\n_FEATURE.fields_by_name['int64_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURES_FEATUREENTRY.fields_by_name['value'].message_type = _FEATURE\n_FEATURES_FEATUREENTRY.containing_type = _FEATURES\n_FEATURES.fields_by_name['feature'].message_type = _FEATURES_FEATUREENTRY",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE.fields_by_name['float_list'].containing_oneof",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURE.fields_by_name['float_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURE.oneofs_by_name['kind'].fields.append(\n  _FEATURE.fields_by_name['int64_list'])\n_FEATURE.fields_by_name['int64_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURES_FEATUREENTRY.fields_by_name['value'].message_type = _FEATURE\n_FEATURES_FEATUREENTRY.containing_type = _FEATURES\n_FEATURES.fields_by_name['feature'].message_type = _FEATURES_FEATUREENTRY\n_FEATURELIST.fields_by_name['feature'].message_type = _FEATURE\n_FEATURELISTS_FEATURELISTENTRY.fields_by_name['value'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE.fields_by_name['int64_list'].containing_oneof",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURE.fields_by_name['int64_list'].containing_oneof = _FEATURE.oneofs_by_name['kind']\n_FEATURES_FEATUREENTRY.fields_by_name['value'].message_type = _FEATURE\n_FEATURES_FEATUREENTRY.containing_type = _FEATURES\n_FEATURES.fields_by_name['feature'].message_type = _FEATURES_FEATUREENTRY\n_FEATURELIST.fields_by_name['feature'].message_type = _FEATURE\n_FEATURELISTS_FEATURELISTENTRY.fields_by_name['value'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name['feature_list'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURES_FEATUREENTRY.fields_by_name['value'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURES_FEATUREENTRY.fields_by_name['value'].message_type = _FEATURE\n_FEATURES_FEATUREENTRY.containing_type = _FEATURES\n_FEATURES.fields_by_name['feature'].message_type = _FEATURES_FEATUREENTRY\n_FEATURELIST.fields_by_name['feature'].message_type = _FEATURE\n_FEATURELISTS_FEATURELISTENTRY.fields_by_name['value'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name['feature_list'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURES_FEATUREENTRY.containing_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURES_FEATUREENTRY.containing_type = _FEATURES\n_FEATURES.fields_by_name['feature'].message_type = _FEATURES_FEATUREENTRY\n_FEATURELIST.fields_by_name['feature'].message_type = _FEATURE\n_FEATURELISTS_FEATURELISTENTRY.fields_by_name['value'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name['feature_list'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURES.fields_by_name['feature'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURES.fields_by_name['feature'].message_type = _FEATURES_FEATUREENTRY\n_FEATURELIST.fields_by_name['feature'].message_type = _FEATURE\n_FEATURELISTS_FEATURELISTENTRY.fields_by_name['value'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name['feature_list'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURELIST.fields_by_name['feature'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURELIST.fields_by_name['feature'].message_type = _FEATURE\n_FEATURELISTS_FEATURELISTENTRY.fields_by_name['value'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name['feature_list'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURELISTS_FEATURELISTENTRY.fields_by_name['value'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURELISTS_FEATURELISTENTRY.fields_by_name['value'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name['feature_list'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURELISTS_FEATURELISTENTRY.containing_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name['feature_list'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURELISTS.fields_by_name['feature_list'].message_type",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURELISTS.fields_by_name['feature_list'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nBytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['BytesList']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['BytesList'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nBytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {\n  'DESCRIPTOR' : _BYTESLIST,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['FloatList']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['FloatList'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nBytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {\n  'DESCRIPTOR' : _BYTESLIST,\n  '__module__' : 'feature_pb2'",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['Int64List']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['Int64List'] = _INT64LIST\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nBytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {\n  'DESCRIPTOR' : _BYTESLIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.BytesList)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['Feature']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nBytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {\n  'DESCRIPTOR' : _BYTESLIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.BytesList)\n  })",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['Features']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['Features'] = _FEATURES\nDESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nBytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {\n  'DESCRIPTOR' : _BYTESLIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.BytesList)\n  })\n_sym_db.RegisterMessage(BytesList)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['FeatureList']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['FeatureList'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nBytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {\n  'DESCRIPTOR' : _BYTESLIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.BytesList)\n  })\n_sym_db.RegisterMessage(BytesList)\nFloatList = _reflection.GeneratedProtocolMessageType('FloatList', (_message.Message,), {",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['FeatureLists']",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['FeatureLists'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nBytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {\n  'DESCRIPTOR' : _BYTESLIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.BytesList)\n  })\n_sym_db.RegisterMessage(BytesList)\nFloatList = _reflection.GeneratedProtocolMessageType('FloatList', (_message.Message,), {\n  'DESCRIPTOR' : _FLOATLIST,",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "BytesList",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "BytesList = _reflection.GeneratedProtocolMessageType('BytesList', (_message.Message,), {\n  'DESCRIPTOR' : _BYTESLIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.BytesList)\n  })\n_sym_db.RegisterMessage(BytesList)\nFloatList = _reflection.GeneratedProtocolMessageType('FloatList', (_message.Message,), {\n  'DESCRIPTOR' : _FLOATLIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.FloatList)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "FloatList",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "FloatList = _reflection.GeneratedProtocolMessageType('FloatList', (_message.Message,), {\n  'DESCRIPTOR' : _FLOATLIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.FloatList)\n  })\n_sym_db.RegisterMessage(FloatList)\nInt64List = _reflection.GeneratedProtocolMessageType('Int64List', (_message.Message,), {\n  'DESCRIPTOR' : _INT64LIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.Int64List)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "Int64List",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "Int64List = _reflection.GeneratedProtocolMessageType('Int64List', (_message.Message,), {\n  'DESCRIPTOR' : _INT64LIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.Int64List)\n  })\n_sym_db.RegisterMessage(Int64List)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.Feature)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "Feature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.Feature)\n  })\n_sym_db.RegisterMessage(Feature)\nFeatures = _reflection.GeneratedProtocolMessageType('Features', (_message.Message,), {\n  'FeatureEntry' : _reflection.GeneratedProtocolMessageType('FeatureEntry', (_message.Message,), {\n    'DESCRIPTOR' : _FEATURES_FEATUREENTRY,\n    '__module__' : 'feature_pb2'",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "Features",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "Features = _reflection.GeneratedProtocolMessageType('Features', (_message.Message,), {\n  'FeatureEntry' : _reflection.GeneratedProtocolMessageType('FeatureEntry', (_message.Message,), {\n    'DESCRIPTOR' : _FEATURES_FEATUREENTRY,\n    '__module__' : 'feature_pb2'\n    # @@protoc_insertion_point(class_scope:face.Features.FeatureEntry)\n    })\n  ,\n  'DESCRIPTOR' : _FEATURES,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.Features)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "FeatureList",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "FeatureList = _reflection.GeneratedProtocolMessageType('FeatureList', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURELIST,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.FeatureList)\n  })\n_sym_db.RegisterMessage(FeatureList)\nFeatureLists = _reflection.GeneratedProtocolMessageType('FeatureLists', (_message.Message,), {\n  'FeatureListEntry' : _reflection.GeneratedProtocolMessageType('FeatureListEntry', (_message.Message,), {\n    'DESCRIPTOR' : _FEATURELISTS_FEATURELISTENTRY,\n    '__module__' : 'feature_pb2'",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "FeatureLists",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "FeatureLists = _reflection.GeneratedProtocolMessageType('FeatureLists', (_message.Message,), {\n  'FeatureListEntry' : _reflection.GeneratedProtocolMessageType('FeatureListEntry', (_message.Message,), {\n    'DESCRIPTOR' : _FEATURELISTS_FEATURELISTENTRY,\n    '__module__' : 'feature_pb2'\n    # @@protoc_insertion_point(class_scope:face.FeatureLists.FeatureListEntry)\n    })\n  ,\n  'DESCRIPTOR' : _FEATURELISTS,\n  '__module__' : 'feature_pb2'\n  # @@protoc_insertion_point(class_scope:face.FeatureLists)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR._options",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "DESCRIPTOR._options = None\n_FLOATLIST.fields_by_name['value']._options = None\n_INT64LIST.fields_by_name['value']._options = None\n_FEATURES_FEATUREENTRY._options = None\n_FEATURELISTS_FEATURELISTENTRY._options = None\n# @@protoc_insertion_point(module_scope)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FLOATLIST.fields_by_name['value']._options",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FLOATLIST.fields_by_name['value']._options = None\n_INT64LIST.fields_by_name['value']._options = None\n_FEATURES_FEATUREENTRY._options = None\n_FEATURELISTS_FEATURELISTENTRY._options = None\n# @@protoc_insertion_point(module_scope)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_INT64LIST.fields_by_name['value']._options",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_INT64LIST.fields_by_name['value']._options = None\n_FEATURES_FEATUREENTRY._options = None\n_FEATURELISTS_FEATURELISTENTRY._options = None\n# @@protoc_insertion_point(module_scope)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURES_FEATUREENTRY._options",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURES_FEATUREENTRY._options = None\n_FEATURELISTS_FEATURELISTENTRY._options = None\n# @@protoc_insertion_point(module_scope)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURELISTS_FEATURELISTENTRY._options",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.data.feature_pb2",
        "description": "TFace.recognition.torchkit.data.feature_pb2",
        "peekOfCode": "_FEATURELISTS_FEATURELISTENTRY._options = None\n# @@protoc_insertion_point(module_scope)",
        "detail": "TFace.recognition.torchkit.data.feature_pb2",
        "documentation": {}
    },
    {
        "label": "IndexParser",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.data.parser",
        "description": "TFace.recognition.torchkit.data.parser",
        "peekOfCode": "class IndexParser(object):\n    \"\"\" Class for Line parser.\n    \"\"\"\n    def __init__(self) -> None:\n        self.sample_num = 0\n        self.class_num = 0\n    def __call__(self, line):\n        line_s = line.rstrip().split('\\t')\n        if len(line_s) == 2:\n            # Default line format",
        "detail": "TFace.recognition.torchkit.data.parser",
        "documentation": {}
    },
    {
        "label": "ImgSampleParser",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.data.parser",
        "description": "TFace.recognition.torchkit.data.parser",
        "peekOfCode": "class ImgSampleParser(object):\n    \"\"\" Class for Image Sample parser\n    \"\"\"\n    def __init__(self, transform) -> None:\n        self.transform = transform\n    def __call__(self, path, label):\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            image = self.transform(image)",
        "detail": "TFace.recognition.torchkit.data.parser",
        "documentation": {}
    },
    {
        "label": "TFRecordSampleParser",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.data.parser",
        "description": "TFace.recognition.torchkit.data.parser",
        "peekOfCode": "class TFRecordSampleParser(object):\n    \"\"\" Class for TFRecord Sample parser\n    \"\"\"\n    def __init__(self, transform) -> None:\n        self.transform = transform\n        self.file_readers = dict()\n    def __call__(self, record_path, offset, label):\n        rr = self.file_readers.get(record_path, None)\n        if rr is None:\n            rr = db.RecordReader(record_path)",
        "detail": "TFace.recognition.torchkit.data.parser",
        "documentation": {}
    },
    {
        "label": "MultiDistributedSampler",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.data.sampler",
        "description": "TFace.recognition.torchkit.data.sampler",
        "peekOfCode": "class MultiDistributedSampler(Sampler):\n    \"\"\" MultiDistributedSampler\n    \"\"\"\n    def __init__(self, dataset, batch_sizes, init_seed=0) -> None:\n        \"\"\" Create a ``MultiDistributedSampler`` object\n            A ``MultiDistributedSampler`` object will generate MultiDataset indices,\n            Args:\n                dataset: MultiDataset object\n                batch_sizes: batch sizes for all datasets\n        \"\"\"",
        "detail": "TFace.recognition.torchkit.data.sampler",
        "documentation": {}
    },
    {
        "label": "ArcFace",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.distfc.arcface",
        "description": "TFace.recognition.torchkit.head.distfc.arcface",
        "peekOfCode": "class ArcFace(CommonFace):\n    \"\"\" Implement of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf)\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 gpu_index,\n                 weight_init,\n                 class_split,\n                 scale=64.0,\n                 margin=0.5,",
        "detail": "TFace.recognition.torchkit.head.distfc.arcface",
        "documentation": {}
    },
    {
        "label": "CommonFace",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.distfc.common",
        "description": "TFace.recognition.torchkit.head.distfc.common",
        "peekOfCode": "class CommonFace(nn.Module):\n    \"\"\" CommonFace head\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 gpu_index,\n                 weight_init,\n                 class_split):\n        \"\"\" Args:\n            in_features: size of input features",
        "detail": "TFace.recognition.torchkit.head.distfc.common",
        "documentation": {}
    },
    {
        "label": "CosFace",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.distfc.cosface",
        "description": "TFace.recognition.torchkit.head.distfc.cosface",
        "peekOfCode": "class CosFace(CommonFace):\n    \"\"\" Implement of CosFace (https://arxiv.org/abs/1801.09414)\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 gpu_index,\n                 weight_init,\n                 class_split,\n                 scale=64.0,\n                 margin=0.4):",
        "detail": "TFace.recognition.torchkit.head.distfc.cosface",
        "documentation": {}
    },
    {
        "label": "CurricularFace",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.distfc.curricularface",
        "description": "TFace.recognition.torchkit.head.distfc.curricularface",
        "peekOfCode": "class CurricularFace(CommonFace):\n    \"\"\" Implement of CurricularFace (https://arxiv.org/abs/2004.00288)\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 gpu_index,\n                 weight_init,\n                 class_split,\n                 scale=64.0,\n                 margin=0.5,",
        "detail": "TFace.recognition.torchkit.head.distfc.curricularface",
        "documentation": {}
    },
    {
        "label": "NormFace",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.distfc.normface",
        "description": "TFace.recognition.torchkit.head.distfc.normface",
        "peekOfCode": "class NormFace(CommonFace):\n    \"\"\" Implement of NormFace\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 gpu_index,\n                 weight_init,\n                 class_split,\n                 scale=64.):\n        \"\"\" Args:",
        "detail": "TFace.recognition.torchkit.head.distfc.normface",
        "documentation": {}
    },
    {
        "label": "PartialFC",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.distfc.partial_fc",
        "description": "TFace.recognition.torchkit.head.distfc.partial_fc",
        "peekOfCode": "class PartialFC(nn.Module):\n    \"\"\" Implement of PartialFC (https://arxiv.org/abs/2010.05222)\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 gpu_index,\n                 weight_init,\n                 class_split,\n                 scale=64.0,\n                 margin=0.40):",
        "detail": "TFace.recognition.torchkit.head.distfc.partial_fc",
        "documentation": {}
    },
    {
        "label": "ArcFace",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.localfc.arcface",
        "description": "TFace.recognition.torchkit.head.localfc.arcface",
        "peekOfCode": "class ArcFace(nn.Module):\n    \"\"\" Implement of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf)\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 scale=64.0,\n                 margin=0.5,\n                 easy_margin=False):\n        \"\"\" Args:",
        "detail": "TFace.recognition.torchkit.head.localfc.arcface",
        "documentation": {}
    },
    {
        "label": "Cifp",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.localfc.cifp",
        "description": "TFace.recognition.torchkit.head.localfc.cifp",
        "peekOfCode": "class Cifp(nn.Module):\n    \"\"\" Implement of  (CVPR2021 Consistent Instance False Positive Improves Fairness in Face Recognition)\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 scale=64.0,\n                 margin=0.35):\n        \"\"\" Args:\n            in_features: size of each input features",
        "detail": "TFace.recognition.torchkit.head.localfc.cifp",
        "documentation": {}
    },
    {
        "label": "calc_logits",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.head.localfc.common",
        "description": "TFace.recognition.torchkit.head.localfc.common",
        "peekOfCode": "def calc_logits(embeddings, kernel):\n    \"\"\" calculate original logits\n    \"\"\"\n    embeddings = l2_norm(embeddings, axis=1)\n    kernel_norm = l2_norm(kernel, axis=0)\n    cos_theta = torch.mm(embeddings, kernel_norm)\n    cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n    with torch.no_grad():\n        origin_cos = cos_theta.clone()\n    return cos_theta, origin_cos",
        "detail": "TFace.recognition.torchkit.head.localfc.common",
        "documentation": {}
    },
    {
        "label": "CosFace",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.localfc.cosface",
        "description": "TFace.recognition.torchkit.head.localfc.cosface",
        "peekOfCode": "class CosFace(nn.Module):\n    \"\"\" Implement of CosFace (https://arxiv.org/abs/1801.09414)\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 scale=64.0,\n                 margin=0.40):\n        \"\"\" Args:\n            in_features: size of each input features",
        "detail": "TFace.recognition.torchkit.head.localfc.cosface",
        "documentation": {}
    },
    {
        "label": "CurricularFace",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.head.localfc.curricularface",
        "description": "TFace.recognition.torchkit.head.localfc.curricularface",
        "peekOfCode": "class CurricularFace(nn.Module):\n    \"\"\" Implement of CurricularFace (https://arxiv.org/abs/2004.00288)\n    \"\"\"\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 scale=64.0,\n                 margin=0.5,\n                 alpha=0.1):\n        \"\"\" Args:",
        "detail": "TFace.recognition.torchkit.head.localfc.curricularface",
        "documentation": {}
    },
    {
        "label": "Hook",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.hooks.base_hook",
        "description": "TFace.recognition.torchkit.hooks.base_hook",
        "peekOfCode": "class Hook(object):\n    \"\"\" BaseClass for Hook\n    \"\"\"\n    stages = ('before_run', 'before_train_epoch', 'before_train_iter',\n        'after_train_iter', 'after_train_epoch', 'after_run')\n    def before_run(self, *args):\n        pass\n    def before_train_epoch(self, *args):\n        pass\n    def before_train_iter(self, *args):",
        "detail": "TFace.recognition.torchkit.hooks.base_hook",
        "documentation": {}
    },
    {
        "label": "CheckpointHook",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.hooks.checkpoint_hook",
        "description": "TFace.recognition.torchkit.hooks.checkpoint_hook",
        "peekOfCode": "class CheckpointHook(Hook):\n    \"\"\" CheckpointHook\n    \"\"\"\n    def __init__(self, save_epochs):\n        self.save_epochs = save_epochs\n    def before_run(self, task):\n        task.load_pretrain_model()\n    def after_train_epoch(self, task, epoch):\n        if epoch + 1 in self.save_epochs:\n            task.save_ckpt(epoch + 1)",
        "detail": "TFace.recognition.torchkit.hooks.checkpoint_hook",
        "documentation": {}
    },
    {
        "label": "LearningRateHook",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "description": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "peekOfCode": "class LearningRateHook(Hook):\n    \"\"\" LearningRate Hook, adjust learning rate in training\n    \"\"\"\n    def __init__(self,\n                 learning_rates,\n                 stages,\n                 warmup_step):\n        \"\"\" Create a ``LearningRateHook`` object\n            Args:\n            learning_rates: all learning rates value",
        "detail": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "documentation": {}
    },
    {
        "label": "set_optimizer_lr",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "description": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "peekOfCode": "def set_optimizer_lr(optimizer, lr):\n    if isinstance(optimizer, dict):\n        backbone_opt, head_opts = optimizer['backbone'], optimizer['heads']\n        for param_group in backbone_opt.param_groups:\n            param_group['lr'] = lr\n        for _, head_opt in head_opts.items():\n            for param_group in head_opt.param_groups:\n                param_group['lr'] = lr\n    else:\n        for param_group in optimizer.param_groups:",
        "detail": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "documentation": {}
    },
    {
        "label": "warm_up_lr",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "description": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "peekOfCode": "def warm_up_lr(step, warmup_step, init_lr, optimizer):\n    \"\"\" Warm up learning rate when batch step below warmup steps\n    \"\"\"\n    lr = step * init_lr / warmup_step\n    if step % 500 == 0:\n        logging.info(\"Current step {}, learning rate {}\".format(step, lr))\n    set_optimizer_lr(optimizer, lr)\ndef adjust_lr(epoch, learning_rates, stages, optimizer):\n    \"\"\" Decay the learning rate based on schedule\n    \"\"\"",
        "detail": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "documentation": {}
    },
    {
        "label": "adjust_lr",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "description": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "peekOfCode": "def adjust_lr(epoch, learning_rates, stages, optimizer):\n    \"\"\" Decay the learning rate based on schedule\n    \"\"\"\n    pos = bisect(stages, epoch)\n    lr = learning_rates[pos]\n    logging.info(\"Current epoch {}, learning rate {}\".format(epoch + 1, lr))\n    set_optimizer_lr(optimizer, lr)\nclass LearningRateHook(Hook):\n    \"\"\" LearningRate Hook, adjust learning rate in training\n    \"\"\"",
        "detail": "TFace.recognition.torchkit.hooks.learning_rate_hook",
        "documentation": {}
    },
    {
        "label": "LogHook",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.hooks.log_hook",
        "description": "TFace.recognition.torchkit.hooks.log_hook",
        "peekOfCode": "class LogHook(Hook):\n    \"\"\" LogHook, print log info in training\n    \"\"\"\n    def __init__(self, freq, rank):\n        \"\"\" Create a LogHook object\n            Args:\n            freq: step interval\n            rank: work rank in ddp\n        \"\"\"\n        self.freq = freq",
        "detail": "TFace.recognition.torchkit.hooks.log_hook",
        "documentation": {}
    },
    {
        "label": "SummaryHook",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.hooks.summary_hook",
        "description": "TFace.recognition.torchkit.hooks.summary_hook",
        "peekOfCode": "class SummaryHook(Hook):\n    \"\"\" SummaryHook, write tensorboard summery in training\n    \"\"\"\n    def __init__(self, log_root, freq, rank):\n        \"\"\" Create A SummaryHook object\n            Args:\n            log_root: tensorboard summary root path\n            freq: step interval\n            rank: gpu rank\n        \"\"\"",
        "detail": "TFace.recognition.torchkit.hooks.summary_hook",
        "documentation": {}
    },
    {
        "label": "DDL",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.loss.ddl",
        "description": "TFace.recognition.torchkit.loss.ddl",
        "peekOfCode": "class DDL(torch.nn.Module):\n    \"\"\" Implented of  \"https://arxiv.org/abs/2002.03662\"\n    \"\"\"\n    def __init__(self, pos_kl_weight=0.1, neg_kl_weight=0.02,\n                 order_loss_weight=0.5, positive_threshold=0.0):\n        \"\"\" Args:\n            pos_kl_weight: weight for positive kl loss\n            neg_kl_weight: weight for negative kl loss\n            order_loss_weight: weight for order loss\n            positive_threshold:  threshold fo positive pair",
        "detail": "TFace.recognition.torchkit.loss.ddl",
        "documentation": {}
    },
    {
        "label": "DistCrossEntropyFunc",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.loss.dist_softmax",
        "description": "TFace.recognition.torchkit.loss.dist_softmax",
        "peekOfCode": "class DistCrossEntropyFunc(torch.autograd.Function):\n    \"\"\" CrossEntropy loss is calculated in parallel, allreduce all logits into single gpu and calculate loss.\n        Implemented of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n    \"\"\"\n    @staticmethod\n    def forward(ctx, logit_part, part_labels):\n        ctx.batch_size = logit_part.size(0)\n        # for numerical stability\n        logit_part_max, _ = torch.max(logit_part, dim=1, keepdim=True)\n        dist.all_reduce(logit_part_max, ReduceOp.MAX)",
        "detail": "TFace.recognition.torchkit.loss.dist_softmax",
        "documentation": {}
    },
    {
        "label": "DistCrossEntropy",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.loss.dist_softmax",
        "description": "TFace.recognition.torchkit.loss.dist_softmax",
        "peekOfCode": "class DistCrossEntropy(nn.Module):\n    def __init__(self):\n        super(DistCrossEntropy, self).__init__()\n    def forward(self, logit_part, label_part):\n        return DistCrossEntropyFunc.apply(logit_part, label_part)",
        "detail": "TFace.recognition.torchkit.loss.dist_softmax",
        "documentation": {}
    },
    {
        "label": "BranchMetaInfo",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.task.base_task",
        "description": "TFace.recognition.torchkit.task.base_task",
        "peekOfCode": "class BranchMetaInfo(object):\n    \"\"\" BranchMetaInfo class\n    \"\"\"\n    def __init__(self, name, batch_size, weight=1.0, scale=64.0, margin=0.5):\n        self.name = name\n        self.batch_size = batch_size\n        self.weight = weight\n        self.scale = scale\n        self.margin = margin\nclass BaseTask(object):",
        "detail": "TFace.recognition.torchkit.task.base_task",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.task.base_task",
        "description": "TFace.recognition.torchkit.task.base_task",
        "peekOfCode": "class BaseTask(object):\n    def __init__(self, cfg_file):\n        \"\"\" Create a ``BaseTask`` object\n            A ``BaseTask`` object will create a base class about training scheduler\n            Args:\n            cfg_file: config file, which defines datasets, backbone and head names, loss name\n            and some training envs\n        \"\"\"\n        self.cfg = load_config(cfg_file)\n        self.rank = 0",
        "detail": "TFace.recognition.torchkit.task.base_task",
        "documentation": {}
    },
    {
        "label": "CkptLoader",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.util.checkpoint",
        "description": "TFace.recognition.torchkit.util.checkpoint",
        "peekOfCode": "class CkptLoader(object):\n    @staticmethod\n    def load_backbone(backbone, backbone_resume, local_rank):\n        \"\"\" load pretrain backbone checkpoint\n        \"\"\"\n        if not path.isfile(backbone_resume):\n            logging.info(\"Backbone checkpoint %s not exists\" % backbone_resume)\n        else:\n            # For DDP trained model, it should specify map_location device in load\n            device = torch.device(\"cuda:%d\" % local_rank)",
        "detail": "TFace.recognition.torchkit.util.checkpoint",
        "documentation": {}
    },
    {
        "label": "CkptSaver",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.util.checkpoint",
        "description": "TFace.recognition.torchkit.util.checkpoint",
        "peekOfCode": "class CkptSaver(object):\n    @staticmethod\n    def save_backbone(backbone, model_root, epoch, rank):\n        \"\"\" save backbone ckpt\n        \"\"\"\n        if rank == 0:\n            backbone_path = path.join(model_root, \"Backbone_Epoch_%d_checkpoint.pth\" % epoch)\n            torch.save(backbone.module.state_dict(), backbone_path)\n    @staticmethod\n    def save_heads(heads, model_root, epoch, dist_fc, rank):",
        "detail": "TFace.recognition.torchkit.util.checkpoint",
        "documentation": {}
    },
    {
        "label": "AllGatherFunc",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.util.distributed_functions",
        "description": "TFace.recognition.torchkit.util.distributed_functions",
        "peekOfCode": "class AllGatherFunc(Function):\n    \"\"\" AllGather op with gradient backward\n    \"\"\"\n    @staticmethod\n    def forward(ctx, tensor, world_size):\n        gather_list = [torch.zeros_like(tensor) for _ in range(world_size)]\n        dist.all_gather(gather_list, tensor)\n        return tuple(gather_list)\n    @staticmethod\n    def backward(ctx, *grads):",
        "detail": "TFace.recognition.torchkit.util.distributed_functions",
        "documentation": {}
    },
    {
        "label": "AllGather",
        "kind": 5,
        "importPath": "TFace.recognition.torchkit.util.distributed_functions",
        "description": "TFace.recognition.torchkit.util.distributed_functions",
        "peekOfCode": "AllGather = AllGatherFunc.apply",
        "detail": "TFace.recognition.torchkit.util.distributed_functions",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "class AverageMeter(object):\n    \"\"\" Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "Timer",
        "kind": 6,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "class Timer(object):\n    \"\"\" Timer for count duration\n    \"\"\"\n    def __init__(self):\n        self.start_time = time.time()\n        self.count = 0\n        self.capacity = 500\n    def get_duration(self):\n        self.count += 1\n        duration = (time.time() - self.start_time) / self.count",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "def l2_norm(input, axis=1):\n    \"\"\" l2 normalize\n    \"\"\"\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n    return output\n@torch.no_grad()\ndef all_gather_tensor(input_tensor, dim=0):\n    \"\"\" allgather tensor from all workers\n    \"\"\"",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "all_gather_tensor",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "def all_gather_tensor(input_tensor, dim=0):\n    \"\"\" allgather tensor from all workers\n    \"\"\"\n    world_size = dist.get_world_size()\n    tensor_size = torch.tensor([input_tensor.shape[0]], dtype=torch.int64).cuda()\n    tensor_size_list = [torch.ones_like(tensor_size) for _ in range(world_size)]\n    dist.all_gather(tensor_list=tensor_size_list, tensor=tensor_size, async_op=False)\n    max_size = torch.cat(tensor_size_list, dim=0).max()\n    padded = torch.empty(max_size.item(), *input_tensor.shape[1:], dtype=input_tensor.dtype).cuda()\n    padded[:input_tensor.shape[0]] = input_tensor",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "get_class_split",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "def get_class_split(num_classes, num_gpus):\n    \"\"\" split the num of classes by num of gpus\n    \"\"\"\n    class_split = []\n    for i in range(num_gpus):\n        _class_num = num_classes // num_gpus\n        if i < (num_classes % num_gpus):\n            _class_num += 1\n        class_split.append(_class_num)\n    return class_split",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "separate_irse_bn_paras",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "def separate_irse_bn_paras(modules):\n    \"\"\" sepeated bn params and wo-bn params\n    \"\"\"\n    if not isinstance(modules, list):\n        modules = [*modules.modules()]\n    paras_only_bn = []\n    paras_wo_bn = []\n    for layer in modules:\n        if 'model' in str(layer.__class__):\n            continue",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "separate_resnet_bn_paras",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "def separate_resnet_bn_paras(modules):\n    \"\"\" sepeated bn params and wo-bn params\n    \"\"\"\n    all_parameters = modules.parameters()\n    paras_only_bn = []\n    for pname, param in modules.named_parameters():\n        if pname.find('bn') >= 0:\n            paras_only_bn.append(param)\n    paras_only_bn_id = list(map(id, paras_only_bn))\n    paras_wo_bn = list(filter(lambda p: id(p) not in paras_only_bn_id,",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "def accuracy(output, target, topk=(1,)):\n    \"\"\" Computes the precision@k for the specified values of k\n    \"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []\n    for k in topk:",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "accuracy_dist",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "def accuracy_dist(cfg, outputs, labels, class_split, topk=(1,)):\n    \"\"\" Computes the precision@k for the specified values of k in parallel\n    \"\"\"\n    assert cfg['WORLD_SIZE'] == len(class_split), \\\n        \"world size should equal to the number of class split\"\n    base = sum(class_split[:cfg['RANK']])\n    maxk = max(topk)\n    # add each gpu part max index by base\n    scores, preds = outputs.topk(maxk, 1, True, True)\n    preds += base",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "TFace.recognition.torchkit.util.utils",
        "description": "TFace.recognition.torchkit.util.utils",
        "peekOfCode": "def load_config(config_file):\n    with open(config_file, 'r') as ifs:\n        config = yaml.safe_load(ifs)\n    return config",
        "detail": "TFace.recognition.torchkit.util.utils",
        "documentation": {}
    },
    {
        "label": "TrainTask",
        "kind": 6,
        "importPath": "TFace.recognition.train",
        "description": "TFace.recognition.train",
        "peekOfCode": "class TrainTask(BaseTask):\n    \"\"\" TrainTask in distfc mode, which means classifier shards into multi workers\n    \"\"\"\n    def __init__(self, cfg_file):\n        super(TrainTask, self).__init__(cfg_file)\n    def update_log_and_summary(self, am_loss, am_top1, am_top5):\n        scalars = {\n            'train/loss': am_loss,\n            'train/top1': am_top1,\n            'train/top5': am_top5,",
        "detail": "TFace.recognition.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.recognition.train",
        "description": "TFace.recognition.train",
        "peekOfCode": "def main():\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    task = TrainTask(os.path.join(task_dir, 'train.yaml'))\n    task.init_env()\n    task.train()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.recognition.train",
        "documentation": {}
    },
    {
        "label": "create_base_dataloader",
        "kind": 2,
        "importPath": "TFace.security.common.data.base_dataloader",
        "description": "TFace.security.common.data.base_dataloader",
        "peekOfCode": "def create_base_dataloader(args, dataset, split):\n    \"\"\"Base data loader\n    Args:\n        args: Dataset config args\n        split (string): Load \"train\", \"val\" or \"test\"\n    Returns:\n        [dataloader]: Corresponding Dataloader\n    \"\"\"\n    sampler = None\n    if args.distributed:",
        "detail": "TFace.security.common.data.base_dataloader",
        "documentation": {}
    },
    {
        "label": "create_base_transforms",
        "kind": 2,
        "importPath": "TFace.security.common.data.base_transform",
        "description": "TFace.security.common.data.base_transform",
        "peekOfCode": "def create_base_transforms(args, split='train'):\n    \"\"\"Base data transformation\n    Args:\n        args: Data transformation args\n        split (str, optional): Defaults to 'train'.\n    Returns:\n        [transform]: Data transform\n    \"\"\"\n    num_segments = args.num_segments if 'num_segments' in args else 1\n    additional_targets = {}",
        "detail": "TFace.security.common.data.base_transform",
        "documentation": {}
    },
    {
        "label": "EuclideanLoss",
        "kind": 6,
        "importPath": "TFace.security.common.losses.euclidean_loss",
        "description": "TFace.security.common.losses.euclidean_loss",
        "peekOfCode": "class EuclideanLoss(nn.Module):\n    '''Compute euclidean distance between two tensors\n    '''\n    def __init__(self, reduction=None):\n        super(EuclideanLoss, self).__init__()\n        self.reduction = reduction\n    def forward(self, x, y):\n        n = x.size(0)\n        m = n\n        d = x.size(1)",
        "detail": "TFace.security.common.losses.euclidean_loss",
        "documentation": {}
    },
    {
        "label": "test_module",
        "kind": 2,
        "importPath": "TFace.security.common.task.fas.modules",
        "description": "TFace.security.common.task.fas.modules",
        "peekOfCode": "def test_module(model, test_data_loaders, forward_function, device='cuda', distributed=False):\n    \"\"\"Test module for Face Anti-spoofing\n    Args:\n        model (nn.module): fas model\n        test_data_loaders (torch.dataloader): list of test data loaders\n        forward_function (function): model forward function\n        device (str, optional): Defaults to 'cuda'.\n        distributed (bool, optional): whether to use distributed training. Defaults to False.\n    Returns:\n        y_preds (list): predictions",
        "detail": "TFace.security.common.task.fas.modules",
        "documentation": {}
    },
    {
        "label": "BaseTask",
        "kind": 6,
        "importPath": "TFace.security.common.task.base_task",
        "description": "TFace.security.common.task.base_task",
        "peekOfCode": "class BaseTask(object):\n    def __init__(self, cfg, task_dir, logger=None):\n        self.cfg = cfg\n        self.task_dir = task_dir\n        self.device = torch.device(\"cuda\", self.cfg.local_rank)\n        if logger is None:\n            self.logger = loguru.logger\n    def prepare(self):\n        self._init_env()\n        self._build_inputs()",
        "detail": "TFace.security.common.task.base_task",
        "documentation": {}
    },
    {
        "label": "flatten_dict",
        "kind": 2,
        "importPath": "TFace.security.common.utils.cli_utils",
        "description": "TFace.security.common.utils.cli_utils",
        "peekOfCode": "def flatten_dict(dictionary, delimiter ='.'):\n    flat_dict = dict()\n    for key, value in dictionary.items():\n        if isinstance(value, dict):\n            flatten_value_dict = flatten_dict(value, delimiter)\n            for k, v in flatten_value_dict.items():\n                flat_dict[f\"{key}{delimiter}{k}\"] = v\n        else:\n            flat_dict[key] = value\n    return flat_dict",
        "detail": "TFace.security.common.utils.cli_utils",
        "documentation": {}
    },
    {
        "label": "nested_set",
        "kind": 2,
        "importPath": "TFace.security.common.utils.cli_utils",
        "description": "TFace.security.common.utils.cli_utils",
        "peekOfCode": "def nested_set(dic, keys, value):\n    for key in keys[:-1]:\n        dic = dic.setdefault(key, {})\n    dic[keys[-1]] = value\ndef warn_print(x):\n    \"\"\"Print info in the highlighted effect.\n    Args:\n        x (any): Info to print.\n    \"\"\"\n    x = str(x)",
        "detail": "TFace.security.common.utils.cli_utils",
        "documentation": {}
    },
    {
        "label": "warn_print",
        "kind": 2,
        "importPath": "TFace.security.common.utils.cli_utils",
        "description": "TFace.security.common.utils.cli_utils",
        "peekOfCode": "def warn_print(x):\n    \"\"\"Print info in the highlighted effect.\n    Args:\n        x (any): Info to print.\n    \"\"\"\n    x = str(x)\n    x = \"\\x1b[33;1m\" + x + \"\\x1b[0m\"\n    print(x)\ndef get_params(to_dict=False, **new_kwargs):\n    \"\"\"Parse the parameters from a yaml config file, and allow for param modification in the command line.",
        "detail": "TFace.security.common.utils.cli_utils",
        "documentation": {}
    },
    {
        "label": "get_params",
        "kind": 2,
        "importPath": "TFace.security.common.utils.cli_utils",
        "description": "TFace.security.common.utils.cli_utils",
        "peekOfCode": "def get_params(to_dict=False, **new_kwargs):\n    \"\"\"Parse the parameters from a yaml config file, and allow for param modification in the command line.\n    The input '-c' or '--config' must be specified.\n    Three arguments have default presets: distributed, local_rank and world_size.\n    Argument priority: cmd args > new_kwargs > dict in config.\n    Args:\n        to_dict (bool, optional): \n            Whether to return the parsed args in Python Dict type. Defaults to False.\n        new_kwargs (**kwargs):\n            Allow for overloading some params by specifying new params here.",
        "detail": "TFace.security.common.utils.cli_utils",
        "documentation": {}
    },
    {
        "label": "reduce_tensor",
        "kind": 2,
        "importPath": "TFace.security.common.utils.distribute_utils",
        "description": "TFace.security.common.utils.distribute_utils",
        "peekOfCode": "def reduce_tensor(tensor, mean=True):\n    \"\"\"Reduce tensor in the distributed settting.\n    Args:\n        tensor (torch.tensor): \n            Input torch tensor to reduce.\n        mean (bool, optional): \n            Whether to apply mean. Defaults to True.\n    Returns:\n        [torch.tensor]: Returned reduced torch tensor or.\n    \"\"\"",
        "detail": "TFace.security.common.utils.distribute_utils",
        "documentation": {}
    },
    {
        "label": "gather_tensor",
        "kind": 2,
        "importPath": "TFace.security.common.utils.distribute_utils",
        "description": "TFace.security.common.utils.distribute_utils",
        "peekOfCode": "def gather_tensor(inp, world_size=None, dist_=True, to_numpy=False):\n    \"\"\"Gather tensor in the distributed setting.\n    Args:\n        inp (torch.tensor): \n            Input torch tensor to gather.\n        world_size (int, optional): \n            Dist world size. Defaults to None. If None, world_size = dist.get_world_size().\n        dist_ (bool, optional):\n            Whether to use all_gather method to gather all the tensors. Defaults to True.\n        to_numpy (bool, optional): ",
        "detail": "TFace.security.common.utils.distribute_utils",
        "documentation": {}
    },
    {
        "label": "add_face_margin",
        "kind": 2,
        "importPath": "TFace.security.common.utils.face_utils",
        "description": "TFace.security.common.utils.face_utils",
        "peekOfCode": "def add_face_margin(x, y, w, h, margin=0.5):\n    \"\"\"Add marigin to face bounding box\n    \"\"\"\n    x_marign = int(w * margin / 2)\n    y_marign = int(h * margin / 2)\n    x1 = x - x_marign\n    x2 = x + w + x_marign\n    y1 = y - y_marign\n    y2 = y + h + y_marign\n    return x1, x2, y1, y2",
        "detail": "TFace.security.common.utils.face_utils",
        "documentation": {}
    },
    {
        "label": "get_face_box",
        "kind": 2,
        "importPath": "TFace.security.common.utils.face_utils",
        "description": "TFace.security.common.utils.face_utils",
        "peekOfCode": "def get_face_box(img, landmarks, margin):\n    \"\"\"Get faca bounding box from landmarks\n    Args:\n        img (np.array): input image\n        landmarks (np.array): face landmarks\n        margin (float): margin for face box\n    Returns:\n        list: face bouding box\n    \"\"\"\n    # load the positions of five landmarks",
        "detail": "TFace.security.common.utils.face_utils",
        "documentation": {}
    },
    {
        "label": "MultiProcessingHandler",
        "kind": 6,
        "importPath": "TFace.security.common.utils.logger_utils",
        "description": "TFace.security.common.utils.logger_utils",
        "peekOfCode": "class MultiProcessingHandler(logging.Handler):\n    def __init__(self, name, sub_handler=None):\n        \"\"\"Multiprocessing logger handler. Code from:\n        https://github.com/jruere/multiprocessing-logging/blob/master/multiprocessing_logging.py\n        Args:\n            name (str): Logger name. The same logger name refers to the same logger instance.\n            sub_handler (logging.Handler, optional): The additional sub_handler to emit records. Defaults to None.\n        \"\"\"\n        super(MultiProcessingHandler, self).__init__()\n        if sub_handler is None:",
        "detail": "TFace.security.common.utils.logger_utils",
        "documentation": {}
    },
    {
        "label": "CustomFormatter",
        "kind": 6,
        "importPath": "TFace.security.common.utils.logger_utils",
        "description": "TFace.security.common.utils.logger_utils",
        "peekOfCode": "class CustomFormatter(logging.Formatter):\n    \"\"\"Logging Formatter to add colors\n    \"\"\"\n    # format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\"\n    format_ = \"%(asctime)s | %(message)s\"\n    FORMATS = {\n        logging.DEBUG: logging_color_set[\"underline_grey\"] + format_ + logging_color_set[\"reset\"],\n        logging.INFO: logging_color_set[\"grey\"] + format_ + logging_color_set[\"reset\"],\n        logging.WARNING: logging_color_set[\"yellow\"] + format_ + logging_color_set[\"reset\"],\n        logging.ERROR: logging_color_set[\"red\"] + format_ + logging_color_set[\"reset\"],",
        "detail": "TFace.security.common.utils.logger_utils",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "TFace.security.common.utils.logger_utils",
        "description": "TFace.security.common.utils.logger_utils",
        "peekOfCode": "def get_logger(logger_name='', console=True, log_path=None, level=\"DEBUG\", \n              multi_processing=False, propagate=False, file_mode='w'):\n    \"\"\"A logger wrapper function.\n    Args:\n        logger_name (str, optional): \n            The name for the logger. loggers with the same name all point to the same logger instance. Defaults to ''.\n        console (bool, optional): \n            If True, redirect the logging message to the stdout. Defaults to True.\n        log_path (str, optional): \n            If set, redirect the logging message to the log file. Defaults to None.",
        "detail": "TFace.security.common.utils.logger_utils",
        "documentation": {}
    },
    {
        "label": "original_print",
        "kind": 5,
        "importPath": "TFace.security.common.utils.logger_utils",
        "description": "TFace.security.common.utils.logger_utils",
        "peekOfCode": "original_print = print\nlogging_color_set = {\n    \"underline_grey\":  \"\\033[4m\",\n    \"grey\": \"\\x1b[38;21m\",\n    \"yellow\":  \"\\x1b[33;1m\",\n    \"red\": \"\\x1b[31;1m\",\n    \"green\": \"\\033[32m\",\n    \"reset\":  \"\\x1b[0m\",\n}\nclass MultiProcessingHandler(logging.Handler):",
        "detail": "TFace.security.common.utils.logger_utils",
        "documentation": {}
    },
    {
        "label": "logging_color_set",
        "kind": 5,
        "importPath": "TFace.security.common.utils.logger_utils",
        "description": "TFace.security.common.utils.logger_utils",
        "peekOfCode": "logging_color_set = {\n    \"underline_grey\":  \"\\033[4m\",\n    \"grey\": \"\\x1b[38;21m\",\n    \"yellow\":  \"\\x1b[33;1m\",\n    \"red\": \"\\x1b[31;1m\",\n    \"green\": \"\\033[32m\",\n    \"reset\":  \"\\x1b[0m\",\n}\nclass MultiProcessingHandler(logging.Handler):\n    def __init__(self, name, sub_handler=None):",
        "detail": "TFace.security.common.utils.logger_utils",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "kind": 6,
        "importPath": "TFace.security.common.utils.meters",
        "description": "TFace.security.common.utils.meters",
        "peekOfCode": "class AverageMeter(object):\n    '''\n        The Class AverageMeter record the metrics during the training process\n        Examples:\n            >>> acces = AverageMeter('_Acc', ':.5f')\n            >>> acc = (prediction == labels).float().mean()\n            >>> acces.update(acc)\n    '''\n    def __init__(self, name='metric', fmt=':f'):\n        self.name = name",
        "detail": "TFace.security.common.utils.meters",
        "documentation": {}
    },
    {
        "label": "ProgressMeter",
        "kind": 6,
        "importPath": "TFace.security.common.utils.meters",
        "description": "TFace.security.common.utils.meters",
        "peekOfCode": "class ProgressMeter(object):\n    '''\n        The ProgressMeter to record all AverageMeter and print the results\n        Examples:\n            >>> acces = AverageMeter('_Acc', ':.5f')\n            >>> progress = ProgressMeter(epoch_size, [acces]) \n            >>> progress.display(iterations)\n    '''\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)",
        "detail": "TFace.security.common.utils.meters",
        "documentation": {}
    },
    {
        "label": "find_best_threshold",
        "kind": 2,
        "importPath": "TFace.security.common.utils.metrics",
        "description": "TFace.security.common.utils.metrics",
        "peekOfCode": "def find_best_threshold(y_trues, y_preds):\n    '''\n        This function is utilized to find the threshold corresponding to the best ACER\n        Args:\n            y_trues (list): the list of the ground-truth labels, which contains the int data\n            y_preds (list): the list of the predicted results, which contains the float data\n    '''\n    print(\"Finding best threshold...\")\n    best_thre = 0.5\n    best_metrics = None",
        "detail": "TFace.security.common.utils.metrics",
        "documentation": {}
    },
    {
        "label": "cal_metrics",
        "kind": 2,
        "importPath": "TFace.security.common.utils.metrics",
        "description": "TFace.security.common.utils.metrics",
        "peekOfCode": "def cal_metrics(y_trues, y_preds, threshold=0.5):\n    '''\n        This function is utilized to calculate the performance of the methods\n        Args:\n            y_trues (list): the list of the ground-truth labels, which contains the int data\n            y_preds (list): the list of the predicted results, which contains the float data\n            threshold (float, optional): \n                'best': calculate the best results\n                'auto': calculate the results corresponding to the thresholds of EER\n                float: calculate the results of the specific thresholds",
        "detail": "TFace.security.common.utils.metrics",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "TFace.security.common.utils.misc",
        "description": "TFace.security.common.utils.misc",
        "peekOfCode": "def set_seed(SEED):\n    \"\"\"This function set the random seed for the training process\n    Args:\n        SEED (int): the random seed\n    \"\"\"\n    if SEED:\n        random.seed(SEED)\n        np.random.seed(SEED)\n        torch.manual_seed(SEED)\n        torch.cuda.manual_seed(SEED)",
        "detail": "TFace.security.common.utils.misc",
        "documentation": {}
    },
    {
        "label": "setup",
        "kind": 2,
        "importPath": "TFace.security.common.utils.misc",
        "description": "TFace.security.common.utils.misc",
        "peekOfCode": "def setup(cfg):\n    if getattr(cfg, 'torch_home', None):\n        os.environ['TORCH_HOME'] = cfg.torch_home\n    warnings.filterwarnings(\"ignore\")\n    seed = cfg.seed\n    set_seed(seed)\ndef init_exam_dir(cfg):\n    if cfg.local_rank == 0:\n        if not os.path.exists(cfg.exam_dir):\n            os.makedirs(cfg.exam_dir)",
        "detail": "TFace.security.common.utils.misc",
        "documentation": {}
    },
    {
        "label": "init_exam_dir",
        "kind": 2,
        "importPath": "TFace.security.common.utils.misc",
        "description": "TFace.security.common.utils.misc",
        "peekOfCode": "def init_exam_dir(cfg):\n    if cfg.local_rank == 0:\n        if not os.path.exists(cfg.exam_dir):\n            os.makedirs(cfg.exam_dir)\n        ckpt_dir = os.path.join(cfg.exam_dir, 'ckpt')\n        if not os.path.exists(ckpt_dir):\n            os.makedirs(ckpt_dir)\ndef init_wandb_workspace(cfg):\n    \"\"\"This function initializes the wandb workspace\n    \"\"\"",
        "detail": "TFace.security.common.utils.misc",
        "documentation": {}
    },
    {
        "label": "init_wandb_workspace",
        "kind": 2,
        "importPath": "TFace.security.common.utils.misc",
        "description": "TFace.security.common.utils.misc",
        "peekOfCode": "def init_wandb_workspace(cfg):\n    \"\"\"This function initializes the wandb workspace\n    \"\"\"\n    if cfg.wandb.name is None:\n        cfg.wandb.name = cfg.config.split('/')[-1].replace('.yaml', '')\n    wandb.init(**cfg.wandb)\n    allow_val_change = False if cfg.wandb.resume is None else True\n    wandb.config.update(cfg, allow_val_change)\n    wandb.save(cfg.config)\n    if cfg.debug or wandb.run.dir == '/tmp':",
        "detail": "TFace.security.common.utils.misc",
        "documentation": {}
    },
    {
        "label": "save_test_results",
        "kind": 2,
        "importPath": "TFace.security.common.utils.misc",
        "description": "TFace.security.common.utils.misc",
        "peekOfCode": "def save_test_results(img_paths, y_preds, y_trues, filename='results.log'):\n    assert len(y_trues) == len(y_preds) == len(img_paths)\n    with open(filename, 'w') as f:\n        for i in range(len(img_paths)):\n            print(img_paths[i], end=' ', file=f)\n            print(y_preds[i], file=f)\n            print(y_trues[i], end=' ', file=f)",
        "detail": "TFace.security.common.utils.misc",
        "documentation": {}
    },
    {
        "label": "weights_init_xavier",
        "kind": 2,
        "importPath": "TFace.security.common.utils.model_init",
        "description": "TFace.security.common.utils.model_init",
        "peekOfCode": "def weights_init_xavier(m):\n    ''' Xavier initialization '''\n    classname = m.__class__.__name__\n    if classname.find('Conv2d') != -1:\n        init.xavier_normal_(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal_(m.weight.data, gain=1)\n        if m.bias is not None:\n            init.constant_(m.bias.data, 0.0)\n    elif classname.find('BatchNorm2d') != -1:",
        "detail": "TFace.security.common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "weights_init_normal",
        "kind": 2,
        "importPath": "TFace.security.common.utils.model_init",
        "description": "TFace.security.common.utils.model_init",
        "peekOfCode": "def weights_init_normal(m):\n    ''' Normal initialization '''\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        init.uniform(m.weight.data, 0.0, 0.02)\n    elif classname.find('Linear') != -1:\n        init.uniform(m.weight.data, 0.0, 0.02)\n        if m.bias is not None:\n            init.constant_(m.bias.data, 0.0)\n    elif classname.find('BatchNorm2d') != -1:",
        "detail": "TFace.security.common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "weights_init_kaiming",
        "kind": 2,
        "importPath": "TFace.security.common.utils.model_init",
        "description": "TFace.security.common.utils.model_init",
        "peekOfCode": "def weights_init_kaiming(m):\n    ''' Kaiming initialization '''\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('Linear') != -1:\n        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n        if m.bias is not None:\n            init.constant_(m.bias.data, 0.0)\n    elif classname.find('BatchNorm2d') != -1:",
        "detail": "TFace.security.common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "weights_init_orthogonal",
        "kind": 2,
        "importPath": "TFace.security.common.utils.model_init",
        "description": "TFace.security.common.utils.model_init",
        "peekOfCode": "def weights_init_orthogonal(m):\n    ''' Orthogonal initialization '''\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        init.orthogonal_(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.orthogonal_(m.weight.data, gain=1)\n        if m.bias is not None:\n            init.constant_(m.bias.data, 0.0)\n    elif classname.find('BatchNorm2d') != -1:",
        "detail": "TFace.security.common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "weights_init_orthogonal_rnn",
        "kind": 2,
        "importPath": "TFace.security.common.utils.model_init",
        "description": "TFace.security.common.utils.model_init",
        "peekOfCode": "def weights_init_orthogonal_rnn(m):\n    ''' Orthogonal_RNN initialization '''\n    classname = m.__class__.__name__\n    if classname.find('LSTM') != -1:\n        init.orthogonal_(m.all_weights[0][0], gain=1)\n        init.orthogonal_(m.all_weights[0][1], gain=1)\n        init.constant_(m.all_weights[0][2], 1)\n        init.constant_(m.all_weights[0][3], 1)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal_(m.weight.data, gain=1)",
        "detail": "TFace.security.common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "TFace.security.common.utils.model_init",
        "description": "TFace.security.common.utils.model_init",
        "peekOfCode": "def init_weights(net, init_type='normal'):\n    if init_type == 'normal':\n        net.apply(weights_init_normal)\n    elif init_type == 'xavier':\n        net.apply(weights_init_xavier)\n    elif init_type == 'kaiming':\n        net.apply(weights_init_kaiming)\n    elif init_type == 'orthogonal':\n        net.apply(weights_init_orthogonal)\n    elif init_type == 'orthogonal_rnn':",
        "detail": "TFace.security.common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "init_model",
        "kind": 2,
        "importPath": "TFace.security.common.utils.model_init",
        "description": "TFace.security.common.utils.model_init",
        "peekOfCode": "def init_model(net, restore, init_type, init=True):\n    \"\"\"Init models with cuda and weights.\"\"\"\n    # init weights of model\n    if init:\n        init_weights(net, init_type)\n    # restore model weights\n    if restore is not None:\n        if os.path.exists(restore):\n            # original saved file with DataParallel\n            state_dict = torch.load(restore)",
        "detail": "TFace.security.common.utils.model_init",
        "documentation": {}
    },
    {
        "label": "get_parameters",
        "kind": 2,
        "importPath": "TFace.security.common.utils.parameters",
        "description": "TFace.security.common.utils.parameters",
        "peekOfCode": "def get_parameters():\n    \"\"\"define the parameter for training\n    Args:\n        --config (string): the path of config files\n        --distributed (int): train the model in the mode of DDP or Not, default: 1\n        --local_rank (int): define the rank of this process\n        --world_size (int): define the Number of GPU\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-c', '--config', type=str, default='configs/train.yaml')",
        "detail": "TFace.security.common.utils.parameters",
        "documentation": {}
    },
    {
        "label": "laplacian_matrix",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "peekOfCode": "def laplacian_matrix(n, m):\n    \"\"\"Generate the Poisson matrix. \n    Refer to: \n    https://en.wikipedia.org/wiki/Discrete_Poisson_equation\n    Note: it's the transpose of the wiki's matrix \n    \"\"\"\n    mat_D = scipy.sparse.lil_matrix((m, m))\n    mat_D.setdiag(-1, -1)\n    mat_D.setdiag(4)\n    mat_D.setdiag(-1, 1)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "documentation": {}
    },
    {
        "label": "poisson_edit",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "peekOfCode": "def poisson_edit(source, target, mask):\n    \"\"\"The poisson blending function. \n    Refer to: \n    Perez et. al., \"Poisson Image Editing\", 2003.\n    \"\"\"\n    # Assume: \n    # target is not smaller than source.\n    # shape of mask is same as shape of target.\n    y_max, x_max = target.shape[:-1]\n    y_min, x_min = 0, 0",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "peekOfCode": "def main():    \n    before_dir = '../Datasets_Makeup/before_aligned_600'\n    test_imgs_tmp = '../Datasets_Makeup/test_imgs_tmp'\n    poisson_res_dir = '../Datasets_Makeup/test_imgs_poisson'\n    api_landmarks = pickle.load(open('../Datasets_Makeup/landmark_aligned_600.pk', 'rb'))\n    for i, before_name in enumerate(os.listdir(before_dir)):\n        lmks = api_landmarks['before_aligned_600/' + before_name].astype(int)\n        before_img = cv2.imread(before_dir + '/' + before_name)\n        prefix = before_name.split('.')[0]\n        for target_name in os.listdir(test_imgs_tmp):",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.Poisson_Image_Editing.poisson_image_editing_makeup",
        "documentation": {}
    },
    {
        "label": "dataset_makeup",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.datasets.dataset",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.datasets.dataset",
        "peekOfCode": "class dataset_makeup(data.Dataset):\n    def __init__(self, config):\n        self.resize_size = config.resize_size\n        self.data_dir = config.data_dir\n        self.lmk_name = config.lmk_name\n        self.after_dir = config.after_dir\n        self.before_dir = config.before_dir\n        self.eye_area = config.eye_area\n        # Load image landmarks for the face images\n        self.api_landmarks = pickle.load(open(os.path.join(self.data_dir, self.lmk_name), 'rb'))",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "MakeupAttack",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.makeup_attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.makeup_attack",
        "peekOfCode": "class MakeupAttack(nn.Module):\n    def __init__(self, config):\n        super(MakeupAttack, self).__init__()\n        self.config = config\n        self.lr = config.lr\n        self.lr_discr = self.lr / 2.5\n        self.update_lr_m = config.update_lr_m\n        self.device = torch.device('cuda:{}'.format(config.gpu)) if config.gpu >= 0 else torch.device('cpu')\n        self.data_dir = config.data_dir\n        self.api_landmarks = pickle.load(open(self.data_dir + '/' + config.lmk_name, 'rb'))",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.makeup_attack",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, input_dim, ngf=64):\n        super(Encoder, self).__init__()\n        self.config_enc = config_enc\n        # Init param list of kernels\n        self.vars_enc = nn.ParameterList()\n        # Init param list of batch-norm\n        self.vars_bn_enc = nn.ParameterList()\n        for i, (name_enc, param_enc) in enumerate(self.config_enc):\n            if name_enc == 'conv2d':",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, output_dim, ngf=64):\n        super(Decoder, self).__init__()\n        self.config_dec = config_dec\n        # Init param list of kernels\n        self.vars_dec = nn.ParameterList()\n        # Init param list of batch-norm\n        self.vars_bn_dec = nn.ParameterList()\n        for i, (name_dec, param_dec) in enumerate(self.config_dec):\n            if name_dec == 'conv2d':",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "documentation": {}
    },
    {
        "label": "Discriminator",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "peekOfCode": "class Discriminator(nn.Module):\n    def __init__(self, input_dim, ndf=64):\n        super(Discriminator, self).__init__()\n        model = []\n        model += [LeakyReLUConv2d(input_dim, ndf * 2, kernel_size=3, stride=2, padding=1, norm='Instance')]\n        model += [LeakyReLUConv2d(ndf * 2, ndf * 2, kernel_size=3, stride=2, padding=1, norm='Instance')]\n        model += [LeakyReLUConv2d(ndf * 2, ndf * 2, kernel_size=3, stride=2, padding=1, norm='Instance')]\n        model += [LeakyReLUConv2d(ndf * 2, ndf * 2, kernel_size=1, stride=1, padding=0)]\n        model += [nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0)]\n        self.model = nn.Sequential(*model)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "documentation": {}
    },
    {
        "label": "LeakyReLUConv2d",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "peekOfCode": "class LeakyReLUConv2d(nn.Module):\n    def __init__(self, inplanes, outplanes, kernel_size, stride, padding=0, norm='None', sn=False):\n        super(LeakyReLUConv2d, self).__init__()\n        model = []\n        model += [nn.ReflectionPad2d(padding)]\n        if sn:\n            model += [nn.utils.spectral_norm(nn.Conv2d(inplanes, outplanes,\n                                                       kernel_size=kernel_size, stride=stride, padding=0, bias=True))]\n        else:\n            model += [nn.Conv2d(inplanes, outplanes,",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "peekOfCode": "def init_weights(net, init_type, gain):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a=0, mode='fainplanes')",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "documentation": {}
    },
    {
        "label": "init_net",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "peekOfCode": "def init_net(net, device, init_type='normal', gain=0.02):\n    net.to(device)\n    init_weights(net, init_type, gain)\n    return net",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "documentation": {}
    },
    {
        "label": "config_enc",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "peekOfCode": "config_enc = [\n    ('conv2d', [64, 3, 5, 5, 2, 2]),\n    ('lrelu', [0.2, True]),\n    ('conv2d', [128, 64, 5, 5, 2, 2]),\n    ('bn', [128])\n]\n'''\nDecoder structure configuration\n'''\nconfig_dec = [",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "documentation": {}
    },
    {
        "label": "config_dec",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "peekOfCode": "config_dec = [\n    ('relu', [True]),\n    ('conv2d', [128, 128, 3, 3, 1, 1]),\n    ('bn', [128]),\n    ('cat1', 1),\n    ('relu', [True]),\n    ('up', [nn.functional.interpolate, 2, 'bilinear', True]),\n    ('conv2d', [64, 256, 3, 3, 1, 1]),\n    ('bn', [64]),\n    ('cat2', 1),",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.networks",
        "documentation": {}
    },
    {
        "label": "Vgg16",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "peekOfCode": "class Vgg16(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super(Vgg16, self).__init__()\n        model = models.vgg16(pretrained=False)\n        model.load_state_dict(torch.load('./models/VGG_Model/vgg16.pth'))\n        vgg_pretrained_features = model.features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "documentation": {}
    },
    {
        "label": "MeanShift",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "peekOfCode": "class MeanShift(nn.Conv2d):\n    def __init__(self, gpu_id):\n        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n        rgb_range = 1\n        rgb_mean = (0.4488, 0.4371, 0.4040)\n        rgb_std = (1.0, 1.0, 1.0)\n        sign = -1\n        std = torch.Tensor(rgb_std).to(gpu_id)\n        self.weight.data = torch.eye(3).view(3, 3, 1, 1).to(gpu_id) / std.view(3, 1, 1, 1)\n        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean).to(gpu_id) / std",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "documentation": {}
    },
    {
        "label": "laplacian_filter_tensor",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "peekOfCode": "def laplacian_filter_tensor(img_tensor, gpu_id):\n    \"\"\"\n    :param img_tensor: input image tensor (B, C, H, W)\n    :param gpu_id: obj to the inferring device, GPU or CPU\n    :return: three channels of the obtained gradient tensor\n    \"\"\"\n    laplacian_filter = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n    laplacian_conv = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n    laplacian_conv.weight = nn.Parameter(\n        torch.from_numpy(laplacian_filter).float().unsqueeze(0).unsqueeze(0).to(gpu_id))",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "documentation": {}
    },
    {
        "label": "numpy2tensor",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "peekOfCode": "def numpy2tensor(np_array, gpu_id):\n    if len(np_array.shape) == 2:\n        tensor = torch.from_numpy(np_array).unsqueeze(0).float().to(gpu_id)\n    else:\n        tensor = torch.from_numpy(np_array).unsqueeze(0).transpose(1, 3).transpose(2, 3).float().to(gpu_id)\n    return tensor\ndef compute_gt_gradient(x_start, y_start, source_img, target_img, mask, gpu_id):\n    # compute source image gradient\n    source_img_tensor = torch.from_numpy(source_img).unsqueeze(0).transpose(1, 3).transpose(2, 3).float().to(gpu_id)\n    red_source_gradient_tensor, green_source_gradient_tensor, blue_source_gradient_tenosr \\",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "documentation": {}
    },
    {
        "label": "compute_gt_gradient",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "peekOfCode": "def compute_gt_gradient(x_start, y_start, source_img, target_img, mask, gpu_id):\n    # compute source image gradient\n    source_img_tensor = torch.from_numpy(source_img).unsqueeze(0).transpose(1, 3).transpose(2, 3).float().to(gpu_id)\n    red_source_gradient_tensor, green_source_gradient_tensor, blue_source_gradient_tenosr \\\n        = laplacian_filter_tensor(source_img_tensor, gpu_id)\n    red_source_gradient = red_source_gradient_tensor.cpu().data.numpy()[0]\n    green_source_gradient = green_source_gradient_tensor.cpu().data.numpy()[0]\n    blue_source_gradient = blue_source_gradient_tenosr.cpu().data.numpy()[0]\n    # compute target image gradient\n    target_img_tensor = torch.from_numpy(target_img).unsqueeze(0).transpose(1, 3).transpose(2, 3).float().to(gpu_id)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "documentation": {}
    },
    {
        "label": "gram_matrix",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "peekOfCode": "def gram_matrix(y):\n    (b, ch, h, w) = y.size()\n    features = y.view(b, ch, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t) / (ch * h * w)\n    return gram\nclass MeanShift(nn.Conv2d):\n    def __init__(self, gpu_id):\n        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n        rgb_range = 1",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "documentation": {}
    },
    {
        "label": "read_img_from_path",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "peekOfCode": "def read_img_from_path(data_dir, img_path, mean, std, device):\n    \"\"\"\n    :param data_dir: parents folder to the input images data\n    :param img_path: path to the aligned image\n    :param mean: mean for image normalization\n    :param std: std for image normalization\n    :param device: obj to the inferring device, GPU or CPU\n    :return: preprocessed image (B, C, H, W) with the type of tensor\n    \"\"\"\n    img = cv2.imread(data_dir + '/' + img_path)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "peekOfCode": "def preprocess(im, mean, std, device):\n    if len(im.size()) == 3:\n        im = im.transpose(0, 2).transpose(1, 2).unsqueeze(0)\n    elif len(im.size()) == 4:\n        im = im.transpose(1, 3).transpose(2, 3)\n    mean = torch.tensor(mean).to(device)\n    mean = mean.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n    std = torch.tensor(std).to(device)\n    std = std.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n    im = (im - mean) / std",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.models.utils",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "peekOfCode": "def test(model, test_loader, target_name, simi_scores_dict):\n    for it, data in enumerate(test_loader):\n        # Un-makeup eye area images\n        before_img = data[0].to(device).detach()\n        # Un-makeup images' path\n        before_path = data[2]\n        # Save the generated examples with adversarial makeup\n        # and save the simi-scores to the \"simi_scores_dict\"\n        model.save_res_img(it, before_img, before_path, simi_scores_dict, target_name)\n        model.save_res_tmp_img(it, before_img, before_path, target_name)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "documentation": {}
    },
    {
        "label": "asr_calculation",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "peekOfCode": "def asr_calculation(simi_scores_dict):\n    # Iterate each image pair's simi-score from \"simi_scores_dict\" and compute the attacking success rate\n    for key, values in simi_scores_dict.items():\n        th01, th001, th0001 = th_dict[key]\n        total = len(values)\n        success01 = 0\n        success001 = 0\n        success0001 = 0\n        for v in values:\n            if v > th01:",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "peekOfCode": "def main():\n    simi_scores_dict = {}\n    # Calculate the overall simi-scores among all the image pairs\n    # and save the results to the \"simi_scores_dict\"\n    for target_name in os.listdir(args.data_dir + '/target_aligned_600')[0:]:\n        print(target_name)\n        # Initialize the data-loader\n        dataset = dataset_makeup(args)\n        test_loader = torch.utils.data.DataLoader(dataset, batch_size=1,\n                                                  shuffle=False, num_workers=args.n_threads)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "peekOfCode": "args = OmegaConf.load('../configs/config.yaml')\ndevice = torch.device('cuda:{}'.format(args.gpu)) if args.gpu >= 0 else torch.device('cpu')\nth_dict = {'ir152':(0.094632, 0.166788, 0.227922), 'irse50':(0.144840, 0.241045, 0.312703),\n           'facenet':(0.256587, 0.409131, 0.591191), 'mobile_face':(0.183635, 0.301611, 0.380878)}\ndef test(model, test_loader, target_name, simi_scores_dict):\n    for it, data in enumerate(test_loader):\n        # Un-makeup eye area images\n        before_img = data[0].to(device).detach()\n        # Un-makeup images' path\n        before_path = data[2]",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "peekOfCode": "device = torch.device('cuda:{}'.format(args.gpu)) if args.gpu >= 0 else torch.device('cpu')\nth_dict = {'ir152':(0.094632, 0.166788, 0.227922), 'irse50':(0.144840, 0.241045, 0.312703),\n           'facenet':(0.256587, 0.409131, 0.591191), 'mobile_face':(0.183635, 0.301611, 0.380878)}\ndef test(model, test_loader, target_name, simi_scores_dict):\n    for it, data in enumerate(test_loader):\n        # Un-makeup eye area images\n        before_img = data[0].to(device).detach()\n        # Un-makeup images' path\n        before_path = data[2]\n        # Save the generated examples with adversarial makeup",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "documentation": {}
    },
    {
        "label": "th_dict",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "peekOfCode": "th_dict = {'ir152':(0.094632, 0.166788, 0.227922), 'irse50':(0.144840, 0.241045, 0.312703),\n           'facenet':(0.256587, 0.409131, 0.591191), 'mobile_face':(0.183635, 0.301611, 0.380878)}\ndef test(model, test_loader, target_name, simi_scores_dict):\n    for it, data in enumerate(test_loader):\n        # Un-makeup eye area images\n        before_img = data[0].to(device).detach()\n        # Un-makeup images' path\n        before_path = data[2]\n        # Save the generated examples with adversarial makeup\n        # and save the simi-scores to the \"simi_scores_dict\"",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.test",
        "documentation": {}
    },
    {
        "label": "model_save",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "peekOfCode": "def model_save(model, ep, target_name):\n    if (ep + 1) % 50 == 0:\n        os.makedirs(os.path.join(args.model_dir, target_name.split('.')[0]), exist_ok=True)\n        print('--- save the model @ ep %d ---' % (ep))\n        # Save the params of encoder in the generator\n        torch.save(model.enc.state_dict(),\n                   '%s/%05d_enc.pth' % (os.path.join(args.model_dir, target_name.split('.')[0]), ep))\n        # Save the params of decoder in the generator\n        torch.save(model.dec.state_dict(),\n                   '%s/%05d_dec.pth' % (os.path.join(args.model_dir, target_name.split('.')[0]), ep))",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "documentation": {}
    },
    {
        "label": "train_single_epoch",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "peekOfCode": "def train_single_epoch(model, train_loader, target_name):\n    for it, data in enumerate(train_loader):\n        # Un-makeup eye area images (attacker's)\n        before_img = data[0].to(device).detach()\n        # Real-world makeup eye area images\n        after_img = data[1].to(device).detach()\n        # Un-makeup images' path\n        before_path = data[2]\n        # Update the eye makeup discriminator\n        model.update_discr(before_img, after_img)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "peekOfCode": "def train(model, train_loader, target_name):\n    for ep in range(args.epoch_steps):\n        # Re-init all the params for each training epoch\n        model.res_init(ep)\n        # Update a single epoch\n        train_single_epoch(model, train_loader, target_name)\n        # Save the visualized images during training and output the training logs\n        model.visualization(ep, len(train_loader))\n        # Save model\n        model_save(model, ep, target_name)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "peekOfCode": "def main():\n    for target_name in os.listdir(args.data_dir + '/target_aligned_600'):\n        print(\"Target: %s\" % (target_name))\n        # Initialize the Adv-Makeup, networks and optimizers\n        model = MakeupAttack(args)\n        # Initialize the data-loader\n        dataset = dataset_makeup(args)\n        train_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True,\n                                                   num_workers=args.n_threads)\n        train(model, train_loader, target_name)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "peekOfCode": "args = OmegaConf.load('./configs/config.yaml')\ndevice = torch.device('cuda:{}'.format(args.gpu)) if args.gpu >= 0 else torch.device('cpu')\ndef model_save(model, ep, target_name):\n    if (ep + 1) % 50 == 0:\n        os.makedirs(os.path.join(args.model_dir, target_name.split('.')[0]), exist_ok=True)\n        print('--- save the model @ ep %d ---' % (ep))\n        # Save the params of encoder in the generator\n        torch.save(model.enc.state_dict(),\n                   '%s/%05d_enc.pth' % (os.path.join(args.model_dir, target_name.split('.')[0]), ep))\n        # Save the params of decoder in the generator",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "peekOfCode": "device = torch.device('cuda:{}'.format(args.gpu)) if args.gpu >= 0 else torch.device('cpu')\ndef model_save(model, ep, target_name):\n    if (ep + 1) % 50 == 0:\n        os.makedirs(os.path.join(args.model_dir, target_name.split('.')[0]), exist_ok=True)\n        print('--- save the model @ ep %d ---' % (ep))\n        # Save the params of encoder in the generator\n        torch.save(model.enc.state_dict(),\n                   '%s/%05d_enc.pth' % (os.path.join(args.model_dir, target_name.split('.')[0]), ep))\n        # Save the params of decoder in the generator\n        torch.save(model.dec.state_dict(),",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Adv-Makeup.train",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "class Flatten(Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\ndef l2_norm(input, axis=1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n    return output\nclass SEModule(Module):\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "class SEModule(Module):\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = AdaptiveAvgPool2d(1)\n        self.fc1 = Conv2d(\n            channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n        nn.init.xavier_uniform_(self.fc1.weight.data)\n        self.relu = ReLU(inplace=True)\n        self.fc2 = Conv2d(\n            channels // reduction, channels, kernel_size=1, padding=0, bias=False)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "bottleneck_IR",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "class bottleneck_IR(Module):\n    def __init__(self, in_channel, depth, stride):\n        super(bottleneck_IR, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(\n                Conv2d(in_channel, depth, (1, 1), stride, bias=False), BatchNorm2d(depth))\n        self.res_layer = Sequential(\n            BatchNorm2d(in_channel),",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "bottleneck_IR_SE",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "class bottleneck_IR_SE(Module):\n    def __init__(self, in_channel, depth, stride):\n        super(bottleneck_IR_SE, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(\n                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n                BatchNorm2d(depth))\n        self.res_layer = Sequential(",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\ndef get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    if num_layers == 50:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=4),\n            get_block(in_channel=128, depth=256, num_units=14),",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "fc_block",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "class fc_block(nn.Module):\n    def __init__(self, inplanes, planes, drop_rate=0.15):\n        super(fc_block, self).__init__()\n        self.fc = nn.Linear(inplanes, planes)\n        self.bn = nn.BatchNorm1d(planes)\n        if drop_rate > 0:\n            self.dropout = nn.Dropout(drop_rate)\n        self.relu = nn.ReLU(inplace=True)\n        self.drop_rate = drop_rate\n    def forward(self, x):",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "class Backbone(Module):\n    def __init__(self, input_size, num_layers, mode='ir'):\n        super(Backbone, self).__init__()\n        assert input_size[0] in [112, 224], \"input_size should be [112, 112] or [224, 224]\"\n        assert num_layers in [50, 100, 152], \"num_layers should be 50, 100 or 152\"\n        assert mode in ['ir', 'ir_se'], \"mode should be ir or ir_se\"\n        blocks = get_blocks(num_layers)\n        if mode == 'ir':\n            unit_module = bottleneck_IR\n        elif mode == 'ir_se':",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "Attribute",
        "kind": 6,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "class Attribute(Module):\n    def __init__(self, num_attributes=40,setting=0):\n        super(Attribute, self).__init__()\n        self.ir_152 = IR_152((112,112))\n        if setting != 3:\n            self.ir_152.load_state_dict(torch.load(\"./models/ir152.pth\"))\n        if setting == 0:\n            # Keep input and body 25/50\n            no_grad_prefix = []\n            no_grad_prefix.append(\"ir_152.input\")",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "conv_3x3_bn",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "def conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "conv_1x1_bn",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "def conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\nclass Flatten(Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\ndef l2_norm(input, axis=1):",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "l2_norm",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "def l2_norm(input, axis=1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n    return output\nclass SEModule(Module):\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = AdaptiveAvgPool2d(1)\n        self.fc1 = Conv2d(\n            channels, channels // reduction, kernel_size=1, padding=0, bias=False)",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "get_block",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "def get_block(in_channel, depth, num_units, stride=2):\n    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\ndef get_blocks(num_layers):\n    if num_layers == 50:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=4),\n            get_block(in_channel=128, depth=256, num_units=14),\n            get_block(in_channel=256, depth=512, num_units=3)\n        ]",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "get_blocks",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "def get_blocks(num_layers):\n    if num_layers == 50:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=4),\n            get_block(in_channel=128, depth=256, num_units=14),\n            get_block(in_channel=256, depth=512, num_units=3)\n        ]\n    elif num_layers == 100:\n        blocks = [",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "IR_152_attr_all",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "def IR_152_attr_all(setting=0):\n    \"\"\"Constructs a ir-152 attribute model with all features on aligned face.\n    \"\"\"\n    model = Attribute(setting=setting)\n    return model\ndef IR_152(input_size):\n    \"\"\"Constructs a ir-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir')\n    return model",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "IR_152",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "def IR_152(input_size):\n    \"\"\"Constructs a ir-152 model.\n    \"\"\"\n    model = Backbone(input_size, 152, 'ir')\n    return model",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "peekOfCode": "__all__ = ['IR_152_attr_all']\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.models.models",
        "documentation": {}
    },
    {
        "label": "load_img",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "peekOfCode": "def load_img(img_path, config):\n    \"\"\" Read image from path and convert it to torch tensor\n    Args:\n        :param img_path:\n            the path to the input face image\n        :param config:\n            attacking configurations\n    :return:\n        the processed face image torch tensor object\n    \"\"\"",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "documentation": {}
    },
    {
        "label": "save_adv_img",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "peekOfCode": "def save_adv_img(attack_img, save_path, config):\n    \"\"\" Save adversarial image to the distination path\n    Args:\n        :param attack_img:\n            adversarial face image\n        :param save_path:\n            the path to save the adversarial face image\n        :param config:\n            attacking configurations\n    :return:",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "documentation": {}
    },
    {
        "label": "cos_simi",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "peekOfCode": "def cos_simi(emb_attack_img, emb_victim_img):\n    \"\"\" Calculate cosine similarity between two face features\n    Args:\n        :param emb_attack_img:\n            input feature representation for the attacker\n        :param emb_victim_img:\n            input feature representation for the victim\n    :return:\n        the cosine similarity of two features\n    \"\"\"",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "documentation": {}
    },
    {
        "label": "obtain_attacker_victim",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "peekOfCode": "def obtain_attacker_victim(config):\n    \"\"\" Obtain attackers and victims' image paths\n    Args:\n        :param config:\n            attacking configurations\n    :return:\n        the split path groups of attack and victim face images\n    \"\"\"\n    dataset_dir = config.dataset['dataset_dir']\n    dataset_txt = config.dataset['dataset_txt']",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.utils.utils",
        "documentation": {}
    },
    {
        "label": "load_surrogate_model",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "def load_surrogate_model():\n    \"\"\" Load white-box and black-box models\n    :return:\n        face recognition and attribute recognition models\n    \"\"\"\n    # Load pretrain white-box FR surrogate model\n    fr_model = m.IR_152((112, 112))\n    fr_model.load_state_dict(torch.load('./models/ir152.pth'))\n    fr_model.to(device)\n    fr_model.eval()",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "get_activation",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "def get_activation(name):\n    def hook(model, input, output):\n        activation[name] = output\n    return hook\ngouts = []\ndef backward_hook(module, gin, gout):\n    gouts.append(gout[0].data)\n    return gin\ndef infer_fr_model(attack_img, victim_img, fr_model):\n    \"\"\" Face recognition inference",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "backward_hook",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "def backward_hook(module, gin, gout):\n    gouts.append(gout[0].data)\n    return gin\ndef infer_fr_model(attack_img, victim_img, fr_model):\n    \"\"\" Face recognition inference\n    :param attack_img:\n            attacker face image\n    :param victim_img:\n            victim face image\n    :param fr_model:",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "infer_fr_model",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "def infer_fr_model(attack_img, victim_img, fr_model):\n    \"\"\" Face recognition inference\n    :param attack_img:\n            attacker face image\n    :param victim_img:\n            victim face image\n    :param fr_model:\n            face recognition model\n    :return:\n        feature representations for the attacker and victim face images",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "infer_ar_model",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "def infer_ar_model(attack_img, victim_img, ar_model):\n    \"\"\" Face attribute recognition inference\n    Args:\n        :param attack_img:\n            attacker face image\n        :param victim_img:\n            victim face image\n        :param ar_model:\n            attribute recognition model\n    :return:",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "sibling_attack",
        "kind": 2,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "def sibling_attack(attack_img, victim_img, fr_model, ar_model, config):\n    \"\"\" Perform Sibling-Attack\n    Args:\n        :param attack_img:\n            attacker face image\n        :param victim_img:\n            victim face image\n        :param fr_model:\n            face recognition model\n        :param ar_model:",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "layer_name",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "layer_name = \"ir_152.body.49\"\nactivation = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activation[name] = output\n    return hook\ngouts = []\ndef backward_hook(module, gin, gout):\n    gouts.append(gout[0].data)\n    return gin",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "activation",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "activation = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activation[name] = output\n    return hook\ngouts = []\ndef backward_hook(module, gin, gout):\n    gouts.append(gout[0].data)\n    return gin\ndef infer_fr_model(attack_img, victim_img, fr_model):",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "gouts",
        "kind": 5,
        "importPath": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "description": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "peekOfCode": "gouts = []\ndef backward_hook(module, gin, gout):\n    gouts.append(gout[0].data)\n    return gin\ndef infer_fr_model(attack_img, victim_img, fr_model):\n    \"\"\" Face recognition inference\n    :param attack_img:\n            attacker face image\n    :param victim_img:\n            victim face image",
        "detail": "TFace.security.tasks.Adv-Attack-Defense.Sibling-Attack.attack",
        "documentation": {}
    },
    {
        "label": "DG_Dataset",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.DG_dataset",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.DG_dataset",
        "peekOfCode": "class DG_Dataset(Dataset):\n    '''\n        DG_Dataset contain three datasets and preprocess images of three different domains\n        Args:\n            data_root (str): the path of LMDB_database\n            split (str): \n                'train': generate the datasets for training \n                'val': generate the datasets for validation\n                'test': generate the datasets for testing\n            catgory (str):",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.DG_dataset",
        "documentation": {}
    },
    {
        "label": "create_dg_data_transforms",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.factory",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.factory",
        "peekOfCode": "def create_dg_data_transforms(args, split='train'):\n    '''\n        define the domain generalization transforms accoding to different parameters\n        Args:\n            args ([type]): contain the specific parmaters\n            split (str, optinal):\n                'train': to generate the domain generalization transforms for training\n                'val': to generate the domain generalization transforms for validation\n                'test': to generaate the domain generalization transforms for testing\n    '''",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.factory",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.factory",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.factory",
        "peekOfCode": "def create_dataloader(args, split='train', category=None, print_info=False):\n    kwargs = getattr(args.dataset, args.dataset.name)\n    dg_transform = create_dg_data_transforms(args.transform, split=split)\n    dg_dataset = eval(args.dataset.name)(transform=dg_transform,\n                                      split=split,\n                                      category=category,\n                                      print_info=print_info,\n                                      **kwargs)\n    dg_dataloader = create_base_dataloader(args, dg_dataset, split=split)\n    return dg_dataloader",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.datasets.factory",
        "documentation": {}
    },
    {
        "label": "Conv_block",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "peekOfCode": "class Conv_block(nn.Module):\n    def __init__(self, in_ch, out_ch, AdapNorm=False, AdapNorm_attention_flag=False, model_initial='kaiming'):\n        '''\n            Args:\n                in_ch (int): the channel numbers of input features\n                out_ch (int): the channel numbers of output features\n                AdapNorm (bool): \n                    'True' allow the Conv_block to combine BN and IN\n                    'False' allow the Conv_block to use BN\n                AdapNorm_attention_flag (str):",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "documentation": {}
    },
    {
        "label": "Basic_block",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "peekOfCode": "class Basic_block(nn.Module):\n    def __init__(self, in_ch, out_ch, AdapNorm=True, AdapNorm_attention_flag='1layer', model_initial='kaiming'):\n        '''\n            Basic_block contains three Conv_block\n            Args:\n                in_ch (int): the channel numbers of input features\n                out_ch (int): the channel numbers of output features\n                AdapNorm (bool): \n                    'True' allow the Conv_block to combine BN and IN\n                    'False' allow the Conv_block to use BN",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "peekOfCode": "def conv3x3(in_ch, out_ch, stride=1, padding=1, bias=True):\n    '''\n        the cnn module with specific parameters\n        Args:\n            in_ch (int): the channel numbers of input features\n            out_ch (int): the channel numbers of output features\n            strider (int): the stride paramters of Conv2d\n            padding (int): the padding parameters of Conv2D\n            bias (bool): the bool parameters of Conv2d\n    '''",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.base_block",
        "documentation": {}
    },
    {
        "label": "FeatExtractor",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "peekOfCode": "class FeatExtractor(nn.Module):\n    '''\n        Args:\n            in_ch (int): the channel numbers of input features\n            AdapNorm (bool): \n                'True' allow the Conv_block to combine BN and IN\n                'False' allow the Conv_block to use BN\n            AdapNorm_attention_flag:\n                '1layer' allow the Conv_block to use 1layer FC to generate the balance factor\n                '2layer' allow the Conv_block to use 2layer FC to generate the balance factor",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "documentation": {}
    },
    {
        "label": "FeatEmbedder",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "peekOfCode": "class FeatEmbedder(nn.Module):\n    '''\n        Args:\n            in_ch (int): the channel numbers of input features\n            AdapNorm (bool): \n                'True' allow the Conv_block to combine BN and IN\n                'False' allow the Conv_block to use BN\n            AdapNorm_attention_flag:\n                '1layer' allow the Conv_block to use 1layer FC to generate the balance factor\n                '2layer' allow the Conv_block to use 2layer FC to generate the balance factor",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "documentation": {}
    },
    {
        "label": "DepthEstmator",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "peekOfCode": "class DepthEstmator(nn.Module):\n    '''\n        Args:\n            in_ch (int): the channel numbers of input features\n            AdapNorm (bool): \n                'True' allow the Conv_block to combine BN and IN\n                'False' allow the Conv_block to use BN\n            AdapNorm_attention_flag:\n                '1layer' allow the Conv_block to use 1layer FC to generate the balance factor\n                '2layer' allow the Conv_block to use 2layer FC to generate the balance factor",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "documentation": {}
    },
    {
        "label": "Framework",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "peekOfCode": "class Framework(nn.Module):\n    '''\n        Framework contains all the modules\n        Args:\n            in_ch (int): the channel numbers of input features\n            mid_ch (int): the channel numbers of output features in FeatExtractor\n            AdapNorm (bool): \n                'True' allow the Conv_block to combine BN and IN\n                'False' allow the Conv_block to use BN\n            AdapNorm_attention_flag:",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.models.framework",
        "documentation": {}
    },
    {
        "label": "ANRLTask",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.train",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.train",
        "peekOfCode": "class ANRLTask(BaseTask):\n    def __init__(self, args, task_dir):\n        super(ANRLTask, self).__init__(args, task_dir)\n    def _build_inputs(self, **kwargs):\n        \"\"\" build dataloaders\n        \"\"\"\n        if self.cfg.local_rank == 0:\n            self.logger.info('=> Building dataloader')\n        datasets = importlib.import_module('datasets', package=self.task_dir)\n        self.dg_train_pos = datasets.create_dataloader(self.cfg, split='train', category='pos')",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.train",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.train",
        "peekOfCode": "def main():\n    args = get_parameters()\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    ANRL_task = ANRLTask(args, task_dir)\n    ANRL_task.prepare()\n    ANRL_task.fit()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.ANRL.train",
        "documentation": {}
    },
    {
        "label": "FASDataset",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.dataset",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.dataset",
        "peekOfCode": "class FASDataset(Dataset):\n    '''\n        DG_Dataset contain one dataset and preprocess images \n        Args:\n            data_root (str): the path of LMDB_database\n            split (str): \n                'train': generate the datasets for training \n                'val': generate the datasets for validation\n                'test': generate the datasets for testing\n            catgory (str):",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "create_data_transforms",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.factory",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.factory",
        "peekOfCode": "def create_data_transforms(args, split='train'):\n    '''\n        define the transforms accoding to different parameters\n        Args:\n            args ([type]): contain the specific parmaters\n            split (str, optinal):\n                'train': to generate the transforms for training\n                'val': to generate the transforms for validation\n                'test': to generaate the transforms for testing\n    '''",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.factory",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.factory",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.factory",
        "peekOfCode": "def create_dataloader(args, split='train', category=None, print_info=False):\n    kwargs = getattr(args.dataset, args.dataset.name)\n    transform = create_data_transforms(args.transform, split=split)\n    dataset = eval(args.dataset.name)(transform=transform,\n                                      split=split,\n                                      category=category,\n                                      print_info=print_info,\n                                      **kwargs)\n    dataloader = create_base_dataloader(args, dataset, split=split)\n    return dataloader",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.datasets.factory",
        "documentation": {}
    },
    {
        "label": "Conv2dBlock",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.base_block",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.base_block",
        "peekOfCode": "class Conv2dBlock(nn.Module):\n    '''\n        Args:\n            First_block (bool): \n                'True': indicate the modules is the first block and the input channels is 64\n    '''\n    def __init__(self, First_block=False):\n        super(Conv2dBlock, self).__init__()\n        self.net = []\n        if First_block:",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.base_block",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "peekOfCode": "class Encoder(nn.Module):\n    '''\n        Encoder extract the features map of the inputs\n        Args:\n            in_ch (int): the channel numbers of input features\n    '''\n    def __init__(self, in_ch):\n        super(Encoder, self).__init__()\n        self.net = []\n        self.cnn = nn.Conv2d(in_ch,64,3,1,1,bias=True)",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "documentation": {}
    },
    {
        "label": "Auxilary_Deep",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "peekOfCode": "class Auxilary_Deep(nn.Module):\n    '''\n        Auxilary_Deep calcuate the depth maps of the inputs\n        Args:\n            in_ch (int): the channel numbers of input features\n    '''\n    def __init__(self, in_ch):\n        super(Auxilary_Deep,self).__init__()\n        self.in_ch = in_ch\n        self.model=[]",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "documentation": {}
    },
    {
        "label": "Similarity",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "peekOfCode": "class Similarity(nn.Module):\n    '''\n        Similarity calcuate the similarity between the inputs\n        Args:\n            in_ch (int): the channel numbers of input features\n    '''\n    def __init__(self,in_ch):\n        super(Similarity, self).__init__()\n        self.in_ch = in_ch\n        self.model=[]",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "documentation": {}
    },
    {
        "label": "DCN",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "peekOfCode": "class DCN(nn.Module):\n    '''\n        DCN contains all the modules\n        Args:\n            in_ch (int): the channel numbers of input features\n    '''\n    def __init__(self, in_ch):\n        super(DCN, self).__init__()\n        self.encoder = Encoder(in_ch)\n        self.depth = Auxilary_Deep(in_ch=384)",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.models.framework",
        "documentation": {}
    },
    {
        "label": "DCNTask",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.train",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.train",
        "peekOfCode": "class DCNTask(BaseTask):\n    def __init__(self, args, task_dir):\n        super(DCNTask, self).__init__(args, task_dir)\n    def _build_inputs(self, **kwargs):\n        \"\"\" build dataloaders\n        \"\"\"\n        if self.cfg.local_rank == 0:\n            self.logger.info('=> Building dataloader')\n        datasets = importlib.import_module('datasets', package=self.task_dir)\n        self.kwargs = getattr(self.cfg.dataset, self.cfg.dataset.name)",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Anti-Spoofing.DCN.train",
        "description": "TFace.security.tasks.Face-Anti-Spoofing.DCN.train",
        "peekOfCode": "def main():\n    args = get_parameters()\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    DCN_task = DCNTask(args, task_dir)\n    DCN_task.prepare()\n    DCN_task.fit()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.security.tasks.Face-Anti-Spoofing.DCN.train",
        "documentation": {}
    },
    {
        "label": "StrEnum",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "class StrEnum(str, Enum):\n    \"\"\"Utility functions\n    Args:\n        str ([type]) \n        Enum ([type])\n    \"\"\"\n    def __str__(self):\n        return self.name\n    def __repr__(self):\n        return str(self)",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "Compression",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "class Compression(StrEnum):\n    \"\"\"Compression types in FaceForesics++ dataset\n    Args:\n        StrEnum ([type]): [description]\n    \"\"\"\n    raw = auto()\n    c23 = auto()\n    c40 = auto()\n    random_compressed = auto()\n    masks = auto()",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "DataType",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "class DataType(StrEnum):\n    bounding_boxes = auto()\n    face_images = auto()\n    face_images_tracked = auto()\n    full_images = auto()\n    face_information = auto()\n    videos = auto()\n    resampled_videos = auto()\n    images_v1 = auto()\n    images_v3 = auto()",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "Method",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "class Method:\n    REAL_DIR = \"original_sequences/\"\n    FAKE_DIR = \"manipulated_sequences/\"\n    def __init__(self, name: str, is_real: bool):\n        \"\"\"Method class for FaceForensics++ dataset\n        Args:\n            name (str): \n            is_real (bool): \n        \"\"\"\n        self.name = name",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "FaceForensicsDataStructure",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "class FaceForensicsDataStructure:\n    METHODS = {\n        YOUTUBE.name: YOUTUBE,\n        DEEPFAKES.name: DEEPFAKES,\n        FACE2FACE.name: FACE2FACE,\n        FACE_SWAP.name: FACE_SWAP,\n        NEURAL_TEXTURES.name: NEURAL_TEXTURES,\n        ACTORS.name: ACTORS,\n        DEEP_FAKE_DETECTION.name: DEEP_FAKE_DETECTION,\n        FACESHIFTER.name: FACESHIFTER,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "ACTORS",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "ACTORS = Method(\"actors\", is_real=True)\nYOUTUBE = Method(\"youtube\", is_real=True)\nDEEP_FAKE_DETECTION = Method(\"DeepFakeDetection\", is_real=False)\nDEEPFAKES = Method(\"Deepfakes\", is_real=False)\nFACE2FACE = Method(\"Face2Face\", is_real=False)\nFACE_SWAP = Method(\"FaceSwap\", is_real=False)\nNEURAL_TEXTURES = Method(\"NeuralTextures\", is_real=False)\nFACESHIFTER = Method(\"FaceShifter\", is_real=False)\nclass FaceForensicsDataStructure:\n    METHODS = {",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "YOUTUBE",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "YOUTUBE = Method(\"youtube\", is_real=True)\nDEEP_FAKE_DETECTION = Method(\"DeepFakeDetection\", is_real=False)\nDEEPFAKES = Method(\"Deepfakes\", is_real=False)\nFACE2FACE = Method(\"Face2Face\", is_real=False)\nFACE_SWAP = Method(\"FaceSwap\", is_real=False)\nNEURAL_TEXTURES = Method(\"NeuralTextures\", is_real=False)\nFACESHIFTER = Method(\"FaceShifter\", is_real=False)\nclass FaceForensicsDataStructure:\n    METHODS = {\n        YOUTUBE.name: YOUTUBE,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "DEEP_FAKE_DETECTION",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "DEEP_FAKE_DETECTION = Method(\"DeepFakeDetection\", is_real=False)\nDEEPFAKES = Method(\"Deepfakes\", is_real=False)\nFACE2FACE = Method(\"Face2Face\", is_real=False)\nFACE_SWAP = Method(\"FaceSwap\", is_real=False)\nNEURAL_TEXTURES = Method(\"NeuralTextures\", is_real=False)\nFACESHIFTER = Method(\"FaceShifter\", is_real=False)\nclass FaceForensicsDataStructure:\n    METHODS = {\n        YOUTUBE.name: YOUTUBE,\n        DEEPFAKES.name: DEEPFAKES,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "DEEPFAKES",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "DEEPFAKES = Method(\"Deepfakes\", is_real=False)\nFACE2FACE = Method(\"Face2Face\", is_real=False)\nFACE_SWAP = Method(\"FaceSwap\", is_real=False)\nNEURAL_TEXTURES = Method(\"NeuralTextures\", is_real=False)\nFACESHIFTER = Method(\"FaceShifter\", is_real=False)\nclass FaceForensicsDataStructure:\n    METHODS = {\n        YOUTUBE.name: YOUTUBE,\n        DEEPFAKES.name: DEEPFAKES,\n        FACE2FACE.name: FACE2FACE,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "FACE2FACE",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "FACE2FACE = Method(\"Face2Face\", is_real=False)\nFACE_SWAP = Method(\"FaceSwap\", is_real=False)\nNEURAL_TEXTURES = Method(\"NeuralTextures\", is_real=False)\nFACESHIFTER = Method(\"FaceShifter\", is_real=False)\nclass FaceForensicsDataStructure:\n    METHODS = {\n        YOUTUBE.name: YOUTUBE,\n        DEEPFAKES.name: DEEPFAKES,\n        FACE2FACE.name: FACE2FACE,\n        FACE_SWAP.name: FACE_SWAP,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "FACE_SWAP",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "FACE_SWAP = Method(\"FaceSwap\", is_real=False)\nNEURAL_TEXTURES = Method(\"NeuralTextures\", is_real=False)\nFACESHIFTER = Method(\"FaceShifter\", is_real=False)\nclass FaceForensicsDataStructure:\n    METHODS = {\n        YOUTUBE.name: YOUTUBE,\n        DEEPFAKES.name: DEEPFAKES,\n        FACE2FACE.name: FACE2FACE,\n        FACE_SWAP.name: FACE_SWAP,\n        NEURAL_TEXTURES.name: NEURAL_TEXTURES,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "NEURAL_TEXTURES",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "NEURAL_TEXTURES = Method(\"NeuralTextures\", is_real=False)\nFACESHIFTER = Method(\"FaceShifter\", is_real=False)\nclass FaceForensicsDataStructure:\n    METHODS = {\n        YOUTUBE.name: YOUTUBE,\n        DEEPFAKES.name: DEEPFAKES,\n        FACE2FACE.name: FACE2FACE,\n        FACE_SWAP.name: FACE_SWAP,\n        NEURAL_TEXTURES.name: NEURAL_TEXTURES,\n        ACTORS.name: ACTORS,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "FACESHIFTER",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "peekOfCode": "FACESHIFTER = Method(\"FaceShifter\", is_real=False)\nclass FaceForensicsDataStructure:\n    METHODS = {\n        YOUTUBE.name: YOUTUBE,\n        DEEPFAKES.name: DEEPFAKES,\n        FACE2FACE.name: FACE2FACE,\n        FACE_SWAP.name: FACE_SWAP,\n        NEURAL_TEXTURES.name: NEURAL_TEXTURES,\n        ACTORS.name: ACTORS,\n        DEEP_FAKE_DETECTION.name: DEEP_FAKE_DETECTION,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.data_structure",
        "documentation": {}
    },
    {
        "label": "RandomPatch",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.random_patch",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.random_patch",
        "peekOfCode": "class RandomPatch:\n    def __init__(self, grid_size=3):\n        \"\"\"Random shuffle patches of images\n        Args:\n            grid_size (int, optional):  Defaults to 3.\n        \"\"\"\n        self.grid_size = grid_size\n        self.n_grids = grid_size ** 2\n        self.permutations = PERMUTATIONS_40\n    def __call__(self, image, mask=None):",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.random_patch",
        "documentation": {}
    },
    {
        "label": "PERMUTATIONS_40",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.random_patch",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.random_patch",
        "peekOfCode": "PERMUTATIONS_40 = [\n    [7, 2, 0, 1, 5, 6, 3, 4, 8],\n    [0, 1, 2, 3, 4, 5, 6, 8, 7],\n    [1, 0, 3, 2, 6, 4, 8, 7, 5],\n    [2, 3, 1, 0, 7, 8, 4, 5, 6],\n    [3, 4, 5, 6, 8, 0, 7, 1, 2],\n    [4, 5, 6, 8, 0, 7, 1, 2, 3],\n    [5, 6, 8, 7, 1, 2, 0, 3, 4],\n    [6, 8, 7, 4, 2, 3, 5, 0, 1],\n    [8, 7, 4, 5, 3, 1, 2, 6, 0],",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.random_patch",
        "documentation": {}
    },
    {
        "label": "setup_srm_weights",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.srm",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.srm",
        "peekOfCode": "def setup_srm_weights(input_channels: int = 3, output_channel=1) -> torch.Tensor:\n    \"\"\"Creates the SRM kernels for noise analysis.\n    note: values taken from Zhou et al., \"Learning Rich Features for Image Manipulation Detection\", CVPR2018\n    Args:\n        input_channels (int, optional):  Defaults to 3.\n        output_channel (int, optional): Defaults to 1.\n    Returns:\n        torch.Tensor\n    \"\"\"\n    srm_kernel = torch.from_numpy(",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.srm",
        "documentation": {}
    },
    {
        "label": "setup_srm_layer",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.srm",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.srm",
        "peekOfCode": "def setup_srm_layer(input_channels: int = 3, output_channel=None) -> torch.nn.Module:\n    \"\"\"Creates a SRM convolution layer for noise analysis.\n    Args:\n        input_channels (int, optional): [description]. Defaults to 3.\n        output_channel ([type], optional): [description]. Defaults to None.\n    Returns:\n        torch.nn.Module: [description]\n    \"\"\"\n    if output_channel == None:\n        weights = setup_srm_weights(input_channels)",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.utils.srm",
        "documentation": {}
    },
    {
        "label": "CelebDF",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.celeb_df",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.celeb_df",
        "peekOfCode": "class CelebDF(Dataset):\n    def __init__(self,\n                 data_root,\n                 data_types,\n                 num_frames,\n                 split,\n                 transform=None,\n                 pair=False,\n                 random_patch=None,\n                 sample=0):",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.celeb_df",
        "documentation": {}
    },
    {
        "label": "create_data_transforms",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.factory",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.factory",
        "peekOfCode": "def create_data_transforms(args, split='train'):\n    \"\"\"Create data transofrms\n    Args:\n        args: data transforms configs\n        split (str, optional): split for train, val or test. Defaults to 'train'.\n    Returns:\n        albumentation: pytorch data augmentations\n    \"\"\"\n    base_transform = create_base_transforms(args, split=split)\n    if split == 'train':",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.factory",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.factory",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.factory",
        "peekOfCode": "def create_dataloader(args, split='train'):\n    \"\"\"create data loader\n    Args:\n        args: data loader configs\n        split (str, optional): split for train, val or test. Defaults to 'train'.\n    Returns:\n        [type]: [description]\n    \"\"\"\n    dataset_params = getattr(args.dataset, args.dataset.name)\n    transform = create_data_transforms(args.transform, split=split)",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.factory",
        "documentation": {}
    },
    {
        "label": "FaceForensics",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "peekOfCode": "class FaceForensics(Dataset):\n    def __init__(self,\n                 data_root,\n                 data_types,\n                 num_frames,\n                 split,\n                 transform=None,\n                 compressions='c23',\n                 mask_size=10,\n                 methods=None,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "documentation": {}
    },
    {
        "label": "listdir_with_full_paths",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "peekOfCode": "def listdir_with_full_paths(dir_path):\n    return [os.path.join(dir_path, x) for x in os.listdir(dir_path)]\ndef get_file_name(file_path):\n    return file_path.split('/')[-1]\ndef read_json(file_path):\n    with open(file_path) as inp:\n        return json.load(inp)\ndef get_sets(data):\n    return {x[0] for x in data} | {x[1] for x in data} | {'_'.join(x) for x in data} | {'_'.join(x[::-1]) for x in data}\ndef get_video_ids(spl, splits_path):",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "documentation": {}
    },
    {
        "label": "get_file_name",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "peekOfCode": "def get_file_name(file_path):\n    return file_path.split('/')[-1]\ndef read_json(file_path):\n    with open(file_path) as inp:\n        return json.load(inp)\ndef get_sets(data):\n    return {x[0] for x in data} | {x[1] for x in data} | {'_'.join(x) for x in data} | {'_'.join(x[::-1]) for x in data}\ndef get_video_ids(spl, splits_path):\n    return get_sets(read_json(os.path.join(splits_path, f'{spl}.json')))\nif __name__ == '__main__':",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "documentation": {}
    },
    {
        "label": "read_json",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "peekOfCode": "def read_json(file_path):\n    with open(file_path) as inp:\n        return json.load(inp)\ndef get_sets(data):\n    return {x[0] for x in data} | {x[1] for x in data} | {'_'.join(x) for x in data} | {'_'.join(x[::-1]) for x in data}\ndef get_video_ids(spl, splits_path):\n    return get_sets(read_json(os.path.join(splits_path, f'{spl}.json')))\nif __name__ == '__main__':\n    from transforms import create_data_transforms\n    from omegaconf import OmegaConf",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "documentation": {}
    },
    {
        "label": "get_sets",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "peekOfCode": "def get_sets(data):\n    return {x[0] for x in data} | {x[1] for x in data} | {'_'.join(x) for x in data} | {'_'.join(x[::-1]) for x in data}\ndef get_video_ids(spl, splits_path):\n    return get_sets(read_json(os.path.join(splits_path, f'{spl}.json')))\nif __name__ == '__main__':\n    from transforms import create_data_transforms\n    from omegaconf import OmegaConf\n    args = OmegaConf.load('../configs/dcl.yaml')\n    args.dataset.name = 'ffpp'\n    kwargs = getattr(args.dataset, args.dataset.name)",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "documentation": {}
    },
    {
        "label": "get_video_ids",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "peekOfCode": "def get_video_ids(spl, splits_path):\n    return get_sets(read_json(os.path.join(splits_path, f'{spl}.json')))\nif __name__ == '__main__':\n    from transforms import create_data_transforms\n    from omegaconf import OmegaConf\n    args = OmegaConf.load('../configs/dcl.yaml')\n    args.dataset.name = 'ffpp'\n    kwargs = getattr(args.dataset, args.dataset.name)\n    split = 'train'\n    transform = create_data_transforms(args.transform, split)",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.ffpp",
        "documentation": {}
    },
    {
        "label": "WildDeepfake",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "peekOfCode": "class WildDeepfake(Dataset):\n    def __init__(self, root, split, transform=None):\n        \"\"\"Wild Deepfake dataset.\n        Reference: WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection, MM2020\n        Args:\n            root ([str]): [data root of wilddeepfake]\n            split ([str]): [\"train\",\"val\",\"test\"]\n            transform ([type], optional): [Data transform]. Defaults to None.\n        \"\"\"\n        if split not in SPLITS:",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "documentation": {}
    },
    {
        "label": "SPLITS",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "peekOfCode": "SPLITS = [\"train\", \"test\"]\nAVAILABLE_IMAGE_MODE = [\"L\", \"RGB\"]\nclass WildDeepfake(Dataset):\n    def __init__(self, root, split, transform=None):\n        \"\"\"Wild Deepfake dataset.\n        Reference: WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection, MM2020\n        Args:\n            root ([str]): [data root of wilddeepfake]\n            split ([str]): [\"train\",\"val\",\"test\"]\n            transform ([type], optional): [Data transform]. Defaults to None.",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "documentation": {}
    },
    {
        "label": "AVAILABLE_IMAGE_MODE",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "peekOfCode": "AVAILABLE_IMAGE_MODE = [\"L\", \"RGB\"]\nclass WildDeepfake(Dataset):\n    def __init__(self, root, split, transform=None):\n        \"\"\"Wild Deepfake dataset.\n        Reference: WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection, MM2020\n        Args:\n            root ([str]): [data root of wilddeepfake]\n            split ([str]): [\"train\",\"val\",\"test\"]\n            transform ([type], optional): [Data transform]. Defaults to None.\n        \"\"\"",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.datasets.wild_deepfake",
        "documentation": {}
    },
    {
        "label": "BinaryClassifier",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "peekOfCode": "class BinaryClassifier(nn.Module):\n    def __init__(self, encoder, num_classes=2, drop_rate=0.2, has_feature=False, pretrained=False, **kwargs) -> None:\n        \"\"\"Base binary classifier\n        Args:\n            encoder ([nn.Module]): Backbone of the DCL\n            num_classes (int, optional): Defaults to 2.\n            drop_rate (float, optional):  Defaults to 0.2.\n            has_feature (bool, optional): Wthether to return feature maps. Defaults to False.\n            pretrained (bool, optional): Whether to use a pretrained model. Defaults to False.\n        \"\"\"",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "peekOfCode": "__all__ = ['BinaryClassifier']\nMODEL_DICTS = {}\nMODEL_DICTS.update(timm.models.__dict__)\nclass BinaryClassifier(nn.Module):\n    def __init__(self, encoder, num_classes=2, drop_rate=0.2, has_feature=False, pretrained=False, **kwargs) -> None:\n        \"\"\"Base binary classifier\n        Args:\n            encoder ([nn.Module]): Backbone of the DCL\n            num_classes (int, optional): Defaults to 2.\n            drop_rate (float, optional):  Defaults to 0.2.",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "documentation": {}
    },
    {
        "label": "MODEL_DICTS",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "peekOfCode": "MODEL_DICTS = {}\nMODEL_DICTS.update(timm.models.__dict__)\nclass BinaryClassifier(nn.Module):\n    def __init__(self, encoder, num_classes=2, drop_rate=0.2, has_feature=False, pretrained=False, **kwargs) -> None:\n        \"\"\"Base binary classifier\n        Args:\n            encoder ([nn.Module]): Backbone of the DCL\n            num_classes (int, optional): Defaults to 2.\n            drop_rate (float, optional):  Defaults to 0.2.\n            has_feature (bool, optional): Wthether to return feature maps. Defaults to False.",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.base_net",
        "documentation": {}
    },
    {
        "label": "DCL",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.dcl",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.dcl",
        "peekOfCode": "class DCL(nn.Module):\n    def __init__(self, base_encoder, dim=128, K=65536, m=0.999, T=0.07, threshold=5):\n        \"\"\"Dual Contrastive learning for Face Forgery Detection\n        Args:\n            base_encoder (nn.Module)\n            dim (int, optional): Feature dimension. Defaults to 128.\n            K (int, optional): Queue size. Defaults to 65536.\n            m (float, optional): Parameters for moving average. Defaults to 0.999.\n            T (float, optional): Temperature. Defaults to 0.07.\n            threshold (int, optional): Parameters for hard sample selection. Defaults to 5.",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.dcl",
        "documentation": {}
    },
    {
        "label": "concat_all_gather",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.dcl",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.dcl",
        "peekOfCode": "def concat_all_gather(tensor):\n    tensors_gather = [torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n    output = torch.cat(tensors_gather, dim=0)\n    return output",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.models.dcl",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "peekOfCode": "def load_model(args):\n    \"\"\"Load models with args\n    Returns:\n        [nn.Module]: [model]\n    \"\"\"\n    if args.ckpt_path is not None:\n        print(f'resume model from {args.ckpt_path}')\n        checkpoint = torch.load(args.ckpt_path)\n        if getattr(args, 'transform', None) is None:\n            args.transform = checkpoint['args'].transform",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "documentation": {}
    },
    {
        "label": "model_forward",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "peekOfCode": "def model_forward(args, inputs, model):\n    \"\"\"Model forward\n    Args:\n        inputs ([Tensor]): [Input image]\n        model ([nn.Module])\n    Returns:\n        [Tensor]\n    \"\"\"\n    output = model(inputs)\n    if type(output) is tuple or type(output) is list:",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-c', '--config', type=str, default='configs/test.yaml')\n    parser.add_argument('--distributed', type=int, default=0)\n    parser.add_argument('--exam_id', type=str, default='')\n    parser.add_argument('--ckpt_path', type=str, default='')\n    parser.add_argument('--dataset', type=str, default='')\n    parser.add_argument('--compress', type=str, default='')\n    parser.add_argument('--constract', type=bool, default=False)\n    parser.add_argument('--debug', action='store_true', default=False)",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.test",
        "documentation": {}
    },
    {
        "label": "TrainTask",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.train",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.train",
        "peekOfCode": "class TrainTask(BaseTask):\n    \"\"\" TrainTask in dcl mode, which means classifier shards into multi workers\n    \"\"\"\n    def __init__(self, cfg_file, task_dir):\n        super(TrainTask, self).__init__(cfg_file, task_dir)\n    def _build_model(self, **kwargs):\n        \"\"\" build training backbone and heads\n        \"\"\"\n        if self.cfg.local_rank == 0:\n            self.logger.info('=> Building model')",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.DCL.train",
        "description": "TFace.security.tasks.Face-Forgery-Detection.DCL.train",
        "peekOfCode": "def main():\n    args = get_parameters()\n    task_dir = os.path.dirname(os.path.abspath(__file__))\n    task = TrainTask(args, task_dir)\n    task.prepare()\n    task.fit()\nif __name__ == '__main__':\n    main()",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.DCL.train",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.datasets.factory",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.datasets.factory",
        "peekOfCode": "def get_dataloader(args, split):\n    \"\"\"Set dataloader.\n    Args:\n        args (object): Args load from get_params function.\n        split (str): One of ['train', 'test']\n    \"\"\"\n    transform = create_base_transforms(args.transform_params, split=split)\n    dataset_cfg = getattr(args, split).dataset\n    dataset_params = OmegaConf.to_container(dataset_cfg.params, resolve=True)\n    dataset_params['transform'] = transform",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.datasets.factory",
        "documentation": {}
    },
    {
        "label": "FFPP_Dataset",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.datasets.video_dataset",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.datasets.video_dataset",
        "peekOfCode": "class FFPP_Dataset(data.Dataset):\n    def __init__(self,\n                 root,\n                 face_info_path,\n                 method='Deepfakes',\n                 compression='c23',\n                 split='train',\n                 num_segments=16,\n                 transform=None,\n                 sparse_span=150,",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.datasets.video_dataset",
        "documentation": {}
    },
    {
        "label": "ISM_Module",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "peekOfCode": "class ISM_Module(nn.Module):\n    def __init__(self, k_size=3):\n        \"\"\"The Information Supplement Module (ISM).\n        Args:\n            k_size (int, optional): Conv1d kernel_size . Defaults to 3.\n        \"\"\"\n        super(ISM_Module, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size-1)//2, bias=False)\n        self.sigmoid = nn.Sigmoid()",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "documentation": {}
    },
    {
        "label": "TIM_Module",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "peekOfCode": "class TIM_Module(nn.Module):\n    def __init__(self, in_channels, reduction=16, n_segment=8, return_attn=False):\n        \"\"\"The Temporal Inconsistency Module (TIM).\n        Args:\n            in_channels (int): Input channel number.\n            reduction (int, optional): Channel compression ratio r in the split operation.. Defaults to 16.\n            n_segment (int, optional): Number of input frames.. Defaults to 8.\n            return_attn (bool, optional): Whether to return the attention part. Defaults to False.\n        \"\"\"\n        super(TIM_Module, self).__init__()",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "documentation": {}
    },
    {
        "label": "ShiftModule",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "peekOfCode": "class ShiftModule(nn.Module):\n    def __init__(self, input_channels, n_segment=8, n_div=8, mode='shift'):\n        \"\"\"A depth-wise conv on the segment level.\n        Args:\n            input_channels (int): Input channel number.\n            n_segment (int, optional): Number of input frames.. Defaults to 8.\n            n_div (int, optional): How many channels to group as a fold.. Defaults to 8.\n            mode (str, optional): One of \"shift\", \"fixed\", \"norm\". Defaults to 'shift'.\n        \"\"\"\n        super(ShiftModule, self).__init__()",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "documentation": {}
    },
    {
        "label": "SCConv",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "peekOfCode": "class SCConv(nn.Module):\n    \"\"\"\n    The spatial conv in SIM. Used in SCBottleneck\n    \"\"\"\n    def __init__(self, inplanes, planes, stride, padding, dilation, groups, pooling_r, norm_layer):\n        super(SCConv, self).__init__()\n        self.f_w = nn.Sequential(\n                    nn.AvgPool2d(kernel_size=pooling_r, stride=pooling_r), \n                    nn.Conv2d(inplanes, planes, kernel_size=(1,3), stride=1,\n                                padding=(0,padding), dilation=(1,dilation),",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "documentation": {}
    },
    {
        "label": "SCBottleneck",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "peekOfCode": "class SCBottleneck(nn.Module):\n    \"\"\"\n    SCNet SCBottleneck. Variant for ResNet Bottlenect.\n    \"\"\"\n    expansion = 4\n    pooling_r = 4 # down-sampling rate of the avg pooling layer in the K3 path of SC-Conv.\n    def __init__(self, num_segments, inplanes, planes, stride=1, downsample=None,\n                 cardinality=1, bottleneck_width=32,\n                 avd=False, dilation=1, is_first=False,\n                 norm_layer=None):",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "documentation": {}
    },
    {
        "label": "SCNet",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "peekOfCode": "class SCNet(nn.Module):\n    def __init__(self, num_segments, block, layers, groups=1, bottleneck_width=32,\n                 num_classes=1000, dilated=False, dilation=1,\n                 deep_stem=False, stem_width=64, avg_down=False,\n                 avd=False, norm_layer=nn.BatchNorm2d):\n        \"\"\"SCNet, a variant based on ResNet.\n        Args:\n            num_segments (int): \n                Number of input frames.\n            block (class): ",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "documentation": {}
    },
    {
        "label": "scnet50_v1d",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "peekOfCode": "def scnet50_v1d(num_segments, pretrained=False, **kwargs):\n    \"\"\"\n    SCNet backbone, which is based on ResNet-50\n    Args:\n        num_segments (int):\n            Number of input frames.\n        pretrained (bool, optional):\n            Whether to load pretrained weights.\n    \"\"\"\n    model = SCNet(num_segments, SCBottleneck, [3, 4, 6, 3],",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "documentation": {}
    },
    {
        "label": "model_urls",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "peekOfCode": "model_urls = {\n    'scnet50_v1d': 'https://backseason.oss-cn-beijing.aliyuncs.com/scnet/scnet50_v1d-4109d1e1.pth',\n}\nclass ISM_Module(nn.Module):\n    def __init__(self, k_size=3):\n        \"\"\"The Information Supplement Module (ISM).\n        Args:\n            k_size (int, optional): Conv1d kernel_size . Defaults to 3.\n        \"\"\"\n        super(ISM_Module, self).__init__()",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.ops",
        "documentation": {}
    },
    {
        "label": "STIL_Model",
        "kind": 6,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.stil",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.stil",
        "peekOfCode": "class STIL_Model(nn.Module):\n    def __init__(self, \n        num_class=2,\n        num_segment=8,\n        add_softmax=False,\n        **kwargs):\n        \"\"\" Model Builder for STIL model.\n        STIL: Spatiotemporal Inconsistency Learning for DeepFake Video Detection (https://arxiv.org/abs/2109.01860)\n        Args:\n            num_class (int, optional): Number of classes. Defaults to 2.",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.models.stil",
        "documentation": {}
    },
    {
        "label": "to_list",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.lr_utils",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.lr_utils",
        "peekOfCode": "def to_list(inp):\n    if inp is None:\n        return None\n    if isinstance(inp, omegaconf.listconfig.ListConfig):\n        inp = omegaconf.OmegaConf.to_container(inp, resolve=True)\n    return inp\ndef lr_tuner(lr_init, optimizer, epoch_size, tune_dict, global_step=1, use_warmup=False, warmup_epochs=1, lr_min=1e-6):\n    \"\"\"A simple learning rate tuning strategy.\n    Using tune_dict to tune the learning rate.\n    e.g.:",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.lr_utils",
        "documentation": {}
    },
    {
        "label": "lr_tuner",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.lr_utils",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.lr_utils",
        "peekOfCode": "def lr_tuner(lr_init, optimizer, epoch_size, tune_dict, global_step=1, use_warmup=False, warmup_epochs=1, lr_min=1e-6):\n    \"\"\"A simple learning rate tuning strategy.\n    Using tune_dict to tune the learning rate.\n    e.g.:\n        tune_dict: \n            key: strategy name, value: strategy params\n            (1) piecewise:\n            {\n                \"decay_steps\": [100, 200],\n                \"decay_epochs\": [1, 2],",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.lr_utils",
        "documentation": {}
    },
    {
        "label": "compute_metrics",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.metrics",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.metrics",
        "peekOfCode": "def compute_metrics(model_outputs, labels):\n    \"\"\"\n    Compute the accuracy metrics.\n    \"\"\"\n    real_probs = F.softmax(model_outputs, dim=1)[:, 0]\n    bin_preds = (real_probs <= 0.5).int()\n    bin_labels = (labels != 0).int()\n    real_cnt = (bin_labels == 0).sum()\n    fake_cnt = (bin_labels == 1).sum()\n    acc = (bin_preds == bin_labels).float().mean()",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.utils.metrics",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "peekOfCode": "def main():\n    # use distributed test with nccl backend \n    args.local_rank = int(os.environ.get('LOCAL_RANK', 0))\n    dist.init_process_group(backend='nccl', init_method=\"env://\")\n    torch.cuda.set_device(args.local_rank)\n    args.world_size = dist.get_world_size()\n    # set model and wrap it with DistributedDataParallel\n    model = eval(args.model.name)(**args.model.params)\n    model.cuda(args.local_rank)\n    model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "peekOfCode": "def test(dataloader, model, args):\n    # modify the STIL num segment (train and test may have different segments)\n    model.module.set_segment(args.test.dataset.params.num_segments)\n    model.eval()\n    y_outputs, y_labels = [], []\n    with torch.no_grad():\n        for _, datas in enumerate(tqdm(dataloader)):\n            images, labels, video_paths, segment_indices = datas\n            images = images.cuda(args.local_rank)\n            labels = labels.cuda(args.local_rank)",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "peekOfCode": "args = get_params()\nsetup(args)\n###########################\n# main logic for test #\n###########################\ndef main():\n    # use distributed test with nccl backend \n    args.local_rank = int(os.environ.get('LOCAL_RANK', 0))\n    dist.init_process_group(backend='nccl', init_method=\"env://\")\n    torch.cuda.set_device(args.local_rank)",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "peekOfCode": "def main():\n    # use distributed training with nccl backend \n    args.local_rank = int(os.environ.get('LOCAL_RANK', 0))\n    dist.init_process_group(backend='nccl', init_method=\"env://\")\n    torch.cuda.set_device(args.local_rank)\n    args.world_size = dist.get_world_size()\n    # set logger\n    logger = get_logger(str(args.local_rank), console=args.local_rank==0, \n        log_path=os.path.join(args.exam_dir, f'train_{args.local_rank}.log'))\n    # get dataloaders for train and test",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "peekOfCode": "def train(dataloader, model, criterion, optimizer, epoch, global_step, args, logger):\n    epoch_size = len(dataloader)\n    # modify the STIL num segment (train and test may have different segments)\n    model.module.set_segment(args.train.dataset.params.num_segments)\n    # set statistical meters and progress meter\n    acces = AverageMeter('Acc', ':.4f')\n    real_acces = AverageMeter('RealAcc', ':.4f')\n    fake_acces = AverageMeter('FakeACC', ':.4f')\n    losses = AverageMeter('Loss', ':.4f')\n    data_time = AverageMeter('Data', ':.4f')",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "peekOfCode": "def test(dataloader, model, criterion, optimizer, epoch, global_step, args, logger):\n    # modify the STIL num segment (train and test may have different segments)\n    model.module.set_segment(args.test.dataset.params.num_segments)\n    model.eval()\n    y_outputs, y_labels = [], []\n    loss_t = 0.\n    with torch.no_grad():\n        for idx, datas in enumerate(tqdm(dataloader)):\n            # get input data from dataloader\n            images, labels, video_paths, segment_indices = datas",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "description": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "peekOfCode": "args = get_params()\nsetup(args)\ninit_exam_dir(args)\n###########################\n# main logic for training #\n###########################\ndef main():\n    # use distributed training with nccl backend \n    args.local_rank = int(os.environ.get('LOCAL_RANK', 0))\n    dist.init_process_group(backend='nccl', init_method=\"env://\")",
        "detail": "TFace.security.tasks.Face-Forgery-Detection.STIL.train",
        "documentation": {}
    },
    {
        "label": "DeepFaceController",
        "kind": 6,
        "importPath": "core.deepface_controller.controller",
        "description": "core.deepface_controller.controller",
        "peekOfCode": "class DeepFaceController():\n    def __init__(self):\n        # super().__init__()\n        self._face_detector_backend = [\n            'opencv', 'ssd', 'dlib', 'mtcnn', 'fastmtcnn',\n            'retinaface', 'mediapipe', 'yolov8', 'yunet', 'centerface'\n        ]\n        self._face_detector_threshold = 0.6\n        self._face_detector_enforce_detection = False\n        self._model_name = [",
        "detail": "core.deepface_controller.controller",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.deepface_controller.controller",
        "description": "core.deepface_controller.controller",
        "peekOfCode": "logger = get_logger()\n# Configurations for dependencies\nfrom deepface.commons import package_utils, folder_utils\npackage_utils.validate_for_keras3()\nwarnings.filterwarnings(\"ignore\")\ntf_version = package_utils.get_tf_major_version()\nif tf_version == 2:\n    tf.get_logger().setLevel(logging.ERROR)\n# Configure TensorFlow to use GPU\nmessange = None",
        "detail": "core.deepface_controller.controller",
        "documentation": {}
    },
    {
        "label": "tf_version",
        "kind": 5,
        "importPath": "core.deepface_controller.controller",
        "description": "core.deepface_controller.controller",
        "peekOfCode": "tf_version = package_utils.get_tf_major_version()\nif tf_version == 2:\n    tf.get_logger().setLevel(logging.ERROR)\n# Configure TensorFlow to use GPU\nmessange = None\nif len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')\n    messange = \"Using GPU for TensorFlow operations\"",
        "detail": "core.deepface_controller.controller",
        "documentation": {}
    },
    {
        "label": "messange",
        "kind": 5,
        "importPath": "core.deepface_controller.controller",
        "description": "core.deepface_controller.controller",
        "peekOfCode": "messange = None\nif len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')\n    messange = \"Using GPU for TensorFlow operations\"\n    logger.info(messange)\nelse:\n    messange = \"No GPU found. Running on CPU\"\n    logger.info(messange)",
        "detail": "core.deepface_controller.controller",
        "documentation": {}
    },
    {
        "label": "DeepFaceController",
        "kind": 6,
        "importPath": "core.deepface_controller.controller_old",
        "description": "core.deepface_controller.controller_old",
        "peekOfCode": "class DeepFaceController():\n    def __init__(self):\n        # super().__init__()\n        self._face_detector_backend = [\n            'opencv', 'ssd', 'dlib', 'mtcnn', 'fastmtcnn',\n            'retinaface', 'mediapipe', 'yolov8', 'yunet', 'centerface'\n        ]\n        self._face_detector_threshold = 0.6\n        self._face_detector_enforce_detection = False\n        self._model_name = [",
        "detail": "core.deepface_controller.controller_old",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.deepface_controller.controller_old",
        "description": "core.deepface_controller.controller_old",
        "peekOfCode": "logger = get_logger()\n# -----------------------------------\n# configurations for dependencies\nfrom deepface.commons import package_utils, folder_utils\n# Users should install tf_keras package if they are using tf 2.16 or later versions\npackage_utils.validate_for_keras3()\nwarnings.filterwarnings(\"ignore\")\ntf_version = package_utils.get_tf_major_version()\nif tf_version == 2:\n    tf.get_logger().setLevel(logging.ERROR)",
        "detail": "core.deepface_controller.controller_old",
        "documentation": {}
    },
    {
        "label": "tf_version",
        "kind": 5,
        "importPath": "core.deepface_controller.controller_old",
        "description": "core.deepface_controller.controller_old",
        "peekOfCode": "tf_version = package_utils.get_tf_major_version()\nif tf_version == 2:\n    tf.get_logger().setLevel(logging.ERROR)\nmessange = None\n# Configure TensorFlow to use GPU\nif len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n    # Ensure that TensorFlow uses the GPU\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')",
        "detail": "core.deepface_controller.controller_old",
        "documentation": {}
    },
    {
        "label": "messange",
        "kind": 5,
        "importPath": "core.deepface_controller.controller_old",
        "description": "core.deepface_controller.controller_old",
        "peekOfCode": "messange = None\n# Configure TensorFlow to use GPU\nif len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n    # Ensure that TensorFlow uses the GPU\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')\n    messange = (\"Using GPU for TensorFlow operations\")\n    logger.info(\"Using GPU for TensorFlow operations\")\nelse:",
        "detail": "core.deepface_controller.controller_old",
        "documentation": {}
    },
    {
        "label": "DirectoryHash",
        "kind": 6,
        "importPath": "core.utils.models.directory_hash",
        "description": "core.utils.models.directory_hash",
        "peekOfCode": "class DirectoryHash(Persistent):\n    def __init__(self, directory_name: str, hash_value: str):\n        self.directory_name = directory_name\n        self.hash_value = hash_value\n    def update_hash(self, new_hash_value: str):\n        self.hash_value = new_hash_value",
        "detail": "core.utils.models.directory_hash",
        "documentation": {}
    },
    {
        "label": "FaceData",
        "kind": 6,
        "importPath": "core.utils.models.face_data",
        "description": "core.utils.models.face_data",
        "peekOfCode": "class FaceData(Persistent):\n    def __init__(self, uid: str, image_paths: list, embedding: Optional[list] = None):\n        self.uid = uid\n        self.image_paths = image_paths\n        self.embedding = embedding if embedding is not None else []\n    def add_image(self, image_path: str, embedding: Optional[list] = None):\n        self.image_paths.append(image_path)\n        if embedding:\n            self.embedding.append(embedding)\n    def remove_image(self, image_path: str):",
        "detail": "core.utils.models.face_data",
        "documentation": {}
    },
    {
        "label": "TaskQueue",
        "kind": 6,
        "importPath": "core.utils.models.task_queue",
        "description": "core.utils.models.task_queue",
        "peekOfCode": "class TaskQueue(Persistent):\n    def __init__(self, task_id: str, status: str, created_at: float):\n        self.task_id = task_id\n        self.status = status\n        self.created_at = created_at\n        self.updated_at = None\n    def update_status(self, new_status: str):\n        self.status = new_status\n        self.updated_at = time.time()",
        "detail": "core.utils.models.task_queue",
        "documentation": {}
    },
    {
        "label": "shift_image",
        "kind": 2,
        "importPath": "core.utils.augment_images",
        "description": "core.utils.augment_images",
        "peekOfCode": "def shift_image(image, shift):\n    width, height = image.size\n    shifted_image = Image.new(\"RGB\", (width, height))\n    shifted_image.paste(image, (shift[0], shift[1]))\n    return shifted_image\ndef process_image(image, operation, *args):\n    if operation == 'rotate':\n        return image.rotate(*args, resample=Image.BICUBIC)\n    elif operation == 'flip':\n        return ImageOps.mirror(image)",
        "detail": "core.utils.augment_images",
        "documentation": {}
    },
    {
        "label": "process_image",
        "kind": 2,
        "importPath": "core.utils.augment_images",
        "description": "core.utils.augment_images",
        "peekOfCode": "def process_image(image, operation, *args):\n    if operation == 'rotate':\n        return image.rotate(*args, resample=Image.BICUBIC)\n    elif operation == 'flip':\n        return ImageOps.mirror(image)\n    elif operation == 'brightness':\n        enhancer = ImageEnhance.Brightness(image)\n        return enhancer.enhance(*args)\n    elif operation == 'shift':\n        return shift_image(image, args)",
        "detail": "core.utils.augment_images",
        "documentation": {}
    },
    {
        "label": "augment_image",
        "kind": 2,
        "importPath": "core.utils.augment_images",
        "description": "core.utils.augment_images",
        "peekOfCode": "def augment_image(image_data):\n    if hasattr(image_data, 'read'):\n        image = Image.open(image_data)\n        image.load()\n    elif isinstance(image_data, str):\n        image = Image.open(image_data)\n        image.load()\n    else:\n        image = image_data\n    augmented_images = []",
        "detail": "core.utils.augment_images",
        "documentation": {}
    },
    {
        "label": "ZoDB",
        "kind": 6,
        "importPath": "core.utils.database",
        "description": "core.utils.database",
        "peekOfCode": "class ZoDB:\n    def __init__(self, db_path='database/zodb.fs'):\n        storage = FileStorage.FileStorage(db_path)\n        self.db = DB(storage)\n        self.connection = None\n        self.root = None\n    def connect(self):\n        self.connection = self.db.open()\n        self.root = self.connection.root()\n        # Initialize persistent objects if they don't exist",
        "detail": "core.utils.database",
        "documentation": {}
    },
    {
        "label": "SQLiteManager",
        "kind": 6,
        "importPath": "core.utils.database_old",
        "description": "core.utils.database_old",
        "peekOfCode": "class SQLiteManager:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(__name__)\n        self.local = threading.local()  # Thread-local storage for thread-specific connections\n    def optimize_sqlite(self):\n        \"\"\"Optimize SQLite settings for better performance.\"\"\"\n        with self.get_connection() as conn:\n            conn.execute(\"PRAGMA synchronous = OFF\")\n            conn.execute(\"PRAGMA journal_mode = WAL\")",
        "detail": "core.utils.database_old",
        "documentation": {}
    },
    {
        "label": "SQLiteManager",
        "kind": 6,
        "importPath": "core.utils.databasev2",
        "description": "core.utils.databasev2",
        "peekOfCode": "class SQLiteManager:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(__name__)\n        # Initialize SQLite settings for optimization\n    def optimize_sqlite(self):\n        \"\"\"Optimize SQLite settings for better performance.\"\"\"\n        with self.get_connection() as conn:\n            conn.execute(\"PRAGMA synchronous = OFF\")\n            conn.execute(\"PRAGMA journal_mode = WAL\")",
        "detail": "core.utils.databasev2",
        "documentation": {}
    },
    {
        "label": "delete_directory_if_empty",
        "kind": 2,
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "peekOfCode": "def delete_directory_if_empty(save_dir: str) -> bool:\n    remaining_images = [f for f in os.listdir(save_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    if not remaining_images:\n        shutil.rmtree(save_dir)\n        logger.info(f\"Deleted empty directory: {save_dir}\")\n        return True\n    return False\ndef extract_base_identity(identity):\n    # Split the identity by '_' and take the last part without the augmentation part\n    identity = os.path.splitext(os.path.basename(identity))[0]",
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "extract_base_identity",
        "kind": 2,
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "peekOfCode": "def extract_base_identity(identity):\n    # Split the identity by '_' and take the last part without the augmentation part\n    identity = os.path.splitext(os.path.basename(identity))[0]\n    identity = \"_\".join(identity.split('_')[2:]).split('_')[0]\n    return identity\ndef delete_images_for_uid(uid: str, base_uid: str):\n    # Adjust the pattern to correctly match files associated with the base_uid and uid\n    image_pattern = os.path.join(IMAGES_DIR, base_uid, f\"*_{uid}*.png\")\n    # Ensure the pattern is correct and matches files\n    matching_files = glob.glob(image_pattern)",
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "delete_images_for_uid",
        "kind": 2,
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "peekOfCode": "def delete_images_for_uid(uid: str, base_uid: str):\n    # Adjust the pattern to correctly match files associated with the base_uid and uid\n    image_pattern = os.path.join(IMAGES_DIR, base_uid, f\"*_{uid}*.png\")\n    # Ensure the pattern is correct and matches files\n    matching_files = glob.glob(image_pattern)\n    if not matching_files:\n        logger.info(f\"No files found for pattern\")\n    for file_path in matching_files:\n        try:\n            os.remove(file_path)",
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "save_image",
        "kind": 2,
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "peekOfCode": "def save_image(image: Any, uid: str, base_dir: str, prefix: str, anti_spoofing: bool = False):\n    \"\"\"\n    Save the given image to a directory with a specific format. If the directory contains more than\n    56 images (8 original and 48 augmented), the oldest augmented images will be deleted.\n    Parameters:\n    - image: The image object to be saved.\n    - uid: The UID or identifier for the directory.\n    - base_dir: The base directory where the UID directory will be created.\n    - prefix: The prefix to use for the filename (e.g., \"query\" or UID).\n    - anti_spoofing: Boolean flag indicating if the image is flagged for anti-spoofing.",
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "delete_images",
        "kind": 2,
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "peekOfCode": "def delete_images(directory: str):\n    \"\"\"\n    Delete all images in the specified directory.\n    Parameters:\n    - directory: The directory where images will be deleted.\n    \"\"\"\n    if os.path.exists(directory):\n        for filename in os.listdir(directory):\n            file_path = os.path.join(directory, filename)\n            if os.path.isfile(file_path):",
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.utils.images_handler",
        "description": "core.utils.images_handler",
        "peekOfCode": "logger = get_logger()\ndef delete_directory_if_empty(save_dir: str) -> bool:\n    remaining_images = [f for f in os.listdir(save_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    if not remaining_images:\n        shutil.rmtree(save_dir)\n        logger.info(f\"Deleted empty directory: {save_dir}\")\n        return True\n    return False\ndef extract_base_identity(identity):\n    # Split the identity by '_' and take the last part without the augmentation part",
        "detail": "core.utils.images_handler",
        "documentation": {}
    },
    {
        "label": "Logger",
        "kind": 6,
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "peekOfCode": "class Logger:\n    def __init__(self, module=None):\n        self.module = module\n        self.log_file_path = LOG_PATH_FILE\n        self.log_level = self._get_log_level()\n        self._setup_logging()\n        self._create_log_files()  # Create log files if they don't exist\n    def _get_log_level(self):\n        log_levels = {\n            \"CRITICAL\": logging.CRITICAL,",
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "core.utils.logging",
        "description": "core.utils.logging",
        "peekOfCode": "def get_logger():\n    return Logger()",
        "detail": "core.utils.logging",
        "documentation": {}
    },
    {
        "label": "require_api_key",
        "kind": 2,
        "importPath": "core.utils.middleware",
        "description": "core.utils.middleware",
        "peekOfCode": "def require_api_key(f):\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        api_key = request.headers.get('X-API-Key')\n        # print(f\"Received API Key: {api_key}\")\n        # print(f\"Expected API Key: {API_KEY}\")\n        if api_key and api_key == API_KEY:\n            return f(*args, **kwargs)\n        else:\n            return jsonify({\"message\": \"Invalid or missing API key\"}), 403",
        "detail": "core.utils.middleware",
        "documentation": {}
    },
    {
        "label": "cached_read",
        "kind": 2,
        "importPath": "core.utils.monitor_folder_hash",
        "description": "core.utils.monitor_folder_hash",
        "peekOfCode": "def cached_read(filepath):\n    with open(filepath, 'rb') as file:\n        return file.read()\nlogger = get_logger()\ndef has_directory_changed(directory_path: str, previous_hash: Optional[str] = None) -> (bool, str): # type: ignore\n    current_hash = hash_directory(directory_path)\n    return previous_hash is None or current_hash != previous_hash, current_hash\ndef hash_directory(directory_path: str) -> str:\n    hash_obj = hashlib.md5()\n    for root, _, files in os.walk(directory_path):",
        "detail": "core.utils.monitor_folder_hash",
        "documentation": {}
    },
    {
        "label": "has_directory_changed",
        "kind": 2,
        "importPath": "core.utils.monitor_folder_hash",
        "description": "core.utils.monitor_folder_hash",
        "peekOfCode": "def has_directory_changed(directory_path: str, previous_hash: Optional[str] = None) -> (bool, str): # type: ignore\n    current_hash = hash_directory(directory_path)\n    return previous_hash is None or current_hash != previous_hash, current_hash\ndef hash_directory(directory_path: str) -> str:\n    hash_obj = hashlib.md5()\n    for root, _, files in os.walk(directory_path):\n        for filename in sorted(files):\n            if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n                filepath = os.path.join(root, filename)\n                try:",
        "detail": "core.utils.monitor_folder_hash",
        "documentation": {}
    },
    {
        "label": "hash_directory",
        "kind": 2,
        "importPath": "core.utils.monitor_folder_hash",
        "description": "core.utils.monitor_folder_hash",
        "peekOfCode": "def hash_directory(directory_path: str) -> str:\n    hash_obj = hashlib.md5()\n    for root, _, files in os.walk(directory_path):\n        for filename in sorted(files):\n            if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n                filepath = os.path.join(root, filename)\n                try:\n                    stat = os.stat(filepath)\n                    file_content = cached_read(filepath)\n                    hash_obj.update(f\"{filename}{stat.st_size}{stat.st_mtime}\".encode())",
        "detail": "core.utils.monitor_folder_hash",
        "documentation": {}
    },
    {
        "label": "check_and_update_directory_hash",
        "kind": 2,
        "importPath": "core.utils.monitor_folder_hash",
        "description": "core.utils.monitor_folder_hash",
        "peekOfCode": "def check_and_update_directory_hash(dir_name: str, dir_path: str , app):\n    \"\"\"Kim tra hash ca th mc v cp nht vo ZoDB nu c thay i.\"\"\"\n    directory_hash = app.config[\"ZoDB\"].get_directory_hash(dir_name)\n    if directory_hash is None:\n        # First run or hash missing, just set the current hash without triggering DB recreation\n        current_hash = hash_directory(dir_path)\n        logger.info(f\"Setting initial hash for directory {dir_name}.\")\n        app.config[\"ZoDB\"].set_directory_hash(dir_name, current_hash)\n        app.config[\"deepface_controller\"].find(\n            img_path=os.path.join(BASE_PATH, \"static\", \"temp.png\"),",
        "detail": "core.utils.monitor_folder_hash",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.utils.monitor_folder_hash",
        "description": "core.utils.monitor_folder_hash",
        "peekOfCode": "logger = get_logger()\nfrom functools import lru_cache\n@lru_cache(maxsize=100)\ndef cached_read(filepath):\n    with open(filepath, 'rb') as file:\n        return file.read()\nlogger = get_logger()\ndef has_directory_changed(directory_path: str, previous_hash: Optional[str] = None) -> (bool, str): # type: ignore\n    current_hash = hash_directory(directory_path)\n    return previous_hash is None or current_hash != previous_hash, current_hash",
        "detail": "core.utils.monitor_folder_hash",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.utils.monitor_folder_hash",
        "description": "core.utils.monitor_folder_hash",
        "peekOfCode": "logger = get_logger()\ndef has_directory_changed(directory_path: str, previous_hash: Optional[str] = None) -> (bool, str): # type: ignore\n    current_hash = hash_directory(directory_path)\n    return previous_hash is None or current_hash != previous_hash, current_hash\ndef hash_directory(directory_path: str) -> str:\n    hash_obj = hashlib.md5()\n    for root, _, files in os.walk(directory_path):\n        for filename in sorted(files):\n            if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n                filepath = os.path.join(root, filename)",
        "detail": "core.utils.monitor_folder_hash",
        "documentation": {}
    },
    {
        "label": "TEMP_DIR",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "TEMP_DIR = tempfile.gettempdir()\nBASE_PATH = \"core\"\nDB_PATH = os.path.join(BASE_PATH, \"database\", \"zodb.fs\")\nIMAGES_DIR = os.path.join(BASE_PATH, \"static\", \"images\")\nVECTOR_SIZE = 512\nMAX_SCORE = -1\nAPI_KEY = os.getenv(\"API_KEY\")\nLOG_PATH_FILE = os.path.join(BASE_PATH, \"logs\")\nNUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "BASE_PATH",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "BASE_PATH = \"core\"\nDB_PATH = os.path.join(BASE_PATH, \"database\", \"zodb.fs\")\nIMAGES_DIR = os.path.join(BASE_PATH, \"static\", \"images\")\nVECTOR_SIZE = 512\nMAX_SCORE = -1\nAPI_KEY = os.getenv(\"API_KEY\")\nLOG_PATH_FILE = os.path.join(BASE_PATH, \"logs\")\nNUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "DB_PATH",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "DB_PATH = os.path.join(BASE_PATH, \"database\", \"zodb.fs\")\nIMAGES_DIR = os.path.join(BASE_PATH, \"static\", \"images\")\nVECTOR_SIZE = 512\nMAX_SCORE = -1\nAPI_KEY = os.getenv(\"API_KEY\")\nLOG_PATH_FILE = os.path.join(BASE_PATH, \"logs\")\nNUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "IMAGES_DIR",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "IMAGES_DIR = os.path.join(BASE_PATH, \"static\", \"images\")\nVECTOR_SIZE = 512\nMAX_SCORE = -1\nAPI_KEY = os.getenv(\"API_KEY\")\nLOG_PATH_FILE = os.path.join(BASE_PATH, \"logs\")\nNUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "VECTOR_SIZE",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "VECTOR_SIZE = 512\nMAX_SCORE = -1\nAPI_KEY = os.getenv(\"API_KEY\")\nLOG_PATH_FILE = os.path.join(BASE_PATH, \"logs\")\nNUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "MAX_SCORE",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "MAX_SCORE = -1\nAPI_KEY = os.getenv(\"API_KEY\")\nLOG_PATH_FILE = os.path.join(BASE_PATH, \"logs\")\nNUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "API_KEY",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "API_KEY = os.getenv(\"API_KEY\")\nLOG_PATH_FILE = os.path.join(BASE_PATH, \"logs\")\nNUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "LOG_PATH_FILE",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "LOG_PATH_FILE = os.path.join(BASE_PATH, \"logs\")\nNUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "NUMBER_WORKER",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "NUMBER_WORKER = min(32, os.cpu_count() or 1)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_ENABLE_ONEDNN_OPTS']",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_USE_LEGACY_KERAS\"]",
        "kind": 5,
        "importPath": "core.utils.static_variable",
        "description": "core.utils.static_variable",
        "peekOfCode": "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"",
        "detail": "core.utils.static_variable",
        "documentation": {}
    },
    {
        "label": "generate_unique_task_id",
        "kind": 2,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "def generate_unique_task_id() -> str:\n    return f\"task-{int(time.time())}-{uuid.uuid4().hex}\"\ndef add_task_to_queue(func, *args, **kwargs):\n    task_id = generate_unique_task_id()\n    with queue_lock:\n        if task_queue.full():\n            logger.warning(f\"Task queue is full, waiting to add task {task_id}\")\n        task_queue.put((task_id, lambda: func(*args, **kwargs)))\n        logger.info(f\"Added task {task_id} to the queue\")\ndef worker(app):",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "add_task_to_queue",
        "kind": 2,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "def add_task_to_queue(func, *args, **kwargs):\n    task_id = generate_unique_task_id()\n    with queue_lock:\n        if task_queue.full():\n            logger.warning(f\"Task queue is full, waiting to add task {task_id}\")\n        task_queue.put((task_id, lambda: func(*args, **kwargs)))\n        logger.info(f\"Added task {task_id} to the queue\")\ndef worker(app):\n    logger.info(\"Worker thread started\")\n    with app.app_context():",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "worker",
        "kind": 2,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "def worker(app):\n    logger.info(\"Worker thread started\")\n    with app.app_context():\n        while True:\n            task_id, task = task_queue.get()\n            if task is None:\n                break\n            start_time = time.time()\n            try:\n                logger.info(f\"Worker picked up task {task_id}\")",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "start_workers",
        "kind": 2,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "def start_workers(app):\n    global threads\n    if threads:  # Check if workers are already started\n        logger.warning(\"Workers already started.\")\n        return\n    for i in range(NUMBER_WORKER):\n        t = threading.Thread(target=worker, args=(app,), name=f\"Worker-{i}\")\n        t.start()\n        threads.append(t)\n        logger.info(f\"Worker thread {i} started\")",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "stop_workers",
        "kind": 2,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "def stop_workers():\n    global threads\n    for _ in range(NUMBER_WORKER):\n        task_queue.put((None, None))\n    for t in threads:\n        t.join()\n    # Cleanup\n    threads = []\n    logger.info(\"All workers stopped\")",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "executor",
        "kind": 5,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "executor = ThreadPoolExecutor(max_workers=NUMBER_WORKER)\ntask_queue = queue.Queue(maxsize=100)  # Bounded queue to prevent overflow\nlogger = get_logger()\nthreads = []\nqueue_lock = threading.Lock()\n# Initialize ThreadPoolExecutor once and reuse\nexecutor = ThreadPoolExecutor(max_workers=NUMBER_WORKER)\ndef generate_unique_task_id() -> str:\n    return f\"task-{int(time.time())}-{uuid.uuid4().hex}\"\ndef add_task_to_queue(func, *args, **kwargs):",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "task_queue",
        "kind": 5,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "task_queue = queue.Queue(maxsize=100)  # Bounded queue to prevent overflow\nlogger = get_logger()\nthreads = []\nqueue_lock = threading.Lock()\n# Initialize ThreadPoolExecutor once and reuse\nexecutor = ThreadPoolExecutor(max_workers=NUMBER_WORKER)\ndef generate_unique_task_id() -> str:\n    return f\"task-{int(time.time())}-{uuid.uuid4().hex}\"\ndef add_task_to_queue(func, *args, **kwargs):\n    task_id = generate_unique_task_id()",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "logger = get_logger()\nthreads = []\nqueue_lock = threading.Lock()\n# Initialize ThreadPoolExecutor once and reuse\nexecutor = ThreadPoolExecutor(max_workers=NUMBER_WORKER)\ndef generate_unique_task_id() -> str:\n    return f\"task-{int(time.time())}-{uuid.uuid4().hex}\"\ndef add_task_to_queue(func, *args, **kwargs):\n    task_id = generate_unique_task_id()\n    with queue_lock:",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "threads",
        "kind": 5,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "threads = []\nqueue_lock = threading.Lock()\n# Initialize ThreadPoolExecutor once and reuse\nexecutor = ThreadPoolExecutor(max_workers=NUMBER_WORKER)\ndef generate_unique_task_id() -> str:\n    return f\"task-{int(time.time())}-{uuid.uuid4().hex}\"\ndef add_task_to_queue(func, *args, **kwargs):\n    task_id = generate_unique_task_id()\n    with queue_lock:\n        if task_queue.full():",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "queue_lock",
        "kind": 5,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "queue_lock = threading.Lock()\n# Initialize ThreadPoolExecutor once and reuse\nexecutor = ThreadPoolExecutor(max_workers=NUMBER_WORKER)\ndef generate_unique_task_id() -> str:\n    return f\"task-{int(time.time())}-{uuid.uuid4().hex}\"\ndef add_task_to_queue(func, *args, **kwargs):\n    task_id = generate_unique_task_id()\n    with queue_lock:\n        if task_queue.full():\n            logger.warning(f\"Task queue is full, waiting to add task {task_id}\")",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "executor",
        "kind": 5,
        "importPath": "core.utils.threading",
        "description": "core.utils.threading",
        "peekOfCode": "executor = ThreadPoolExecutor(max_workers=NUMBER_WORKER)\ndef generate_unique_task_id() -> str:\n    return f\"task-{int(time.time())}-{uuid.uuid4().hex}\"\ndef add_task_to_queue(func, *args, **kwargs):\n    task_id = generate_unique_task_id()\n    with queue_lock:\n        if task_queue.full():\n            logger.warning(f\"Task queue is full, waiting to add task {task_id}\")\n        task_queue.put((task_id, lambda: func(*args, **kwargs)))\n        logger.info(f\"Added task {task_id} to the queue\")",
        "detail": "core.utils.threading",
        "documentation": {}
    },
    {
        "label": "home",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def home():\n    version = service.check_version()\n    return version\n@blueprint.route(\"/users\", methods=[\"GET\"])\n@require_api_key\ndef get_users():\n    uid_filter = request.args.get(\"uid\")  # Get UID from query parameter, if provided\n    try:\n        users = service.list_users(uid_filter, current_app)\n        return users",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "get_users",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def get_users():\n    uid_filter = request.args.get(\"uid\")  # Get UID from query parameter, if provided\n    try:\n        users = service.list_users(uid_filter, current_app)\n        return users\n    except Exception as e:\n        logger.error(f\"Failed to retrieve user list: {str(e)}\")\n        return {\n            \"message\": \"Failed to retrieve user list\",\n            \"data\": None,",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "fetchFace",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def fetchFace():\n    face_data = service.get_Face_embedding(current_app)\n    return face_data\n@blueprint.route(\"/hash_dir\", methods=[\"GET\"])\n@require_api_key\ndef hash_dir():\n    hash_data = service.hash_directory_data(current_app)\n    return hash_data\n@blueprint.route(\"/register\", methods=[\"POST\"])\n@require_api_key",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "hash_dir",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def hash_dir():\n    hash_data = service.hash_directory_data(current_app)\n    return hash_data\n@blueprint.route(\"/register\", methods=[\"POST\"])\n@require_api_key\ndef register():\n    uid = request.form.get(\"uid\")\n    if not uid:\n        return {\"message\": \"UID is required\", \"data\": None, \"success\": False}, 400\n    if \"image\" not in request.files:",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "register",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def register():\n    uid = request.form.get(\"uid\")\n    if not uid:\n        return {\"message\": \"UID is required\", \"data\": None, \"success\": False}, 400\n    if \"image\" not in request.files:\n        return {\"message\": \"Image is required\", \"data\": None, \"success\": False}, 400\n    image = request.files[\"image\"]\n    try:\n        response = service.register_face(image, uid, current_app)\n        return response",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "delete_face",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def delete_face():\n    uid = request.form.get(\"uid\")\n    if not uid:\n        return {\"message\": \"UID is required\", \"data\": None, \"success\": False}, 400\n    try:\n        response = service.delete_face(uid, current_app)\n        return response\n    except Exception as e:\n        logger.error(f\"Failed to delete face: {str(e)}\")\n        return {\"message\": \"Failed to delete face\", \"data\": None, \"success\": False}, 500",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "recognize",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def recognize():\n    if \"image\" not in request.files:\n        return {\"message\": \"Image is required\", \"data\": None, \"success\": False}, 400\n    uid = request.form.get(\"uid\")\n    image = request.files[\"image\"]\n    try:\n        response = service.recognize_face(image, uid)\n        return response\n    except Exception as e:\n        logger.error(f\"Failed to recognize face: {str(e)}\")",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "represent",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def represent():\n    input_args = request.get_json()\n    if input_args is None:\n        return {\n            \"message\": \"Empty input set passed\",\n            \"data\": None,\n            \"success\": False,\n        }, 400\n    img_path = input_args.get(\"img\") or input_args.get(\"img_path\")\n    if img_path is None:",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "verify",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def verify():\n    input_args = request.get_json()\n    if input_args is None:\n        return {\n            \"message\": \"Empty input set passed\",\n            \"data\": None,\n            \"success\": False,\n        }, 400\n    img1_path = input_args.get(\"img1\") or input_args.get(\"img1_path\")\n    img2_path = input_args.get(\"img2\") or input_args.get(\"img2_path\")",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "analyze",
        "kind": 2,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "def analyze():\n    try:\n        input_args = request.get_json()\n        if input_args is None:\n            return {\"message\": \"Empty input set passed\"}\n        img_path = input_args.get(\"img\") or input_args.get(\"img_path\")\n        if img_path is None:\n            return {\"message\": \"You must pass img_path input\"}\n        demographies = service.analyze(\n            img_path=img_path,",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "logger = get_logger()\nblueprint = Blueprint(\"routes\", __name__)\n# Assuming you have an instance of DeepFaceController\n@blueprint.route(\"/\")\ndef home():\n    version = service.check_version()\n    return version\n@blueprint.route(\"/users\", methods=[\"GET\"])\n@require_api_key\ndef get_users():",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "blueprint",
        "kind": 5,
        "importPath": "core.routes",
        "description": "core.routes",
        "peekOfCode": "blueprint = Blueprint(\"routes\", __name__)\n# Assuming you have an instance of DeepFaceController\n@blueprint.route(\"/\")\ndef home():\n    version = service.check_version()\n    return version\n@blueprint.route(\"/users\", methods=[\"GET\"])\n@require_api_key\ndef get_users():\n    uid_filter = request.args.get(\"uid\")  # Get UID from query parameter, if provided",
        "detail": "core.routes",
        "documentation": {}
    },
    {
        "label": "get_deepface_controller",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def get_deepface_controller():\n    return flask.current_app.config[\"deepface_controller\"]\ndef hash_directory_data(app) -> Dict[str, Any]:\n    try:\n        hash_data = app.config[\"ZoDB\"].list_all_directory_hashes()\n        return {\n            \"message\": \"Hash data fetched successfully\",\n            \"data\": hash_data,\n            \"success\": True,\n        }",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "hash_directory_data",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def hash_directory_data(app) -> Dict[str, Any]:\n    try:\n        hash_data = app.config[\"ZoDB\"].list_all_directory_hashes()\n        return {\n            \"message\": \"Hash data fetched successfully\",\n            \"data\": hash_data,\n            \"success\": True,\n        }\n    except Exception as e:\n        logger.error(",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "get_Face_embedding",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def get_Face_embedding(uid: str, current_app) -> Dict[str, Any]:\n    try:\n        face_User = current_app.config[\"ZoDB\"].list_face_data(uid_filter = uid)\n        return {\n            \"message\": \"Face embedding fetched successfully\",\n            \"data\": face_User,\n            \"success\": True,\n        }\n    except Exception as e:\n        logger.error(",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "list_users",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def list_users(\n    uid_filter: Optional[str] = None, app: Optional[Any] = None\n) -> Dict[str, Any]:\n    users = []\n    for base_dir in filter(\n        lambda d: os.path.isdir(os.path.join(IMAGES_DIR, d)), os.listdir(IMAGES_DIR)\n    ):\n        user_files = glob.glob(os.path.join(IMAGES_DIR, base_dir, \"*.png\"))\n        unique_uids = {\n            extract_base_identity(f)",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "check_version",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def check_version() -> Dict[str, Any]:\n    try:\n        version = get_deepface_controller().check_version()\n        return {\n            \"message\": \"Version fetched successfully\",\n            \"data\": version,\n            \"success\": True,\n        }\n    except Exception as e:\n        logger.error(",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "recreate_DB",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def recreate_DB(img_path: str, app: Any, uid: str) -> None:\n    logger.info(f\"Starting recreate_DB with img_path: {img_path}, uid: {uid}\")\n    from core.utils.monitor_folder_hash import (\n        hash_directory,\n    )\n    try:\n        with app.app_context():\n            base_uid = uid.split(\"-\")[0]\n            save_dir = os.path.join(img_path, base_uid)\n            get_deepface_controller().find(",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "import_db",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def import_db( img_path: List[str], app: Any, uid: str) -> Dict[str, Any]:\n    for image in img_path:\n        embedding_objs = get_deepface_controller().represent(\n                    img_path=image,\n                    model_name=\"Facenet512\",\n                    detector_backend=\"retinaface\",\n                    anti_spoofing=False,\n                )\n        embedding = embedding_objs[0][\"embedding\"] if embedding_objs else None\n        if embedding:",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "delete_face",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def delete_face(uid: str, current_app) -> Dict[str, Any]:\n    try:\n        base_uid = uid.split(\"-\")[0]\n        save_dir = os.path.join(IMAGES_DIR, base_uid)\n        # Delete images and check if directory is empty\n        delete_images_for_uid(uid, base_uid)\n        if delete_directory_if_empty(save_dir):\n            current_app.config[\"ZoDB\"].del_directory_hash(uid)\n            return {\n                \"message\": \"Face deleted successfully and directory removed.\",",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "register_face",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def register_face(image: Any, uid: str, current_app) -> Dict[str, Any]:\n    try:\n        # Save original and augmented images\n        image_path, _ = save_image(image, uid, IMAGES_DIR, uid)\n        augmented_images = augment_image(image)\n        for i, img in enumerate(augmented_images, start=1):\n            save_image(img, uid, IMAGES_DIR, f\"{uid}_aug{i}\")\n        add_task_to_queue(\n            import_db,\n            img_path=augmented_images,",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "recognize_face",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def recognize_face(image: Any, uid: Optional[str] = None) -> Dict[str, Any]:\n    try:\n        save_dir = IMAGES_DIR if uid is None else os.path.join(IMAGES_DIR, uid.split('-')[0])\n        image_path, _ = save_image(image, \"query_results\", TEMP_DIR, \"\")\n        logger.info(f\"Recognizing face with UID {uid} in directory {save_dir}\")\n        if not any(f.lower().endswith(('.png', '.jpg', '.jpeg')) for f in os.listdir(save_dir)):\n            return {\"message\": \"No faces detected in the image\", \"data\": [], \"success\": False}\n        anti_spoofing_results = get_deepface_controller().extract_faces(img_path=image_path, detector_backend=\"retinaface\", anti_spoofing=True)[0]\n        recognition_results = get_deepface_controller().find(img_path=image_path, db_path=save_dir, model_name=\"Facenet512\", detector_backend=\"retinaface\", anti_spoofing=False)\n        if recognition_results and not recognition_results[0].empty:",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "represent",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def represent(\n    img_path: str,\n    model_name: str,\n    detector_backend: str,\n    enforce_detection: bool,\n    align: bool,\n    anti_spoofing: bool,\n    max_faces: Optional[int] = None,\n) -> Dict[str, Any]:\n    try:",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "verify",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def verify(\n    img1_path: str,\n    img2_path: str,\n    model_name: str,\n    detector_backend: str,\n    distance_metric: str,\n    enforce_detection: bool,\n    align: bool,\n    anti_spoofing: bool,\n) -> Dict[str, Any]:",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "analyze",
        "kind": 2,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "def analyze(\n    img_path: str,\n    actions: List[str],\n    detector_backend: str,\n    enforce_detection: bool,\n    align: bool,\n    anti_spoofing: bool,\n) -> Dict[str, Any]:\n    try:\n        demographies = get_deepface_controller().analyze(",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.service",
        "description": "core.service",
        "peekOfCode": "logger = get_logger()\ndef get_deepface_controller():\n    return flask.current_app.config[\"deepface_controller\"]\ndef hash_directory_data(app) -> Dict[str, Any]:\n    try:\n        hash_data = app.config[\"ZoDB\"].list_all_directory_hashes()\n        return {\n            \"message\": \"Hash data fetched successfully\",\n            \"data\": hash_data,\n            \"success\": True,",
        "detail": "core.service",
        "documentation": {}
    },
    {
        "label": "deepface_app",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "deepface_app = app.create_app()\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-p\", \"--port\", type=int,default=5000, help=\"Port of serving api\")\n    args = parser.parse_args()\n    deepface_app.run(host=\"0.0.0.0\", port=args.port, debug=True,load_dotenv=True, use_reloader=False)",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "create_app",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def create_app():\n    app = Flask(__name__)\n    # Create necessary directories\n    directories = [\"static\", \"static/images\", \"database\"]\n    base_paths = [os.path.join(BASE_PATH, d) for d in directories]\n    for path in base_paths:\n        os.makedirs(path, exist_ok=True)\n    with app.app_context():\n        app.config[\"deepface_controller\"] = deepface_controller\n        # Connection can be created when needed instead of at startup",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "logger = get_logger()\ndeepface_controller = DeepFaceController()\ndef create_app():\n    app = Flask(__name__)\n    # Create necessary directories\n    directories = [\"static\", \"static/images\", \"database\"]\n    base_paths = [os.path.join(BASE_PATH, d) for d in directories]\n    for path in base_paths:\n        os.makedirs(path, exist_ok=True)\n    with app.app_context():",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "deepface_controller",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "deepface_controller = DeepFaceController()\ndef create_app():\n    app = Flask(__name__)\n    # Create necessary directories\n    directories = [\"static\", \"static/images\", \"database\"]\n    base_paths = [os.path.join(BASE_PATH, d) for d in directories]\n    for path in base_paths:\n        os.makedirs(path, exist_ok=True)\n    with app.app_context():\n        app.config[\"deepface_controller\"] = deepface_controller",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "FastCgiRecord",
        "kind": 6,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "class FastCgiRecord(object):\n    \"\"\"Represents a FastCgiRecord.  Encapulates the type, role, flags.  Holds\n    onto the params which we will receive and update later.\"\"\"\n    def __init__(self, type, req_id, role, flags):\n        self.type = type\n        self.req_id = req_id\n        self.role = role\n        self.flags = flags\n        self.params = {}\n    def __repr__(self):",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "_ExitException",
        "kind": 6,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "class _ExitException(Exception):\n    pass\nif sys.version_info[0] >= 3:\n    # indexing into byte strings gives us an int, so\n    # ord is unnecessary on Python 3\n    def ord(x):\n        return x\n    def chr(x):\n        return bytes((x, ))\n    def wsgi_decode(x):",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FILE_NOTIFY_INFORMATION",
        "kind": 6,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "class FILE_NOTIFY_INFORMATION(ctypes.Structure):\n    _fields_ = [('NextEntryOffset', ctypes.c_uint32),\n                ('Action', ctypes.c_uint32),\n                ('FileNameLength', ctypes.c_uint32),\n                ('Filename', ctypes.c_wchar)]\n_ON_EXIT_TASKS = None\ndef run_exit_tasks():\n    global _ON_EXIT_TASKS\n    maybe_log(\"Running on_exit tasks\")\n    while _ON_EXIT_TASKS:",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "handle_response",
        "kind": 6,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "class handle_response(object):\n    \"\"\"A context manager for handling the response. This will ensure that\n    exceptions in the handler are correctly reported, and the FastCGI request is\n    properly terminated.\n    \"\"\"\n    def __init__(self, stream, record, get_output, get_errors):\n        self.stream = stream\n        self.record = record\n        self._get_output = get_output\n        self._get_errors = get_errors",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_fastcgi_record",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_fastcgi_record(stream):\n    \"\"\"reads the main fast cgi record\"\"\"\n    data = stream.read(8)     # read record\n    if not data:\n        # no more data, our other process must have died...\n        raise _ExitException()\n    fcgi_ver, reqtype, req_id, content_size, padding_len, _ = struct.unpack('>BBHHBB', data)\n    content = stream.read(content_size)  # read content\n    stream.read(padding_len)\n    if fcgi_ver != FCGI_VERSION_1:",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_fastcgi_begin_request",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_fastcgi_begin_request(stream, req_id, content):\n    \"\"\"reads the begin request body and updates our _REQUESTS table to include\n    the new request\"\"\"\n    #    typedef struct {\n    #        unsigned char roleB1;\n    #        unsigned char roleB0;\n    #        unsigned char flags;\n    #        unsigned char reserved[5];\n    #    } FCGI_BeginRequestBody;\n    # TODO: Ignore request if it exists",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_encoded_int",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_encoded_int(content, offset):\n    i = struct.unpack_from('>B', content, offset)[0]\n    if i < 0x80:\n        return offset + 1, i\n    return offset + 4, struct.unpack_from('>I', content, offset)[0] & ~0x80000000\ndef read_fastcgi_keyvalue_pairs(content, offset):\n    \"\"\"Reads a FastCGI key/value pair stream\"\"\"\n    offset, name_len = read_encoded_int(content, offset)\n    offset, value_len = read_encoded_int(content, offset)\n    name = content[offset:(offset + name_len)]",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_fastcgi_keyvalue_pairs",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_fastcgi_keyvalue_pairs(content, offset):\n    \"\"\"Reads a FastCGI key/value pair stream\"\"\"\n    offset, name_len = read_encoded_int(content, offset)\n    offset, value_len = read_encoded_int(content, offset)\n    name = content[offset:(offset + name_len)]\n    offset += name_len\n    value = content[offset:(offset + value_len)]\n    offset += value_len\n    return offset, name, value\ndef get_encoded_int(i):",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "get_encoded_int",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def get_encoded_int(i):\n    \"\"\"Writes the length of a single name for a key or value in a key/value\n    stream\"\"\"\n    if i <= 0x7f:\n        return struct.pack('>B', i)\n    elif i < 0x80000000:\n        return struct.pack('>I', i | 0x80000000)\n    else:\n        raise ValueError('cannot encode value %s (%x) because it is too large' % (i, i))\ndef write_fastcgi_keyvalue_pairs(pairs):",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "write_fastcgi_keyvalue_pairs",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def write_fastcgi_keyvalue_pairs(pairs):\n    \"\"\"Creates a FastCGI key/value stream and returns it as a byte string\"\"\"\n    parts = []\n    for raw_key, raw_value in pairs.items():\n        key = wsgi_encode(raw_key)\n        value = wsgi_encode(raw_value)\n        parts.append(get_encoded_int(len(key)))\n        parts.append(get_encoded_int(len(value)))\n        parts.append(key)\n        parts.append(value)",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_fastcgi_params",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_fastcgi_params(stream, req_id, content):\n    if not content:\n        return None\n    offset = 0\n    res = _REQUESTS[req_id].params\n    while offset < len(content):\n        offset, name, value = read_fastcgi_keyvalue_pairs(content, offset)\n        name = wsgi_decode(name)\n        raw_name = RAW_VALUE_NAMES.get(name)\n        if raw_name:",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_fastcgi_input",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_fastcgi_input(stream, req_id, content):\n    \"\"\"reads FastCGI std-in and stores it in wsgi.input passed in the\n    wsgi environment array\"\"\"\n    res = _REQUESTS[req_id].params\n    if 'wsgi.input' not in res:\n        res['wsgi.input'] = content\n    else:\n        res['wsgi.input'] += content\n    if not content:\n        # we've hit the end of the input stream, time to process input...",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_fastcgi_data",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_fastcgi_data(stream, req_id, content):\n    \"\"\"reads FastCGI data stream and publishes it as wsgi.data\"\"\"\n    res = _REQUESTS[req_id].params\n    if 'wsgi.data' not in res:\n        res['wsgi.data'] = content\n    else:\n        res['wsgi.data'] += content\ndef read_fastcgi_abort_request(stream, req_id, content):\n    \"\"\"reads the wsgi abort request, which we ignore, we'll send the\n    finish execution request anyway...\"\"\"",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_fastcgi_abort_request",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_fastcgi_abort_request(stream, req_id, content):\n    \"\"\"reads the wsgi abort request, which we ignore, we'll send the\n    finish execution request anyway...\"\"\"\n    pass\ndef read_fastcgi_get_values(stream, req_id, content):\n    \"\"\"reads the fastcgi request to get parameter values, and immediately \n    responds\"\"\"\n    offset = 0\n    request = {}\n    while offset < len(content):",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_fastcgi_get_values",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_fastcgi_get_values(stream, req_id, content):\n    \"\"\"reads the fastcgi request to get parameter values, and immediately \n    responds\"\"\"\n    offset = 0\n    request = {}\n    while offset < len(content):\n        offset, name, value = read_fastcgi_keyvalue_pairs(content, offset)\n        request[name] = value\n    response = {}\n    if FCGI_MAX_CONNS in request:",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "log",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def log(txt):\n    \"\"\"Logs messages to a log file if WSGI_LOG env var is defined.\"\"\"\n    if APPINSIGHT_CLIENT:\n        try:\n            APPINSIGHT_CLIENT.track_event(txt)\n        except:\n            pass\n    log_file = os.environ.get('WSGI_LOG')\n    if log_file:\n        with open(log_file, 'a+', encoding='utf-8') as f:",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "maybe_log",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def maybe_log(txt):\n    \"\"\"Logs messages to a log file if WSGI_LOG env var is defined, and does not\n    raise exceptions if logging fails.\"\"\"\n    try:\n        log(txt)\n    except:\n        pass\ndef send_response(stream, req_id, resp_type, content, streaming=True):\n    \"\"\"sends a response w/ the given id, type, and content to the server.\n    If the content is streaming then an empty record is sent at the end to ",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "send_response",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def send_response(stream, req_id, resp_type, content, streaming=True):\n    \"\"\"sends a response w/ the given id, type, and content to the server.\n    If the content is streaming then an empty record is sent at the end to \n    terminate the stream\"\"\"\n    if not isinstance(content, bytes):\n        raise TypeError(\"content must be encoded before sending: %r\" % content)\n    offset = 0\n    while True:\n        len_remaining = max(min(len(content) - offset, 0xFFFF), 0)\n        data = struct.pack(",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "get_environment",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def get_environment(dir):\n    web_config = os.path.join(dir, 'Web.config')\n    if not os.path.exists(web_config):\n        return {}\n    d = {}\n    doc = minidom.parse(web_config)\n    config = doc.getElementsByTagName('configuration')\n    for configSection in config:\n        appSettings = configSection.getElementsByTagName('appSettings')\n        for appSettingsSection in appSettings:",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "run_exit_tasks",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def run_exit_tasks():\n    global _ON_EXIT_TASKS\n    maybe_log(\"Running on_exit tasks\")\n    while _ON_EXIT_TASKS:\n        tasks, _ON_EXIT_TASKS = _ON_EXIT_TASKS, []\n        for t in tasks:\n            try:\n                t()\n            except Exception:\n                maybe_log(\"Error in exit task: \" + traceback.format_exc())",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "on_exit",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def on_exit(task):\n    global _ON_EXIT_TASKS\n    if _ON_EXIT_TASKS is None:\n        _ON_EXIT_TASKS = tasks = []\n        try:\n            evt = int(os.getenv('_FCGI_SHUTDOWN_EVENT_'))\n        except (TypeError, ValueError):\n            maybe_log(\"Could not wait on event %s\" % os.getenv('_FCGI_SHUTDOWN_EVENT_'))\n        else:\n            def _wait_for_exit():",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "start_file_watcher",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def start_file_watcher(path, restart_regex):\n    if restart_regex is None:\n        restart_regex = \".*((\\\\.py)|(\\\\.config))$\"\n    elif not restart_regex:\n        # restart regex set to empty string, no restart behavior\n        return\n    def enum_changes(path):\n        \"\"\"Returns a generator that blocks until a change occurs, then yields\n        the filename of the changed file.\n        Yields an empty string and stops if the buffer overruns, indicating that",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "get_wsgi_handler",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def get_wsgi_handler(handler_name):\n    if not handler_name:\n        raise Exception('WSGI_HANDLER env var must be set')\n    if not isinstance(handler_name, str):\n        handler_name = to_str(handler_name)\n    module_name, _, callable_name = handler_name.rpartition('.')\n    should_call = callable_name.endswith('()')\n    callable_name = callable_name[:-2] if should_call else callable_name\n    name_list = [(callable_name, should_call)]\n    handler = None",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "read_wsgi_handler",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def read_wsgi_handler(physical_path):\n    global APPINSIGHT_CLIENT\n    env = get_environment(physical_path)\n    os.environ.update(env)\n    for path in (v for k, v in env.items() if k.lower() == 'pythonpath'):\n        # Expand environment variables manually.\n        expanded_path = re.sub(\n            '%(\\\\w+?)%',\n            lambda m: os.getenv(m.group(1), ''),\n            path",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def main():\n    initialized = False\n    log('wfastcgi.py %s started' % __version__)\n    log('Python version: %s' % sys.version)\n    print('wfastcgi.py %s started' % __version__)\n    try:\n        fcgi_stream = sys.stdin.detach() if sys.version_info[0] >= 3 else sys.stdin\n        try:\n            import msvcrt\n            msvcrt.setmode(fcgi_stream.fileno(), os.O_BINARY)",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "enable",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def enable():\n    executable = '\"' + sys.executable + '\"' if ' ' in sys.executable else sys.executable\n    quoted_file = '\"' + __file__ + '\"' if ' ' in __file__ else __file__\n    res = _run_appcmd([\n        \"set\", \"config\", \"/section:system.webServer/fastCGI\",\n        \"/+[fullPath='\" + executable + \"', arguments='\" + quoted_file + \"', signalBeforeTerminateSeconds='30']\"\n    ])\n    if res == 0:\n        print('\"%s|%s\" can now be used as a FastCGI script processor' % (executable, quoted_file))\n    return res",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "disable",
        "kind": 2,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "def disable():\n    executable = '\"' + sys.executable + '\"' if ' ' in sys.executable else sys.executable\n    quoted_file = '\"' + __file__ + '\"' if ' ' in __file__ else __file__    \n    res = _run_appcmd([\n        \"set\", \"config\", \"/section:system.webServer/fastCGI\",\n        \"/-[fullPath='\" + executable + \"', arguments='\" + quoted_file + \"', signalBeforeTerminateSeconds='30']\"\n    ])\n    if res == 0:\n        print('\"%s|%s\" is no longer registered for use with FastCGI' % (executable, quoted_file))\n    return res",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "__author__ = \"Microsoft Corporation <ptvshelp@microsoft.com>\"\n__version__ = \"3.0.0\"\nimport ctypes\nimport datetime\nimport os\nimport re\nimport struct\nimport sys\nimport traceback\nfrom xml.dom import minidom",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "__version__",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "__version__ = \"3.0.0\"\nimport ctypes\nimport datetime\nimport os\nimport re\nimport struct\nimport sys\nimport traceback\nfrom xml.dom import minidom\n# os.system(\"conda activate Face_Recognition_v3\")",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FCGI_VERSION_1",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FCGI_VERSION_1 = 1\nFCGI_HEADER_LEN = 8\nFCGI_BEGIN_REQUEST       = 1\nFCGI_ABORT_REQUEST       = 2\nFCGI_END_REQUEST         = 3\nFCGI_PARAMS              = 4\nFCGI_STDIN               = 5\nFCGI_STDOUT              = 6\nFCGI_STDERR              = 7\nFCGI_DATA                = 8",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FCGI_HEADER_LEN",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FCGI_HEADER_LEN = 8\nFCGI_BEGIN_REQUEST       = 1\nFCGI_ABORT_REQUEST       = 2\nFCGI_END_REQUEST         = 3\nFCGI_PARAMS              = 4\nFCGI_STDIN               = 5\nFCGI_STDOUT              = 6\nFCGI_STDERR              = 7\nFCGI_DATA                = 8\nFCGI_GET_VALUES          = 9",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FCGI_MAXTYPE",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FCGI_MAXTYPE = FCGI_UNKNOWN_TYPE\nFCGI_NULL_REQUEST_ID    = 0\nFCGI_KEEP_CONN = 1\nFCGI_RESPONDER  = 1\nFCGI_AUTHORIZER = 2\nFCGI_FILTER     = 3\nFCGI_REQUEST_COMPLETE = 0\nFCGI_CANT_MPX_CONN    = 1\nFCGI_OVERLOADED       = 2\nFCGI_UNKNOWN_ROLE     = 3",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FCGI_KEEP_CONN",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FCGI_KEEP_CONN = 1\nFCGI_RESPONDER  = 1\nFCGI_AUTHORIZER = 2\nFCGI_FILTER     = 3\nFCGI_REQUEST_COMPLETE = 0\nFCGI_CANT_MPX_CONN    = 1\nFCGI_OVERLOADED       = 2\nFCGI_UNKNOWN_ROLE     = 3\nFCGI_MAX_CONNS  = \"FCGI_MAX_CONNS\"\nFCGI_MAX_REQS   = \"FCGI_MAX_REQS\"",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FCGI_AUTHORIZER",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FCGI_AUTHORIZER = 2\nFCGI_FILTER     = 3\nFCGI_REQUEST_COMPLETE = 0\nFCGI_CANT_MPX_CONN    = 1\nFCGI_OVERLOADED       = 2\nFCGI_UNKNOWN_ROLE     = 3\nFCGI_MAX_CONNS  = \"FCGI_MAX_CONNS\"\nFCGI_MAX_REQS   = \"FCGI_MAX_REQS\"\nFCGI_MPXS_CONNS = \"FCGI_MPXS_CONNS\"\nclass FastCgiRecord(object):",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FCGI_REQUEST_COMPLETE",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FCGI_REQUEST_COMPLETE = 0\nFCGI_CANT_MPX_CONN    = 1\nFCGI_OVERLOADED       = 2\nFCGI_UNKNOWN_ROLE     = 3\nFCGI_MAX_CONNS  = \"FCGI_MAX_CONNS\"\nFCGI_MAX_REQS   = \"FCGI_MAX_REQS\"\nFCGI_MPXS_CONNS = \"FCGI_MPXS_CONNS\"\nclass FastCgiRecord(object):\n    \"\"\"Represents a FastCgiRecord.  Encapulates the type, role, flags.  Holds\n    onto the params which we will receive and update later.\"\"\"",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FCGI_MPXS_CONNS",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FCGI_MPXS_CONNS = \"FCGI_MPXS_CONNS\"\nclass FastCgiRecord(object):\n    \"\"\"Represents a FastCgiRecord.  Encapulates the type, role, flags.  Holds\n    onto the params which we will receive and update later.\"\"\"\n    def __init__(self, type, req_id, role, flags):\n        self.type = type\n        self.req_id = req_id\n        self.role = role\n        self.flags = flags\n        self.params = {}",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "RAW_VALUE_NAMES",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "RAW_VALUE_NAMES = {\n    'SCRIPT_NAME' : 'wsgi.script_name',\n    'PATH_INFO' : 'wsgi.path_info',\n    'QUERY_STRING' : 'wsgi.query_string',\n    'HTTP_X_ORIGINAL_URL' : 'wfastcgi.http_x_original_url',\n}\ndef read_fastcgi_params(stream, req_id, content):\n    if not content:\n        return None\n    offset = 0",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "REQUEST_PROCESSORS",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "REQUEST_PROCESSORS = {\n    FCGI_BEGIN_REQUEST : read_fastcgi_begin_request,\n    FCGI_ABORT_REQUEST : read_fastcgi_abort_request,\n    FCGI_PARAMS : read_fastcgi_params,\n    FCGI_STDIN : read_fastcgi_input,\n    FCGI_DATA : read_fastcgi_data,\n    FCGI_GET_VALUES : read_fastcgi_get_values\n}\nAPPINSIGHT_CLIENT = None\ndef log(txt):",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "APPINSIGHT_CLIENT",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "APPINSIGHT_CLIENT = None\ndef log(txt):\n    \"\"\"Logs messages to a log file if WSGI_LOG env var is defined.\"\"\"\n    if APPINSIGHT_CLIENT:\n        try:\n            APPINSIGHT_CLIENT.track_event(txt)\n        except:\n            pass\n    log_file = os.environ.get('WSGI_LOG')\n    if log_file:",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "ReadDirectoryChangesW",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "ReadDirectoryChangesW = ctypes.windll.kernel32.ReadDirectoryChangesW\nReadDirectoryChangesW.restype = ctypes.c_uint32\nReadDirectoryChangesW.argtypes  = [\n    ctypes.c_void_p,     # HANDLE hDirectory\n    ctypes.c_void_p,     # LPVOID lpBuffer\n    ctypes.c_uint32,     # DWORD nBufferLength\n    ctypes.c_uint32,     # BOOL bWatchSubtree\n    ctypes.c_uint32,     # DWORD dwNotifyFilter\n    ctypes.POINTER(ctypes.c_uint32),  # LPDWORD lpBytesReturned\n    ctypes.c_void_p,     # LPOVERLAPPED lpOverlapped",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "ReadDirectoryChangesW.restype",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "ReadDirectoryChangesW.restype = ctypes.c_uint32\nReadDirectoryChangesW.argtypes  = [\n    ctypes.c_void_p,     # HANDLE hDirectory\n    ctypes.c_void_p,     # LPVOID lpBuffer\n    ctypes.c_uint32,     # DWORD nBufferLength\n    ctypes.c_uint32,     # BOOL bWatchSubtree\n    ctypes.c_uint32,     # DWORD dwNotifyFilter\n    ctypes.POINTER(ctypes.c_uint32),  # LPDWORD lpBytesReturned\n    ctypes.c_void_p,     # LPOVERLAPPED lpOverlapped\n    ctypes.c_void_p      # LPOVERLAPPED_COMPLETION_ROUTINE lpCompletionRoutine",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FILE_LIST_DIRECTORY",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FILE_LIST_DIRECTORY = 1\nFILE_SHARE_READ = 0x00000001\nFILE_SHARE_WRITE = 0x00000002\nFILE_SHARE_DELETE = 0x00000004\nFILE_FLAG_BACKUP_SEMANTICS = 0x02000000\nMAX_PATH = 260\nFILE_NOTIFY_CHANGE_LAST_WRITE  = 0x10\nERROR_NOTIFY_ENUM_DIR = 1022\nINVALID_HANDLE_VALUE = 0xFFFFFFFF\nclass FILE_NOTIFY_INFORMATION(ctypes.Structure):",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FILE_SHARE_READ",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FILE_SHARE_READ = 0x00000001\nFILE_SHARE_WRITE = 0x00000002\nFILE_SHARE_DELETE = 0x00000004\nFILE_FLAG_BACKUP_SEMANTICS = 0x02000000\nMAX_PATH = 260\nFILE_NOTIFY_CHANGE_LAST_WRITE  = 0x10\nERROR_NOTIFY_ENUM_DIR = 1022\nINVALID_HANDLE_VALUE = 0xFFFFFFFF\nclass FILE_NOTIFY_INFORMATION(ctypes.Structure):\n    _fields_ = [('NextEntryOffset', ctypes.c_uint32),",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FILE_SHARE_WRITE",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FILE_SHARE_WRITE = 0x00000002\nFILE_SHARE_DELETE = 0x00000004\nFILE_FLAG_BACKUP_SEMANTICS = 0x02000000\nMAX_PATH = 260\nFILE_NOTIFY_CHANGE_LAST_WRITE  = 0x10\nERROR_NOTIFY_ENUM_DIR = 1022\nINVALID_HANDLE_VALUE = 0xFFFFFFFF\nclass FILE_NOTIFY_INFORMATION(ctypes.Structure):\n    _fields_ = [('NextEntryOffset', ctypes.c_uint32),\n                ('Action', ctypes.c_uint32),",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FILE_SHARE_DELETE",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FILE_SHARE_DELETE = 0x00000004\nFILE_FLAG_BACKUP_SEMANTICS = 0x02000000\nMAX_PATH = 260\nFILE_NOTIFY_CHANGE_LAST_WRITE  = 0x10\nERROR_NOTIFY_ENUM_DIR = 1022\nINVALID_HANDLE_VALUE = 0xFFFFFFFF\nclass FILE_NOTIFY_INFORMATION(ctypes.Structure):\n    _fields_ = [('NextEntryOffset', ctypes.c_uint32),\n                ('Action', ctypes.c_uint32),\n                ('FileNameLength', ctypes.c_uint32),",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "FILE_FLAG_BACKUP_SEMANTICS",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "FILE_FLAG_BACKUP_SEMANTICS = 0x02000000\nMAX_PATH = 260\nFILE_NOTIFY_CHANGE_LAST_WRITE  = 0x10\nERROR_NOTIFY_ENUM_DIR = 1022\nINVALID_HANDLE_VALUE = 0xFFFFFFFF\nclass FILE_NOTIFY_INFORMATION(ctypes.Structure):\n    _fields_ = [('NextEntryOffset', ctypes.c_uint32),\n                ('Action', ctypes.c_uint32),\n                ('FileNameLength', ctypes.c_uint32),\n                ('Filename', ctypes.c_wchar)]",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "MAX_PATH",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "MAX_PATH = 260\nFILE_NOTIFY_CHANGE_LAST_WRITE  = 0x10\nERROR_NOTIFY_ENUM_DIR = 1022\nINVALID_HANDLE_VALUE = 0xFFFFFFFF\nclass FILE_NOTIFY_INFORMATION(ctypes.Structure):\n    _fields_ = [('NextEntryOffset', ctypes.c_uint32),\n                ('Action', ctypes.c_uint32),\n                ('FileNameLength', ctypes.c_uint32),\n                ('Filename', ctypes.c_wchar)]\n_ON_EXIT_TASKS = None",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "ERROR_NOTIFY_ENUM_DIR",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "ERROR_NOTIFY_ENUM_DIR = 1022\nINVALID_HANDLE_VALUE = 0xFFFFFFFF\nclass FILE_NOTIFY_INFORMATION(ctypes.Structure):\n    _fields_ = [('NextEntryOffset', ctypes.c_uint32),\n                ('Action', ctypes.c_uint32),\n                ('FileNameLength', ctypes.c_uint32),\n                ('Filename', ctypes.c_wchar)]\n_ON_EXIT_TASKS = None\ndef run_exit_tasks():\n    global _ON_EXIT_TASKS",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "INVALID_HANDLE_VALUE",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "INVALID_HANDLE_VALUE = 0xFFFFFFFF\nclass FILE_NOTIFY_INFORMATION(ctypes.Structure):\n    _fields_ = [('NextEntryOffset', ctypes.c_uint32),\n                ('Action', ctypes.c_uint32),\n                ('FileNameLength', ctypes.c_uint32),\n                ('Filename', ctypes.c_wchar)]\n_ON_EXIT_TASKS = None\ndef run_exit_tasks():\n    global _ON_EXIT_TASKS\n    maybe_log(\"Running on_exit tasks\")",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "_ON_EXIT_TASKS",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "_ON_EXIT_TASKS = None\ndef run_exit_tasks():\n    global _ON_EXIT_TASKS\n    maybe_log(\"Running on_exit tasks\")\n    while _ON_EXIT_TASKS:\n        tasks, _ON_EXIT_TASKS = _ON_EXIT_TASKS, []\n        for t in tasks:\n            try:\n                t()\n            except Exception:",
        "detail": "wfastcgi",
        "documentation": {}
    },
    {
        "label": "_REQUESTS",
        "kind": 5,
        "importPath": "wfastcgi",
        "description": "wfastcgi",
        "peekOfCode": "_REQUESTS = {}\ndef main():\n    initialized = False\n    log('wfastcgi.py %s started' % __version__)\n    log('Python version: %s' % sys.version)\n    print('wfastcgi.py %s started' % __version__)\n    try:\n        fcgi_stream = sys.stdin.detach() if sys.version_info[0] >= 3 else sys.stdin\n        try:\n            import msvcrt",
        "detail": "wfastcgi",
        "documentation": {}
    }
]